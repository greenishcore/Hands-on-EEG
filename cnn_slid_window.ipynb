{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T21:19:13.397278Z",
     "end_time": "2023-05-01T21:19:20.070610Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#read the model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 4), stride=(1, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(47104, 128)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        #print('x:', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1:', x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2:', x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print('flatten:', x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = EEGNet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T21:19:20.075644Z",
     "end_time": "2023-05-01T21:19:20.165732Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滑窗后信号形状： (32, 3000, 126)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sliding_window(signal, window_size, step_size):\n",
    "    n_channels, n_samples = signal.shape\n",
    "    n_windows = int((n_samples - window_size) / step_size) + 1\n",
    "    windows = np.zeros((n_channels, window_size, n_windows))\n",
    "    for i in range(n_windows):\n",
    "        windows[:, :,i ] = signal.iloc[:, i*step_size:i*step_size+window_size]\n",
    "    return windows\n",
    "\n",
    "signal = pd.read_csv(\"C:\\\\Users\\\\a1882\\Desktop\\EEG\\eegdata\\\\raw\\lefthand_zyy_05_epocflex_2023.03.22t16.50.54+08.00.md.bp.csv\")\n",
    "signal = pd.DataFrame(signal)\n",
    "# 定义滑窗大小和滑动步长\n",
    "window_size = 3000\n",
    "step_size = 100\n",
    "\n",
    "# 对信号进行滑窗处理\n",
    "windows = sliding_window(signal, window_size, step_size)\n",
    "\n",
    "# 输出滑窗后的信号形状\n",
    "print(\"滑窗后信号形状：\", windows.shape)\n",
    "#print(windows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T21:19:22.885925Z",
     "end_time": "2023-05-01T21:19:24.352855Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[-0.3416,  0.1660, -0.1036, -0.1889]]],\n              \n              \n                      [[[ 0.4762, -0.3724,  0.2967, -0.1713]]],\n              \n              \n                      [[[ 0.4646, -0.3929,  0.1105, -0.0799]]],\n              \n              \n                      [[[ 0.2958, -0.1978,  0.1417,  0.3041]]],\n              \n              \n                      [[[ 0.4667, -0.1876,  0.0840,  0.4087]]],\n              \n              \n                      [[[-0.4505,  0.3464,  0.4886, -0.1638]]],\n              \n              \n                      [[[-0.0178, -0.1342,  0.2622,  0.4577]]],\n              \n              \n                      [[[ 0.2318, -0.2083, -0.0062,  0.0577]]],\n              \n              \n                      [[[ 0.0956,  0.4881,  0.3861,  0.2950]]],\n              \n              \n                      [[[ 0.2090,  0.2152, -0.4189, -0.1308]]],\n              \n              \n                      [[[-0.1299, -0.3265,  0.4227,  0.3056]]],\n              \n              \n                      [[[-0.1953,  0.3706, -0.0484, -0.3606]]],\n              \n              \n                      [[[ 0.2606, -0.4611,  0.0275,  0.3899]]],\n              \n              \n                      [[[ 0.4119, -0.2644, -0.0696,  0.4598]]],\n              \n              \n                      [[[ 0.1768,  0.3315, -0.1015,  0.0694]]],\n              \n              \n                      [[[-0.1764,  0.4724, -0.4194, -0.2834]]]])),\n             ('conv1.bias',\n              tensor([-0.1138,  0.1427, -0.2003,  0.4224,  0.3852, -0.1219,  0.3967, -0.4599,\n                       0.4384, -0.0843,  0.3584, -0.0379, -0.4680,  0.1710,  0.3871, -0.2675])),\n             ('bn1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn1.num_batches_tracked', tensor(0)),\n             ('conv2.weight',\n              tensor([[[[-0.0534,  0.1312]],\n              \n                       [[-0.0855, -0.1541]],\n              \n                       [[-0.0033,  0.1425]],\n              \n                       ...,\n              \n                       [[-0.0835,  0.0777]],\n              \n                       [[-0.1054,  0.1223]],\n              \n                       [[-0.0192, -0.1102]]],\n              \n              \n                      [[[ 0.0426,  0.0937]],\n              \n                       [[-0.1374,  0.1182]],\n              \n                       [[-0.1063,  0.0260]],\n              \n                       ...,\n              \n                       [[ 0.0934, -0.0647]],\n              \n                       [[-0.1720,  0.0770]],\n              \n                       [[-0.0771,  0.1083]]],\n              \n              \n                      [[[-0.1720,  0.1741]],\n              \n                       [[-0.0520, -0.1643]],\n              \n                       [[-0.1220, -0.1467]],\n              \n                       ...,\n              \n                       [[ 0.0196, -0.1184]],\n              \n                       [[ 0.0779, -0.1414]],\n              \n                       [[-0.0644,  0.1766]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-0.1255, -0.1330]],\n              \n                       [[ 0.0521, -0.1604]],\n              \n                       [[ 0.0999,  0.1566]],\n              \n                       ...,\n              \n                       [[-0.0745, -0.0893]],\n              \n                       [[ 0.0280,  0.1760]],\n              \n                       [[ 0.1341, -0.0518]]],\n              \n              \n                      [[[-0.0373, -0.0757]],\n              \n                       [[ 0.0378,  0.1069]],\n              \n                       [[ 0.1329,  0.0727]],\n              \n                       ...,\n              \n                       [[-0.1434,  0.0543]],\n              \n                       [[ 0.1325, -0.1198]],\n              \n                       [[-0.0586, -0.1704]]],\n              \n              \n                      [[[ 0.1388,  0.1254]],\n              \n                       [[-0.0155,  0.0718]],\n              \n                       [[-0.0917,  0.1439]],\n              \n                       ...,\n              \n                       [[ 0.1455, -0.1005]],\n              \n                       [[-0.0901,  0.0464]],\n              \n                       [[-0.0983,  0.0750]]]])),\n             ('conv2.bias',\n              tensor([-0.0559,  0.1259, -0.1487, -0.1474, -0.0893,  0.1639, -0.0571,  0.0389,\n                       0.1592,  0.0899, -0.1253,  0.1541,  0.0595, -0.1316, -0.0130,  0.1142,\n                      -0.0571,  0.1597, -0.0903, -0.1080,  0.1710, -0.0563, -0.1676, -0.0087,\n                      -0.1286,  0.1069, -0.0906, -0.1177, -0.0987,  0.0076, -0.0269,  0.0500])),\n             ('bn2.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn2.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn2.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn2.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn2.num_batches_tracked', tensor(0)),\n             ('fc1.weight',\n              tensor([[-3.3497e-03, -3.2311e-03,  3.5420e-04,  ..., -2.7482e-03,\n                        2.0807e-03,  2.4851e-03],\n                      [ 2.6930e-03, -1.5329e-03,  3.4344e-03,  ..., -1.2108e-03,\n                        1.4174e-03,  1.8389e-03],\n                      [-1.1227e-03,  1.2004e-03,  3.5736e-03,  ...,  1.4990e-03,\n                       -3.1809e-03, -3.1233e-03],\n                      ...,\n                      [ 3.2297e-03,  2.2576e-03, -2.1731e-03,  ...,  2.2925e-03,\n                        1.4265e-03,  5.5552e-04],\n                      [-3.4571e-03,  3.9914e-03, -4.8617e-04,  ..., -1.0862e-05,\n                       -3.6873e-03,  2.5954e-03],\n                      [ 1.2903e-03,  7.3867e-04,  3.7298e-04,  ..., -1.2414e-03,\n                       -3.2978e-04,  2.0774e-03]])),\n             ('fc1.bias',\n              tensor([ 3.1648e-04,  3.6107e-04, -4.3561e-03, -3.2383e-03,  1.6162e-03,\n                       4.5099e-03, -1.2790e-03,  5.0502e-04,  3.1037e-03, -3.4106e-03,\n                      -2.6870e-03, -3.5675e-03, -4.5683e-03,  6.9818e-04,  3.6537e-03,\n                       4.1235e-03, -5.6352e-04, -4.2836e-03, -1.5502e-03, -2.4057e-03,\n                      -3.5860e-04, -8.5992e-04, -3.3586e-03,  5.1261e-04, -1.2531e-03,\n                      -1.8266e-03,  2.4955e-03, -2.5582e-03, -2.3947e-03,  2.2213e-03,\n                      -3.5543e-03, -2.6687e-03,  3.8149e-03, -4.4099e-03,  2.4540e-03,\n                      -1.1697e-03,  1.9728e-04,  4.7437e-04, -3.3364e-03,  3.5094e-03,\n                      -2.0681e-03, -2.7848e-03, -3.6565e-03, -1.8253e-03, -7.9040e-04,\n                      -3.2960e-03,  6.5067e-04,  4.5874e-04, -1.7873e-03, -2.3165e-03,\n                      -1.7093e-03, -1.1350e-03, -2.0323e-03,  3.6392e-03,  1.7162e-03,\n                      -8.3488e-04,  1.1540e-03, -1.8374e-03,  3.7383e-03, -7.4878e-07,\n                       7.7446e-04,  2.5832e-03, -3.9601e-03,  1.7097e-03,  2.9212e-03,\n                      -1.7084e-03,  2.4072e-03,  2.1058e-03,  1.9512e-03,  3.0138e-03,\n                      -3.4487e-04,  4.1495e-03,  1.1926e-06, -3.3270e-03, -2.2770e-03,\n                       2.4560e-03,  5.2057e-04, -3.3257e-03, -1.7833e-03, -6.4446e-04,\n                      -6.3724e-04, -4.4578e-04,  9.4681e-04,  3.8466e-03, -8.4012e-04,\n                       1.7217e-03, -1.5300e-03,  8.6582e-04, -6.7373e-04,  3.4101e-03,\n                       1.4124e-03,  4.2297e-04,  4.0969e-03, -1.3433e-03, -3.0788e-03,\n                       1.2935e-03, -1.0409e-04,  3.2236e-03,  3.5788e-03,  3.8274e-03,\n                       3.0962e-03, -1.3771e-03, -2.6344e-03,  4.4626e-03, -2.2468e-03,\n                      -2.7807e-03, -5.5794e-04, -1.6044e-03, -1.2197e-03, -2.8557e-03,\n                       3.4713e-03,  2.2764e-03, -1.5282e-03,  2.5689e-03, -1.0913e-04,\n                      -1.5265e-03,  3.5269e-03, -2.2990e-03,  6.6568e-04,  3.6197e-03,\n                      -2.0547e-03,  3.6267e-03, -3.3734e-03, -5.4639e-04, -1.6477e-03,\n                      -2.4542e-04,  2.1933e-03, -3.3820e-03])),\n             ('fc2.weight',\n              tensor([[ 4.5045e-02, -6.5605e-02,  5.6447e-02, -1.2421e-02, -8.5646e-02,\n                       -2.0997e-02, -3.6813e-02,  3.0686e-02, -1.6139e-02,  3.2647e-02,\n                        4.7837e-02,  2.5091e-02,  1.7846e-02, -2.4105e-02,  6.4916e-02,\n                       -2.4960e-02, -6.4757e-02,  1.5312e-02,  7.7632e-02, -4.3002e-02,\n                       -6.3362e-02,  8.6926e-02, -3.1377e-02, -8.4254e-02, -7.3564e-02,\n                       -1.4816e-02,  3.9255e-02,  6.3988e-02, -6.0654e-02, -6.5748e-02,\n                       -4.6530e-02,  4.8525e-02,  7.4135e-02,  6.9583e-02, -5.9559e-02,\n                       -6.7419e-02,  5.8490e-02, -5.0873e-02, -7.5213e-02,  2.4103e-02,\n                        7.7819e-02, -7.7669e-02, -3.7714e-02, -5.8778e-02,  7.2649e-02,\n                        4.1905e-02,  3.4594e-02,  4.4023e-03,  6.4399e-02,  1.2226e-02,\n                        2.0756e-02,  4.0366e-02,  1.3963e-02,  1.9217e-02, -4.4830e-02,\n                       -2.2966e-02,  4.1631e-02,  6.1107e-02,  4.1001e-02,  3.7375e-02,\n                       -2.0675e-03,  1.5958e-02, -4.4496e-02, -2.5135e-02, -2.0353e-02,\n                       -8.1707e-02,  2.2915e-02,  3.4557e-02, -3.6629e-02,  6.5851e-02,\n                       -4.2388e-02,  2.7265e-02,  7.5194e-02,  2.0902e-02,  3.5567e-02,\n                        2.4721e-02, -8.2756e-02,  7.8066e-02, -3.9442e-02,  4.4036e-02,\n                       -8.6253e-02, -7.8212e-02,  7.1231e-02,  8.8021e-02,  8.4805e-02,\n                        7.0298e-02, -2.5595e-02,  3.7932e-02,  8.5164e-02, -7.9324e-02,\n                        5.5292e-02, -1.5514e-02,  4.3441e-02, -2.2467e-04,  1.8378e-02,\n                       -5.9434e-02, -5.3514e-02, -5.0523e-02, -7.3037e-02, -6.5912e-02,\n                       -3.5261e-03,  4.4404e-02,  2.7178e-02,  8.5038e-03, -2.4226e-02,\n                       -4.1246e-02,  7.8892e-02,  4.3282e-02,  6.1003e-02, -8.2327e-02,\n                       -1.0096e-02,  8.1616e-02,  4.5492e-02,  6.3980e-02, -5.7720e-02,\n                        3.8333e-02, -4.9412e-02, -5.8088e-02,  5.8868e-02, -4.9152e-02,\n                        2.3705e-02, -6.4813e-02,  2.4629e-02,  3.8632e-03, -5.8341e-03,\n                       -5.0977e-02,  1.1459e-02,  4.2140e-02],\n                      [-2.2729e-02,  6.2941e-02,  7.0480e-03, -1.1986e-02,  3.3192e-02,\n                        6.4012e-02, -2.0910e-02, -8.5884e-02, -3.6928e-02,  4.6410e-02,\n                        2.1147e-02,  2.8852e-02, -3.2700e-02,  2.9979e-02,  3.4580e-02,\n                       -2.6663e-02, -2.7486e-03,  7.4154e-02, -5.0265e-02,  6.5195e-02,\n                       -2.5799e-03,  5.8417e-03,  4.0909e-02,  6.5857e-02,  4.5979e-02,\n                        2.6829e-02,  9.5874e-03,  4.0816e-02,  5.6393e-02,  5.9827e-02,\n                       -5.4852e-02, -5.5859e-02, -3.3991e-02,  3.7308e-02,  7.3205e-02,\n                       -7.2520e-02,  4.9437e-02, -4.2744e-02, -1.4323e-02, -4.9086e-03,\n                        8.5201e-02,  8.2886e-03, -4.4822e-02,  3.1631e-02,  4.7822e-03,\n                        5.4837e-02,  6.5621e-02, -8.8352e-02, -3.6053e-02,  5.3177e-02,\n                       -4.6273e-03,  1.7643e-02, -3.9258e-02, -6.6137e-02,  4.2993e-02,\n                        6.0609e-02,  6.7586e-02,  2.5542e-02,  2.8182e-02, -7.0117e-02,\n                        7.7294e-02,  2.7673e-02, -8.2772e-02, -7.9301e-02, -7.3626e-02,\n                        1.6784e-02,  4.7723e-04, -5.8243e-02, -8.4156e-02,  1.2638e-02,\n                        9.4593e-03,  6.2665e-02,  4.5348e-03,  4.6050e-03,  4.5700e-02,\n                       -3.9472e-02,  3.7558e-02, -4.6851e-02, -1.0089e-02, -6.2411e-02,\n                        6.0820e-02,  4.3511e-02, -1.8053e-02, -4.5485e-02,  8.0206e-02,\n                        6.4039e-02, -4.7358e-02, -1.5750e-02,  1.0776e-03,  9.7166e-03,\n                       -8.1235e-02,  2.7573e-02, -3.4837e-02,  7.5971e-02,  2.1958e-02,\n                       -3.9729e-02, -6.3591e-02, -5.5403e-02, -5.5588e-02,  7.3016e-02,\n                        1.6571e-02, -7.7846e-02,  7.7731e-02,  2.8518e-03,  4.9464e-02,\n                       -4.7607e-02, -4.8078e-02,  4.0923e-02, -7.7492e-02, -5.3545e-02,\n                       -8.5483e-02, -4.9321e-02, -3.7270e-02, -2.4945e-03,  2.8326e-02,\n                        2.8619e-02, -4.5584e-03, -6.2919e-02,  5.7086e-03, -1.2591e-02,\n                        7.7127e-02,  8.1985e-02, -2.6165e-02, -3.4857e-02,  3.5927e-05,\n                        5.9204e-02,  4.1724e-02,  4.9204e-02],\n                      [ 5.1230e-03,  7.4588e-02, -1.4640e-02,  2.4306e-02, -1.6753e-02,\n                       -6.1361e-02,  4.2600e-02,  6.9523e-02,  1.6660e-02, -4.7867e-02,\n                        6.4536e-02,  1.2217e-02,  5.4960e-02,  3.3996e-02, -1.1606e-02,\n                        2.1847e-02, -6.4752e-02, -3.5662e-02,  7.6588e-02, -1.0006e-02,\n                       -8.5222e-02, -2.1240e-02, -5.6901e-02, -3.5936e-02, -3.0849e-02,\n                        5.0345e-02, -6.0304e-02, -4.3607e-02,  6.6590e-02,  3.0610e-02,\n                       -3.1695e-03, -5.4981e-02,  4.2668e-02,  2.0021e-02,  9.2834e-03,\n                       -3.5412e-02, -1.3434e-02,  3.4935e-02,  1.8793e-02,  4.1684e-02,\n                       -6.1268e-02, -1.6959e-02, -3.4182e-02,  8.2854e-02,  3.5841e-02,\n                        1.9869e-02,  6.8531e-02,  1.0908e-02, -1.5473e-02,  4.6470e-02,\n                        2.3916e-02, -3.4055e-02,  3.3708e-02, -4.8327e-02,  6.3788e-02,\n                       -2.5949e-02, -3.9711e-02,  1.1531e-02,  6.1083e-02,  3.3354e-02,\n                       -5.3130e-02,  8.4020e-02,  5.8643e-02,  6.2104e-02,  8.1231e-02,\n                        2.5282e-02, -7.3352e-02, -1.1398e-02, -7.8626e-02, -2.9594e-02,\n                       -2.9453e-02,  8.4915e-02,  4.5571e-02,  4.3118e-02, -7.0875e-02,\n                       -6.3688e-02,  6.1111e-02, -5.1814e-02,  5.3683e-02, -6.5556e-02,\n                       -2.1783e-02, -2.0277e-02, -1.3489e-02, -3.6994e-02, -7.9224e-03,\n                        7.9464e-03,  2.9981e-02, -7.4023e-02,  2.1137e-02, -3.4528e-02,\n                       -5.9938e-02, -5.0287e-02, -3.3695e-02, -6.9688e-02, -2.2265e-02,\n                        8.0181e-02, -5.5373e-02, -3.6013e-02,  3.7044e-02, -3.6946e-02,\n                        1.8669e-02,  6.6143e-02, -4.0375e-02,  3.1288e-02,  8.6253e-02,\n                        5.0145e-02,  2.5081e-02,  6.5970e-02,  6.8974e-02,  5.6838e-02,\n                        3.0597e-02, -3.4173e-02, -9.1520e-03, -5.7299e-02,  2.0557e-02,\n                        6.4129e-02,  8.4235e-02,  3.8107e-02,  4.6500e-02, -6.8636e-02,\n                        1.1327e-02,  7.2074e-02, -9.8461e-03, -3.3963e-02,  5.4080e-03,\n                        7.1019e-02, -4.1195e-02,  2.3494e-03],\n                      [ 4.5622e-02,  1.3441e-02,  5.6610e-02, -7.5515e-02, -8.5372e-03,\n                       -1.9922e-02, -7.4436e-02, -8.7827e-03,  1.8894e-02, -8.4001e-02,\n                       -2.9489e-02, -4.9602e-02,  8.4647e-02,  5.3540e-02,  5.9788e-02,\n                        8.1046e-02, -6.7174e-02,  6.2645e-02,  4.7236e-02, -3.7191e-02,\n                        7.5313e-02, -4.7937e-02,  9.2118e-03, -3.5666e-02, -3.3495e-03,\n                        8.3481e-02, -3.4349e-02, -7.5346e-02,  2.3752e-02,  6.7081e-02,\n                       -6.3163e-02, -5.9293e-02,  2.4422e-02,  3.9058e-02,  7.5198e-02,\n                        2.6622e-02,  6.8925e-02, -2.9171e-02,  5.9043e-03, -1.4675e-02,\n                        7.0022e-02,  8.2256e-02, -5.2207e-02, -5.0538e-02, -6.3196e-02,\n                       -4.3677e-02,  4.1862e-02,  3.0811e-02,  1.3495e-02,  1.3375e-02,\n                        7.4985e-02,  2.9547e-02, -2.2997e-02,  5.0785e-02, -3.1918e-02,\n                       -1.1816e-02, -7.0028e-02,  1.0912e-02,  8.2132e-02,  7.3856e-02,\n                        7.5480e-02,  1.1350e-03,  5.8305e-02, -6.0847e-03, -5.7365e-02,\n                       -3.1739e-02,  3.6242e-02, -5.0446e-02, -8.0821e-02,  5.1203e-02,\n                       -5.2169e-02, -5.7933e-02, -7.4414e-02,  8.7774e-02,  2.3573e-02,\n                       -8.6317e-02,  8.8155e-02,  7.4104e-02,  1.1511e-02,  3.8200e-02,\n                        4.5400e-02,  2.9956e-03,  2.0511e-02,  1.6997e-02,  3.7252e-02,\n                       -8.6604e-02,  7.9083e-02,  1.2521e-02, -3.9564e-02,  3.5637e-02,\n                       -5.4888e-02, -5.4041e-02,  5.4145e-02,  5.9356e-02,  8.1984e-02,\n                        8.1154e-02, -6.9707e-02, -3.7612e-02, -5.3059e-02, -7.0601e-02,\n                       -4.0047e-02, -7.7196e-02,  3.1088e-02,  5.4194e-02, -1.8397e-02,\n                       -3.1141e-02,  7.1727e-02,  2.0447e-02, -3.5982e-02,  3.1431e-02,\n                       -3.5141e-02, -4.5117e-02, -1.2674e-02, -4.7021e-02,  5.2842e-02,\n                       -6.9499e-02, -4.6988e-02,  3.6048e-02,  4.9189e-02,  6.6165e-02,\n                       -5.3318e-02, -1.0755e-02,  3.4348e-02,  8.1132e-02,  8.8312e-02,\n                       -2.7732e-03,  6.7490e-02,  2.3796e-02],\n                      [-6.0394e-02,  4.8188e-02,  5.3177e-03,  2.9921e-02, -9.4231e-03,\n                       -2.7404e-02, -1.6483e-02,  6.1822e-02, -3.4828e-02, -8.2743e-02,\n                       -3.9856e-02,  6.5381e-02, -5.3620e-02,  5.9381e-02, -8.4331e-02,\n                        8.1991e-02,  9.7720e-03, -8.0629e-02, -8.1330e-03, -8.7238e-03,\n                        1.3080e-02,  3.5978e-02,  6.8048e-02,  6.1817e-02, -6.9122e-02,\n                        3.8225e-02, -4.9067e-02,  6.5536e-02, -2.5609e-02,  7.5752e-02,\n                       -8.3063e-02,  1.0578e-03,  1.0567e-02, -6.0240e-02, -5.6319e-02,\n                        6.8600e-02,  5.2852e-02, -1.7726e-02, -3.2649e-02, -8.0340e-02,\n                        3.4553e-02, -8.2031e-02, -6.8390e-02,  2.1192e-02, -3.6138e-02,\n                       -4.6056e-02, -3.6937e-02, -9.2992e-03,  8.0843e-02,  5.8318e-02,\n                       -2.4059e-02,  8.3537e-02,  8.2366e-02, -3.6869e-02, -5.5042e-02,\n                       -3.5081e-02,  2.0664e-02, -4.7508e-02,  1.7710e-02, -1.1443e-02,\n                        6.6667e-02,  8.2870e-02, -7.1995e-02,  8.3649e-02,  7.0614e-02,\n                        6.9996e-02,  8.4151e-02, -3.9471e-02,  6.9047e-02, -4.1927e-03,\n                        1.2950e-02, -3.6484e-02, -7.1997e-02, -1.1892e-02, -2.5534e-02,\n                       -4.1182e-02, -4.1661e-02, -4.4174e-02, -5.9482e-03,  5.9455e-02,\n                        1.5344e-02,  2.5192e-02, -2.7312e-02,  7.0750e-02, -2.6450e-02,\n                        2.8065e-02,  7.3847e-02,  3.4496e-02,  1.5241e-02,  3.6930e-02,\n                       -6.5862e-02,  4.4630e-02, -5.5664e-02, -6.6509e-02,  3.5059e-02,\n                       -5.2892e-03, -8.4744e-02,  7.7742e-02,  2.6893e-02,  8.0970e-02,\n                       -8.4027e-02,  3.4142e-02,  7.8826e-02,  2.7376e-04,  3.6232e-02,\n                        6.0418e-03, -6.9220e-02, -6.9947e-02, -1.4087e-03, -6.2783e-02,\n                        3.5471e-02,  6.8434e-02,  6.9666e-02,  2.0172e-02, -1.6431e-02,\n                        2.9261e-02, -7.5812e-02,  3.7675e-02,  2.2903e-02, -6.0854e-02,\n                       -3.2452e-02,  7.0888e-02,  4.2344e-02, -2.2494e-02,  6.7955e-02,\n                        8.1081e-02, -6.7899e-02,  4.0248e-02],\n                      [-4.7741e-02, -1.9678e-02, -7.2539e-02, -1.5969e-02, -5.7370e-02,\n                       -1.2021e-02,  6.6797e-02,  1.8423e-02, -6.2776e-02, -4.6908e-02,\n                        8.1331e-02, -5.5789e-02, -4.4683e-02, -5.8611e-02,  5.6510e-02,\n                        2.1008e-02, -4.0200e-02, -2.9276e-02,  3.5660e-02, -6.3105e-02,\n                        9.8333e-03, -4.8388e-02, -8.7602e-02, -7.1738e-02,  7.3162e-02,\n                       -1.7697e-02,  2.1665e-02, -4.7433e-02, -5.1097e-02,  1.7692e-02,\n                       -6.6086e-02,  5.9446e-02, -2.2924e-03,  1.8864e-02,  5.2647e-02,\n                       -5.5585e-02, -4.5482e-02,  6.9953e-02, -8.4434e-02, -1.0894e-02,\n                       -3.1204e-02,  7.6372e-02, -8.4832e-02, -8.1303e-02, -5.3645e-02,\n                       -2.2012e-02, -3.9862e-02, -5.2715e-02,  7.4830e-02, -8.1922e-02,\n                       -3.3701e-02, -8.0476e-02,  4.1077e-02, -2.9431e-03, -8.6611e-02,\n                       -6.5433e-02, -8.9210e-03, -8.7115e-02,  5.5311e-02,  1.8440e-03,\n                        4.2835e-02, -4.9400e-02,  1.8104e-02, -9.9850e-03,  1.8336e-02,\n                        4.2172e-02, -4.6210e-02, -6.2914e-02, -2.3806e-02,  6.3985e-03,\n                        8.8343e-02, -6.1363e-02,  2.9093e-02, -4.6145e-02,  1.5148e-02,\n                       -8.3953e-02, -8.3479e-02, -3.2769e-02,  2.2077e-02, -5.9363e-02,\n                        4.1984e-02,  2.9246e-02,  7.3776e-02, -5.5362e-02,  1.3818e-02,\n                        8.2688e-02, -2.1262e-02,  7.9565e-02,  1.9776e-02,  7.8107e-02,\n                       -1.7811e-02, -1.5070e-02, -7.5163e-02, -5.0580e-03,  6.3936e-02,\n                        8.7017e-02, -5.7361e-02,  1.1414e-02,  5.3342e-02, -2.2395e-02,\n                        1.7642e-02,  4.1946e-02, -4.3798e-02,  2.0575e-02,  5.8631e-02,\n                        5.2690e-02,  8.1425e-03, -4.0794e-02,  3.7925e-03,  1.6809e-02,\n                        2.7791e-04,  8.4754e-02,  3.4701e-02,  8.0999e-02,  7.4345e-02,\n                        2.0704e-02, -4.2016e-02,  2.5149e-02,  7.5224e-02,  3.5483e-02,\n                        4.0623e-02, -3.0292e-02,  1.6442e-02,  7.5453e-02,  2.2922e-03,\n                       -1.7061e-02, -4.7555e-02, -5.4565e-02]])),\n             ('fc2.bias',\n              tensor([ 0.0406, -0.0007, -0.0366, -0.0436, -0.0662,  0.0235]))])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "save_path = './cnn_3000_100e_100.pt'\n",
    "\n",
    "onnx_file_name = \"./cnn_3000_100e_100.onnx\"\n",
    "torch.load(save_path, map_location=torch.device('cpu'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T21:19:25.491070Z",
     "end_time": "2023-05-01T21:19:26.016801Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is valid!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "# 我们可以使用异常处理的方法进行检验\n",
    "try:\n",
    "    # 当我们的模型不可用时，将会报出异常\n",
    "    onnx.checker.check_model(onnx_file_name)\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(\"The model is invalid: %s\"%e)\n",
    "else:\n",
    "    # 模型可用时，将不会报出异常，并会输出“The model is valid!”\n",
    "    print(\"The model is valid!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T21:19:28.110433Z",
     "end_time": "2023-05-01T21:19:29.541986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read\n",
      "read\n",
      "read\n",
      "walkl\n",
      "read\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "read\n",
      "read\n",
      "lefthand\n",
      "read\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "rest\n",
      "read\n",
      "read\n",
      "read\n",
      "read\n",
      "read\n",
      "read\n",
      "lefthand\n",
      "rest\n",
      "read\n",
      "read\n",
      "read\n",
      "read\n",
      "read\n",
      "read\n",
      "read\n",
      "walkl\n",
      "walkbase\n",
      "walkbase\n",
      "read\n",
      "read\n",
      "read\n",
      "lefthand\n",
      "read\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "read\n",
      "read\n",
      "walkl\n",
      "read\n",
      "read\n",
      "walkbase\n",
      "walkbase\n",
      "walkfocus\n",
      "walkfocus\n",
      "read\n",
      "read\n",
      "read\n",
      "lefthand\n",
      "walkl\n",
      "lefthand\n",
      "lefthand\n",
      "read\n",
      "lefthand\n",
      "walkbase\n",
      "walkl\n",
      "walkl\n",
      "walkl\n",
      "walkl\n",
      "rest\n",
      "walkl\n",
      "walkl\n",
      "read\n",
      "lefthand\n",
      "lefthand\n",
      "read\n",
      "lefthand\n",
      "walkl\n",
      "walkl\n",
      "lefthand\n",
      "read\n",
      "lefthand\n",
      "read\n",
      "lefthand\n",
      "read\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "read\n",
      "walkl\n",
      "walkl\n",
      "read\n",
      "walkl\n",
      "walkbase\n",
      "lefthand\n",
      "lefthand\n",
      "read\n",
      "lefthand\n",
      "read\n",
      "read\n",
      "read\n",
      "read\n",
      "walkl\n",
      "walkl\n",
      "read\n",
      "read\n",
      "walkl\n",
      "walkbase\n",
      "walkl\n",
      "walkl\n",
      "read\n",
      "read\n",
      "read\n",
      "walkbase\n",
      "walkl\n",
      "walkl\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "read\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "read\n",
      "walkl\n",
      "walkl\n",
      "walkl\n",
      "walkl\n"
     ]
    }
   ],
   "source": [
    "for i in range(windows.shape[2]):\n",
    "    signal = pd.DataFrame(windows[:,:,i])\n",
    "    model = EEGNet()\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "    input = torch.from_numpy(signal.values)\n",
    "    input = input.unsqueeze(0)\n",
    "    output = model(input.float())\n",
    "\n",
    "\n",
    "    _, pred = torch.max(output, dim=1)  # 找到预测分数最大的类别，得到预测类别\n",
    "    label_map = {0: 'lefthand', 1:'read' ,  2:'rest', 3: 'walkbase', 4: 'walkl' ,5: 'walkfocus'}\n",
    "    print(label_map[pred.item()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-01T21:19:30.643823Z",
     "end_time": "2023-05-01T21:19:52.549944Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "    #print(output)\n",
    "    # probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    # print(probabilities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T21:55:51.786321Z",
     "end_time": "2023-04-24T21:55:51.807323Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
