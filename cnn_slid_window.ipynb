{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T17:15:31.258767Z",
     "end_time": "2023-04-20T17:15:31.328349Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "#read the model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 4), stride=(1, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(47104, 128)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        #print('x:', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1:', x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2:', x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print('flatten:', x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = EEGNet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T17:15:50.569652Z",
     "end_time": "2023-04-20T17:15:50.826395Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滑窗后信号形状： (32, 3000, 126)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sliding_window(signal, window_size, step_size):\n",
    "    n_channels, n_samples = signal.shape\n",
    "    n_windows = int((n_samples - window_size) / step_size) + 1\n",
    "    windows = np.zeros((n_channels, window_size, n_windows))\n",
    "    for i in range(n_windows):\n",
    "        windows[:, :,i ] = signal.iloc[:, i*step_size:i*step_size+window_size]\n",
    "    return windows\n",
    "\n",
    "signal = pd.read_csv(\"C:\\\\Users\\\\a1882\\Desktop\\EEG\\eegdata\\\\raw\\lefthand_zyy_05_epocflex_2023.03.22t16.50.54+08.00.md.bp.csv\")\n",
    "signal = pd.DataFrame(signal)\n",
    "# 定义滑窗大小和滑动步长\n",
    "window_size = 3000\n",
    "step_size = 100\n",
    "\n",
    "# 对信号进行滑窗处理\n",
    "windows = sliding_window(signal, window_size, step_size)\n",
    "\n",
    "# 输出滑窗后的信号形状\n",
    "print(\"滑窗后信号形状：\", windows.shape)\n",
    "#print(windows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T19:44:16.399020Z",
     "end_time": "2023-04-20T19:44:18.514021Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[ 3.6045e-01, -3.3587e-01, -4.3303e-01, -2.8015e-01]]],\n              \n              \n                      [[[-3.3945e-02,  3.8945e-01, -1.2490e-01,  4.6733e-01]]],\n              \n              \n                      [[[-3.5287e-01, -7.9286e-02,  3.6804e-01, -2.7890e-01]]],\n              \n              \n                      [[[ 4.1774e-01,  6.2350e-02,  3.3992e-01, -2.6406e-02]]],\n              \n              \n                      [[[ 4.3869e-01, -1.7924e-01, -1.1040e-01,  2.3730e-01]]],\n              \n              \n                      [[[-4.4865e-01,  3.9104e-01, -3.4654e-01, -1.9405e-01]]],\n              \n              \n                      [[[-2.7191e-01, -4.5444e-01, -3.2164e-01,  3.6232e-01]]],\n              \n              \n                      [[[-7.7850e-02,  2.9603e-01, -2.5852e-01, -2.5435e-01]]],\n              \n              \n                      [[[-1.9587e-01, -1.7007e-01,  3.4404e-04, -3.0716e-02]]],\n              \n              \n                      [[[-2.6456e-01, -1.9446e-01,  7.0657e-02, -2.1487e-01]]],\n              \n              \n                      [[[ 1.1679e-01, -4.2697e-01,  1.8548e-01,  1.8259e-01]]],\n              \n              \n                      [[[ 1.6125e-02, -4.8060e-01, -2.7345e-01,  3.5043e-01]]],\n              \n              \n                      [[[-3.1807e-01, -4.7198e-01,  1.4607e-01,  2.9734e-02]]],\n              \n              \n                      [[[-1.2500e-01,  2.7638e-01, -2.8795e-01, -4.6024e-01]]],\n              \n              \n                      [[[ 1.3626e-01, -1.5205e-01, -3.7173e-01, -1.9761e-01]]],\n              \n              \n                      [[[-3.2257e-01, -6.1516e-02, -3.5205e-01, -4.7723e-01]]]])),\n             ('conv1.bias',\n              tensor([ 0.0018,  0.0223,  0.4649, -0.1382,  0.3452,  0.2889, -0.3675,  0.2518,\n                       0.3584, -0.4427, -0.3937, -0.2702,  0.4341,  0.4977, -0.2861,  0.3145])),\n             ('bn1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn1.num_batches_tracked', tensor(0)),\n             ('conv2.weight',\n              tensor([[[[ 0.0196,  0.1702]],\n              \n                       [[-0.1048,  0.0094]],\n              \n                       [[ 0.1684,  0.0816]],\n              \n                       ...,\n              \n                       [[ 0.1677, -0.0992]],\n              \n                       [[ 0.1654,  0.1144]],\n              \n                       [[-0.0752, -0.0667]]],\n              \n              \n                      [[[ 0.1434,  0.0155]],\n              \n                       [[ 0.0914,  0.1693]],\n              \n                       [[ 0.0280,  0.1216]],\n              \n                       ...,\n              \n                       [[ 0.0255, -0.1670]],\n              \n                       [[-0.0209, -0.0950]],\n              \n                       [[-0.1059,  0.0384]]],\n              \n              \n                      [[[ 0.1415,  0.1674]],\n              \n                       [[-0.1569,  0.1302]],\n              \n                       [[-0.0517,  0.0782]],\n              \n                       ...,\n              \n                       [[ 0.0901,  0.1660]],\n              \n                       [[ 0.1382, -0.0704]],\n              \n                       [[-0.1162, -0.0306]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.1256, -0.1000]],\n              \n                       [[ 0.0402, -0.1547]],\n              \n                       [[-0.0784, -0.1663]],\n              \n                       ...,\n              \n                       [[ 0.1593,  0.0230]],\n              \n                       [[ 0.0934, -0.1720]],\n              \n                       [[-0.0380,  0.0686]]],\n              \n              \n                      [[[ 0.0437, -0.0416]],\n              \n                       [[-0.0389,  0.1539]],\n              \n                       [[-0.1409,  0.0481]],\n              \n                       ...,\n              \n                       [[-0.1163,  0.1341]],\n              \n                       [[ 0.1514,  0.0296]],\n              \n                       [[-0.0171, -0.0180]]],\n              \n              \n                      [[[ 0.1273, -0.1449]],\n              \n                       [[-0.1223, -0.0679]],\n              \n                       [[-0.0830, -0.1491]],\n              \n                       ...,\n              \n                       [[ 0.0209,  0.0286]],\n              \n                       [[-0.0153,  0.0035]],\n              \n                       [[-0.0355, -0.0412]]]])),\n             ('conv2.bias',\n              tensor([ 0.0144,  0.0733, -0.1055, -0.1025, -0.1324,  0.1464, -0.1627,  0.0272,\n                      -0.0947,  0.0051,  0.0441,  0.1331,  0.0412, -0.1053, -0.0280, -0.0884,\n                       0.0486, -0.0221,  0.1047, -0.0639, -0.1576,  0.1442,  0.0444, -0.0179,\n                       0.1410,  0.0100, -0.0852, -0.0051,  0.0420,  0.1514,  0.0736,  0.1133])),\n             ('bn2.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn2.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn2.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn2.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn2.num_batches_tracked', tensor(0)),\n             ('fc1.weight',\n              tensor([[-0.0028,  0.0044, -0.0041,  ..., -0.0024, -0.0025,  0.0015],\n                      [ 0.0031,  0.0025,  0.0007,  ...,  0.0016, -0.0023,  0.0026],\n                      [ 0.0020, -0.0045, -0.0020,  ..., -0.0025, -0.0042,  0.0036],\n                      ...,\n                      [ 0.0029, -0.0025,  0.0029,  ...,  0.0006,  0.0007,  0.0017],\n                      [-0.0007,  0.0017, -0.0017,  ...,  0.0037, -0.0028,  0.0018],\n                      [-0.0041,  0.0015,  0.0007,  ..., -0.0010,  0.0018,  0.0007]])),\n             ('fc1.bias',\n              tensor([ 1.2786e-03, -1.2429e-03,  1.0559e-03,  3.2028e-03,  2.6618e-03,\n                       1.2143e-04, -9.7507e-04,  4.5469e-03, -2.5354e-03, -2.1819e-03,\n                      -3.3025e-03, -1.6554e-03, -3.2676e-03,  2.5913e-03,  2.6794e-03,\n                       1.3271e-03,  9.8245e-04, -1.8113e-04,  3.5874e-03,  2.4244e-03,\n                      -2.2513e-03,  7.2918e-04,  3.0488e-03, -2.5773e-03,  3.6426e-03,\n                       1.5210e-03,  6.4267e-04,  4.3510e-03,  8.2610e-04,  3.2878e-03,\n                       1.4613e-03,  1.9760e-03,  9.0206e-04, -2.5063e-03, -4.5019e-04,\n                       1.2514e-03,  2.2297e-03,  2.1164e-03,  3.6902e-03, -1.5860e-04,\n                      -6.6454e-04,  2.9837e-04, -1.4189e-03,  1.7899e-03,  1.4279e-03,\n                      -2.2942e-03, -2.8828e-03, -2.2332e-03, -3.4275e-03, -1.8690e-04,\n                      -2.5403e-03,  1.3414e-03, -2.3588e-04, -6.7313e-04,  3.8611e-03,\n                      -4.4606e-03,  4.0978e-03,  1.2448e-03,  4.0726e-05,  3.9533e-03,\n                      -1.3201e-03,  4.4828e-03,  3.0006e-03,  4.0918e-03,  2.6385e-03,\n                       3.4392e-03,  3.3729e-03,  6.8804e-05, -3.1137e-03,  4.3541e-03,\n                       3.9415e-03,  2.8574e-03, -4.2489e-03, -4.4394e-03,  4.2640e-04,\n                       4.1316e-03, -4.5467e-03, -3.2569e-03,  7.2721e-04, -3.4276e-03,\n                       7.0123e-04, -4.5782e-03,  7.1031e-04, -1.2206e-03, -4.4797e-03,\n                      -3.2412e-03, -1.4880e-03, -1.9187e-03, -9.5271e-04, -4.3924e-03,\n                      -1.8737e-03,  2.6903e-03,  3.0594e-03,  1.8866e-03, -2.1060e-03,\n                       2.8802e-03,  4.5982e-03,  4.3147e-03, -2.5862e-03,  2.2357e-03,\n                       9.6489e-05, -4.0757e-03,  4.1656e-03, -8.4389e-05,  2.7363e-03,\n                       2.3608e-03,  2.5898e-03, -7.1305e-04, -2.8383e-03, -1.2814e-03,\n                       1.4628e-03, -7.9080e-04, -7.7260e-04,  4.5000e-04, -1.9490e-04,\n                      -1.3343e-03,  2.8731e-03,  3.5602e-05, -5.0311e-04, -3.7200e-03,\n                       3.3689e-03,  6.6066e-04, -2.5596e-03, -1.9729e-03, -3.4437e-03,\n                       2.0369e-03, -3.9455e-03, -3.1093e-03])),\n             ('fc2.weight',\n              tensor([[-2.3789e-03, -5.6833e-02, -4.1158e-02, -8.6346e-02, -3.1109e-02,\n                        3.8264e-02, -8.3452e-02, -8.9900e-03,  4.9160e-02,  5.4543e-02,\n                       -7.2389e-02, -7.3029e-02,  2.5014e-03, -1.3566e-02,  4.2260e-02,\n                       -8.0808e-02,  1.5649e-02,  7.1283e-02,  4.7094e-02,  2.2050e-02,\n                       -8.6459e-02, -5.7005e-02, -6.8010e-02,  2.4866e-02, -4.2674e-02,\n                       -3.7552e-02, -6.5967e-02, -4.3814e-02, -1.3930e-02,  5.2605e-02,\n                        2.1858e-02, -8.6452e-02,  3.9958e-02, -7.5143e-02, -5.4145e-02,\n                        5.2274e-02,  6.0252e-02, -2.4858e-02,  3.7654e-02,  1.8591e-02,\n                        2.7883e-02,  6.2346e-02, -7.6897e-02,  8.8363e-02, -5.7273e-02,\n                        5.7044e-02,  5.2654e-02, -6.0902e-02,  1.0925e-02,  8.3306e-02,\n                       -1.1223e-03,  4.3122e-03, -2.2047e-02,  2.5565e-03, -2.4391e-02,\n                        6.0938e-02, -3.8559e-02, -3.9783e-02,  2.4604e-02, -5.7943e-02,\n                       -8.1621e-02,  8.6845e-02,  7.5434e-02, -7.8131e-02,  9.9057e-03,\n                        3.7128e-03, -3.8665e-02,  4.1186e-02,  4.6064e-02,  4.8063e-02,\n                       -7.0897e-02,  2.9919e-02,  4.8941e-03, -7.8900e-02, -4.2003e-02,\n                       -7.9358e-02, -8.2002e-02,  6.2967e-02, -2.9669e-02,  3.6169e-02,\n                       -6.8846e-02, -8.2793e-02, -6.2396e-03, -2.8764e-04,  2.3555e-02,\n                       -4.8181e-02, -4.0226e-02, -3.0630e-03, -8.6684e-02,  7.6929e-02,\n                        3.3144e-02,  5.8712e-03,  7.4629e-02, -1.6711e-02, -8.4689e-02,\n                        6.1052e-03,  4.5884e-02, -2.0101e-02, -1.9224e-03, -6.9090e-02,\n                       -7.8604e-02,  7.1748e-02, -7.5462e-03, -4.7453e-02, -1.8791e-02,\n                        6.6249e-02,  4.4039e-03, -4.2261e-02,  5.3712e-02, -3.8869e-02,\n                        6.3339e-02,  5.5760e-02, -7.6193e-02, -6.7158e-02,  8.9513e-03,\n                        8.0993e-02,  1.2809e-02,  2.3696e-02,  9.4958e-03, -7.3982e-02,\n                        1.2696e-02, -7.0478e-02, -5.8453e-02,  6.5784e-03,  4.8696e-03,\n                        4.0029e-02, -6.2419e-03,  7.5473e-03],\n                      [ 7.5400e-04, -7.2584e-02, -2.3734e-03, -8.3037e-02,  2.7634e-02,\n                        5.5027e-02,  2.0507e-02,  3.0547e-02, -2.2864e-02,  5.9420e-02,\n                       -1.9685e-02, -8.1687e-02, -3.8919e-02,  8.4235e-02, -3.4524e-02,\n                        7.3500e-02,  4.2295e-02,  3.4486e-02,  3.8585e-02,  2.3829e-02,\n                       -7.1253e-03, -8.4662e-02, -5.8791e-02,  4.4094e-02, -5.3057e-02,\n                        3.3440e-02, -7.4196e-02,  8.2873e-02, -6.5602e-02,  7.8761e-04,\n                        5.5044e-02, -2.7300e-03,  8.8553e-03,  1.2180e-02,  8.6877e-02,\n                        8.2008e-02,  7.6187e-02, -8.8094e-02,  4.6238e-02,  6.9245e-02,\n                       -4.1532e-02, -2.6755e-02, -8.0590e-02,  1.8349e-02, -1.7682e-02,\n                       -8.3065e-02, -3.9751e-02,  2.9120e-02,  5.8126e-02,  2.1326e-02,\n                       -5.8620e-03, -5.9888e-02,  1.1513e-02,  4.7877e-02, -5.4925e-02,\n                       -5.4782e-03,  1.9080e-02,  1.4117e-02, -5.6776e-02, -6.7218e-02,\n                       -6.9415e-03, -3.9735e-02, -1.2533e-02,  4.2720e-02,  4.8748e-02,\n                       -4.2941e-02, -5.3225e-02,  6.1115e-02, -6.9966e-02,  3.1402e-02,\n                        2.0292e-02,  8.2894e-03,  6.6011e-02,  2.5486e-02, -7.5571e-02,\n                       -7.4366e-03,  6.6112e-02,  7.8614e-02,  5.8163e-02,  5.8884e-02,\n                        3.7566e-02, -4.7582e-02,  6.4616e-03,  5.9729e-02, -6.1977e-02,\n                        1.6499e-02, -5.5120e-02, -8.5538e-02,  8.5391e-02, -4.2519e-02,\n                        3.7379e-02, -8.8262e-02,  7.5785e-02,  3.5342e-02, -6.2104e-02,\n                        5.9191e-02, -5.7542e-02, -5.6880e-02, -1.7242e-02, -5.3714e-02,\n                       -1.8388e-02,  7.7672e-02,  4.8528e-02,  2.2111e-02, -7.0463e-02,\n                        4.0291e-02, -6.5736e-02, -8.6089e-02,  3.4074e-03, -5.1792e-02,\n                        7.1135e-03, -8.0977e-02, -2.3378e-02,  2.2034e-02,  8.7126e-02,\n                        5.1758e-02, -6.6793e-02,  1.6787e-02,  8.8068e-02,  4.1399e-03,\n                        3.5816e-02,  6.9582e-02,  3.4050e-02,  2.7766e-02, -1.4342e-02,\n                       -6.7767e-02,  3.6706e-02, -6.1365e-03],\n                      [-1.2107e-02,  7.1510e-02, -7.5558e-03, -2.0027e-02,  1.5401e-02,\n                       -2.9216e-03,  1.0144e-02,  4.5558e-02, -1.7128e-02, -5.3824e-02,\n                       -3.9868e-02,  2.2835e-02,  2.5891e-02,  3.9889e-02,  6.3049e-02,\n                       -6.3141e-02, -7.0002e-02, -8.5221e-02,  8.5903e-02, -6.1928e-02,\n                        5.0902e-02, -1.0772e-02,  7.2139e-02, -4.6834e-02, -7.3488e-02,\n                        3.6730e-02,  8.3081e-02, -1.5842e-02,  4.7309e-03, -2.0384e-02,\n                        8.8121e-03, -3.3347e-04, -7.4533e-02,  5.0387e-03, -1.3974e-02,\n                       -2.9887e-02, -4.1810e-02, -3.9163e-02, -1.9458e-03, -5.0333e-02,\n                       -7.1932e-02,  3.1078e-02, -1.2397e-02, -6.0028e-02,  4.5959e-02,\n                       -3.3188e-02,  5.0823e-02,  2.4290e-02,  7.5936e-02, -3.9152e-02,\n                        3.7998e-02, -2.1344e-02,  3.6526e-02,  2.6763e-02,  6.7549e-02,\n                        1.4052e-02, -8.9631e-03,  8.2467e-02,  4.8899e-02,  1.9997e-02,\n                        2.9125e-03, -2.9440e-02, -5.2654e-02,  7.7861e-03, -8.0436e-02,\n                       -8.2341e-02,  5.0178e-03, -3.6645e-02,  5.7432e-02,  3.5703e-03,\n                       -7.9437e-02,  3.9439e-02,  3.1253e-02, -6.1062e-02, -1.9198e-02,\n                        2.6746e-03,  3.4855e-03,  3.4037e-03, -3.9664e-02, -8.6519e-02,\n                       -5.6745e-02,  6.9716e-03,  6.5667e-03,  1.8663e-02, -3.9322e-02,\n                       -7.0036e-02,  6.7482e-02,  9.3054e-03,  3.3099e-02,  5.3500e-02,\n                       -6.9199e-03, -2.9646e-02,  3.9527e-02,  4.8720e-02, -1.3004e-02,\n                        6.7721e-03, -6.1441e-02,  5.0682e-02, -4.8155e-02,  6.4870e-02,\n                       -6.1280e-02, -8.1463e-03, -4.5669e-03, -4.9623e-02,  1.6364e-02,\n                        3.1166e-02,  5.2120e-02, -3.9880e-02, -4.1794e-02, -2.6006e-02,\n                       -1.2303e-02,  2.7395e-02, -8.0595e-02, -3.3395e-02, -2.9285e-02,\n                        8.0774e-02,  4.4892e-02,  4.4435e-02, -2.0432e-02, -4.5751e-02,\n                        7.3973e-02,  3.0054e-02, -6.5029e-02,  8.2889e-02,  3.9412e-02,\n                       -2.2200e-02,  7.4053e-02, -6.9272e-02],\n                      [-4.7705e-02,  2.2653e-02,  7.2100e-02, -5.8116e-02,  2.0128e-02,\n                        5.5463e-03, -8.0369e-02, -8.4735e-02, -5.7104e-02, -5.2009e-02,\n                        7.0561e-02, -7.3632e-02, -5.4250e-02,  1.6882e-02, -3.1198e-02,\n                       -7.9845e-02,  5.4125e-02, -3.4412e-02,  1.6997e-02,  1.2919e-03,\n                       -8.0599e-02,  3.3011e-02,  7.9336e-02,  1.0709e-02, -2.3596e-02,\n                       -1.3680e-02, -3.9047e-02, -8.3229e-02,  2.9144e-02,  5.2397e-02,\n                        4.4104e-02, -3.2481e-03,  7.5440e-02, -2.7177e-02, -3.1827e-02,\n                        7.3327e-02,  2.1431e-02,  8.7821e-03,  7.6621e-02, -5.8345e-02,\n                       -4.2207e-02, -3.7841e-02,  2.6660e-02, -8.1187e-03, -7.6503e-02,\n                        3.8306e-02,  4.0447e-02, -3.6277e-02, -1.3614e-02, -4.8365e-02,\n                        3.8534e-02, -7.3105e-02, -5.1248e-02,  4.9910e-03,  7.2454e-02,\n                        1.3031e-02,  3.3246e-02, -3.9931e-02,  4.7236e-02, -2.3938e-02,\n                        4.7659e-02,  4.0459e-03, -7.2099e-02, -8.3449e-02, -9.5043e-04,\n                       -4.7134e-02,  6.3386e-02,  1.6160e-02, -8.1130e-02, -6.3910e-03,\n                        8.3669e-02, -1.0978e-02,  2.5637e-02, -2.0272e-02,  4.2654e-03,\n                        6.6003e-02,  6.3282e-02, -7.6452e-03, -3.3175e-02, -9.1945e-03,\n                        1.0901e-03,  2.4517e-02, -1.9731e-03,  2.0892e-02, -3.9523e-02,\n                        1.9851e-02,  5.8816e-02, -1.3597e-02,  5.0408e-02, -2.9388e-02,\n                       -6.8301e-02,  2.3077e-02, -2.3187e-02,  4.1792e-02,  3.8690e-03,\n                       -5.9126e-03, -6.2947e-02, -4.0124e-02,  3.2578e-02, -1.1112e-02,\n                        2.1002e-02,  4.2352e-03,  2.2515e-02,  8.0497e-02, -3.1629e-02,\n                       -5.8747e-02, -3.2081e-02,  8.5571e-02,  6.5632e-02,  1.0900e-02,\n                       -4.3920e-02,  5.9209e-02, -5.2087e-02,  6.9613e-02, -6.7046e-02,\n                        5.6520e-02,  7.3347e-02, -8.1742e-02,  8.1099e-02,  6.7932e-02,\n                        7.5378e-02, -8.0257e-03, -2.8829e-02,  5.7109e-02,  3.6024e-02,\n                       -1.7965e-02, -6.3363e-03,  7.7928e-02],\n                      [-8.5477e-02,  4.2693e-02, -7.8141e-02,  3.3750e-02,  4.5358e-02,\n                       -6.8452e-02,  7.1681e-02, -7.0955e-02,  4.4118e-02,  1.0604e-02,\n                        3.5646e-02, -2.8450e-02, -8.7325e-02, -8.5805e-02, -4.6678e-02,\n                        5.7211e-02, -8.4604e-02, -1.9728e-02, -6.1325e-02, -4.0153e-02,\n                       -6.9299e-02,  5.9783e-02, -6.2934e-02, -6.5433e-02,  5.6151e-02,\n                        3.0415e-02, -5.7599e-02,  3.1820e-02,  1.8861e-02, -3.8459e-02,\n                       -6.4655e-02,  6.0343e-02, -2.3514e-02, -5.7727e-02,  4.3182e-02,\n                        5.5834e-02, -3.8529e-02, -3.3633e-02, -4.7790e-03, -4.6435e-02,\n                       -7.6344e-02, -6.3562e-02,  4.2784e-02,  3.7963e-02, -3.3096e-02,\n                       -4.5972e-02,  7.0306e-02, -6.2894e-03,  1.5455e-02, -2.2179e-02,\n                        6.5011e-02,  1.4448e-02,  2.1688e-02, -7.4061e-02, -2.1795e-02,\n                       -1.2356e-02,  5.6768e-02,  6.9625e-02, -6.4194e-02,  4.3297e-03,\n                       -7.9464e-02,  8.7441e-02, -1.2550e-02, -5.9381e-02,  5.7919e-02,\n                        7.9905e-02,  4.5387e-02, -7.4166e-02, -7.2969e-02, -5.3636e-02,\n                       -6.2031e-02, -7.2013e-02,  2.1727e-02, -4.1272e-02, -1.1318e-02,\n                       -5.4798e-02, -5.6850e-02, -4.0224e-02,  1.1647e-02, -3.0474e-02,\n                       -3.8317e-02, -6.9698e-02, -8.3868e-02, -1.8070e-03,  8.7279e-02,\n                        5.1342e-03, -6.1677e-02,  2.6825e-02,  3.5280e-02,  1.3875e-02,\n                        1.1131e-02, -1.7654e-02, -5.5831e-02,  7.9498e-02, -9.2336e-03,\n                       -3.9024e-02,  8.4309e-02, -6.4575e-02, -2.0567e-02, -8.4344e-02,\n                        6.7230e-02, -2.1189e-02,  8.3059e-03,  3.6392e-02,  1.1072e-02,\n                       -8.0416e-02, -7.3223e-02,  1.0186e-02, -5.2342e-02,  5.5671e-02,\n                        6.8486e-02, -4.1096e-02, -3.4461e-03, -4.0724e-02,  2.9019e-02,\n                        6.9495e-02,  1.1232e-02,  1.5391e-02,  7.2666e-02, -4.6514e-02,\n                       -3.3588e-02, -5.2789e-02,  2.2542e-02,  4.3262e-02, -8.4607e-02,\n                       -3.8553e-02, -8.6566e-02,  5.6514e-02],\n                      [ 8.1248e-02, -3.1096e-02, -1.8731e-02, -5.0742e-02, -6.7699e-02,\n                       -4.0399e-02, -4.0942e-02, -3.5371e-02, -7.1105e-02, -6.7858e-02,\n                       -2.7508e-02, -2.1859e-02,  7.8890e-02, -2.9276e-02, -7.4079e-02,\n                        8.1681e-02, -7.1099e-02, -5.7432e-02, -2.7577e-02,  4.9660e-02,\n                        5.9347e-02, -6.8339e-02, -5.7327e-02, -8.7465e-02,  9.4104e-03,\n                       -2.6026e-03, -5.9909e-03,  3.5485e-03,  4.7088e-05,  8.5461e-02,\n                        9.4181e-03,  7.3113e-02, -2.0700e-02,  3.5654e-02, -9.7784e-03,\n                       -3.7029e-02, -7.5013e-02, -6.4718e-02, -1.5234e-02, -4.6938e-02,\n                        6.0300e-02,  7.6692e-02,  5.6088e-02, -4.1725e-02, -8.1358e-02,\n                        8.2094e-02, -6.7093e-02, -8.1232e-02,  7.9076e-02,  4.7505e-02,\n                        2.2503e-03, -2.6965e-02,  1.2057e-02,  4.7439e-02,  6.9732e-03,\n                        1.2181e-02, -1.5894e-03,  5.1986e-02,  5.9711e-02, -5.3753e-02,\n                        7.5889e-02, -5.3762e-02,  6.6725e-02,  1.9926e-02, -5.5934e-02,\n                       -5.4011e-03,  3.8018e-02, -8.6862e-03, -6.1605e-02,  2.2199e-02,\n                        7.3516e-02,  3.4396e-03,  7.1095e-02,  2.3475e-02,  4.8326e-03,\n                       -4.4425e-02, -7.6312e-02,  6.0883e-02,  7.6999e-02,  5.0030e-02,\n                       -5.6485e-02, -5.3122e-02,  1.0829e-02, -2.0556e-02, -1.9172e-02,\n                       -1.2678e-02,  3.6839e-02,  1.6637e-02,  5.7740e-02, -6.9029e-02,\n                        2.9169e-02,  4.0256e-02, -8.4771e-02,  5.5898e-02,  8.1667e-02,\n                       -7.1247e-02, -7.1186e-02,  7.2169e-02,  2.3762e-02, -2.6187e-02,\n                       -3.6630e-02, -7.4655e-02, -8.1885e-02,  7.9674e-02, -5.3983e-02,\n                       -1.0231e-02,  2.5171e-02,  7.2959e-02,  7.3663e-02, -4.6116e-02,\n                       -6.7561e-03, -4.5640e-02,  1.7692e-02,  1.8771e-02,  7.3012e-02,\n                       -2.8331e-02, -6.7477e-02,  8.2675e-02, -3.1065e-03,  8.7756e-02,\n                       -1.9078e-03,  5.8186e-02,  1.9034e-02,  5.4540e-02,  4.2045e-02,\n                        7.0648e-02, -2.6453e-02,  2.3341e-02]])),\n             ('fc2.bias',\n              tensor([-0.0824, -0.0092, -0.0641,  0.0699, -0.0010,  0.0034]))])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "save_path = './model_saved/cnn_3000_100e_1.pt'\n",
    "\n",
    "onnx_file_name = \"./cnn_3000_100e.onnx\"\n",
    "torch.load(save_path, map_location=torch.device('cpu'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T19:44:19.402327Z",
     "end_time": "2023-04-20T19:44:19.656064Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is valid!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "# 我们可以使用异常处理的方法进行检验\n",
    "try:\n",
    "    # 当我们的模型不可用时，将会报出异常\n",
    "    onnx.checker.check_model(onnx_file_name)\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(\"The model is invalid: %s\"%e)\n",
    "else:\n",
    "    # 模型可用时，将不会报出异常，并会输出“The model is valid!”\n",
    "    print(\"The model is valid!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T19:44:20.772720Z",
     "end_time": "2023-04-20T19:44:21.529084Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "2\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "0\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n",
      "1\n",
      "4\n",
      "5\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "5\n",
      "1\n",
      "5\n",
      "5\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(windows.shape[2]):\n",
    "    signal = pd.DataFrame(windows[:,:,i])\n",
    "    model = EEGNet()\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "    input = torch.from_numpy(signal.values)\n",
    "    input = input.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input.float())\n",
    "        #print(output)\n",
    "    # probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    # print(probabilities)\n",
    "\n",
    "    _, pred = torch.max(output, dim=1)  # 找到预测分数最大的类别，得到预测类别\n",
    "    print(pred.item())  # 输出预测类别的整数值"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T19:44:21.953105Z",
     "end_time": "2023-04-20T19:44:36.095272Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
