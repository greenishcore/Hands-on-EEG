{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T10:38:14.965087Z",
     "end_time": "2023-04-25T10:38:22.216907Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#read the model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 4), stride=(1, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(47104, 128)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        #print('x:', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1:', x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2:', x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print('flatten:', x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = EEGNet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T21:55:35.753628Z",
     "end_time": "2023-04-24T21:55:35.897665Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滑窗后信号形状： (32, 3000, 126)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sliding_window(signal, window_size, step_size):\n",
    "    n_channels, n_samples = signal.shape\n",
    "    n_windows = int((n_samples - window_size) / step_size) + 1\n",
    "    windows = np.zeros((n_channels, window_size, n_windows))\n",
    "    for i in range(n_windows):\n",
    "        windows[:, :,i ] = signal.iloc[:, i*step_size:i*step_size+window_size]\n",
    "    return windows\n",
    "\n",
    "signal = pd.read_csv(\"C:\\\\Users\\\\a1882\\Desktop\\EEG\\eegdata\\\\raw\\lefthand_zyy_05_epocflex_2023.03.22t16.50.54+08.00.md.bp.csv\")\n",
    "signal = pd.DataFrame(signal)\n",
    "# 定义滑窗大小和滑动步长\n",
    "window_size = 3000\n",
    "step_size = 100\n",
    "\n",
    "# 对信号进行滑窗处理\n",
    "windows = sliding_window(signal, window_size, step_size)\n",
    "\n",
    "# 输出滑窗后的信号形状\n",
    "print(\"滑窗后信号形状：\", windows.shape)\n",
    "#print(windows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T21:55:35.902660Z",
     "end_time": "2023-04-24T21:55:38.132679Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[ 0.4752,  0.3993, -0.2588,  0.1631]]],\n              \n              \n                      [[[-0.4077,  0.4999, -0.1816, -0.0157]]],\n              \n              \n                      [[[-0.1810, -0.4854,  0.3795,  0.3577]]],\n              \n              \n                      [[[ 0.0896, -0.1243,  0.3935,  0.0694]]],\n              \n              \n                      [[[ 0.1553, -0.2519,  0.2261,  0.3780]]],\n              \n              \n                      [[[ 0.3069,  0.2198,  0.2584, -0.4192]]],\n              \n              \n                      [[[-0.4673,  0.3854,  0.2515, -0.4788]]],\n              \n              \n                      [[[ 0.3410, -0.4043,  0.4628,  0.0794]]],\n              \n              \n                      [[[ 0.0493, -0.0526, -0.4163,  0.4766]]],\n              \n              \n                      [[[-0.3292,  0.4598,  0.1505,  0.4126]]],\n              \n              \n                      [[[ 0.2993,  0.3773,  0.1083, -0.3544]]],\n              \n              \n                      [[[ 0.0700, -0.2947, -0.3477,  0.1264]]],\n              \n              \n                      [[[ 0.1487,  0.0689,  0.3196, -0.4107]]],\n              \n              \n                      [[[-0.3411, -0.1821,  0.1987,  0.3914]]],\n              \n              \n                      [[[-0.4640,  0.4197, -0.2027,  0.4967]]],\n              \n              \n                      [[[-0.2769, -0.2451, -0.0119,  0.4158]]]])),\n             ('conv1.bias',\n              tensor([ 0.4147,  0.0937, -0.3493,  0.3023,  0.4735, -0.3000, -0.4889,  0.1260,\n                       0.3802,  0.4967,  0.2991,  0.0239,  0.0911, -0.2784,  0.1480,  0.4482])),\n             ('bn1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn1.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn1.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn1.num_batches_tracked', tensor(0)),\n             ('conv2.weight',\n              tensor([[[[-0.0687, -0.1441]],\n              \n                       [[-0.0223, -0.0274]],\n              \n                       [[-0.1671,  0.0325]],\n              \n                       ...,\n              \n                       [[-0.1507,  0.1261]],\n              \n                       [[ 0.0840,  0.0287]],\n              \n                       [[-0.0842,  0.0936]]],\n              \n              \n                      [[[-0.0109, -0.0963]],\n              \n                       [[-0.0982,  0.1403]],\n              \n                       [[ 0.1493, -0.0844]],\n              \n                       ...,\n              \n                       [[ 0.0152,  0.1443]],\n              \n                       [[ 0.0919,  0.0692]],\n              \n                       [[ 0.1254,  0.1431]]],\n              \n              \n                      [[[-0.1232, -0.0433]],\n              \n                       [[ 0.1694,  0.0631]],\n              \n                       [[-0.1315, -0.0075]],\n              \n                       ...,\n              \n                       [[-0.0335,  0.0695]],\n              \n                       [[ 0.0065, -0.1617]],\n              \n                       [[ 0.1226,  0.1423]]],\n              \n              \n                      ...,\n              \n              \n                      [[[ 0.0254,  0.1598]],\n              \n                       [[-0.1284,  0.0718]],\n              \n                       [[-0.0968,  0.0889]],\n              \n                       ...,\n              \n                       [[ 0.0979, -0.1195]],\n              \n                       [[-0.0589, -0.1375]],\n              \n                       [[ 0.1421, -0.0925]]],\n              \n              \n                      [[[-0.0570, -0.1017]],\n              \n                       [[-0.1459,  0.0035]],\n              \n                       [[ 0.0964,  0.0373]],\n              \n                       ...,\n              \n                       [[-0.0127,  0.1742]],\n              \n                       [[-0.0242, -0.0457]],\n              \n                       [[-0.0013,  0.0385]]],\n              \n              \n                      [[[-0.1360, -0.0647]],\n              \n                       [[-0.1553, -0.1193]],\n              \n                       [[-0.1105, -0.0467]],\n              \n                       ...,\n              \n                       [[-0.0881, -0.0741]],\n              \n                       [[-0.0026, -0.0231]],\n              \n                       [[ 0.0104,  0.0053]]]])),\n             ('conv2.bias',\n              tensor([ 0.0701, -0.0905, -0.0600, -0.0712,  0.1099, -0.1610, -0.1493,  0.0411,\n                      -0.1459,  0.0009,  0.1767,  0.0378,  0.0523, -0.0884, -0.1347,  0.1038,\n                       0.1250, -0.0579, -0.1218, -0.0667, -0.0286,  0.0049, -0.0461, -0.1110,\n                      -0.0017,  0.0255,  0.0673, -0.1698, -0.1571, -0.0279,  0.0113, -0.0581])),\n             ('bn2.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn2.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn2.running_mean',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('bn2.running_var',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('bn2.num_batches_tracked', tensor(0)),\n             ('fc1.weight',\n              tensor([[ 5.5978e-04,  3.3194e-03,  6.2149e-04,  ..., -2.6678e-03,\n                       -9.3530e-04,  1.1301e-03],\n                      [-1.8353e-03,  2.0234e-03,  3.1785e-03,  ..., -4.2503e-03,\n                       -2.9489e-03, -3.9660e-03],\n                      [-1.3346e-03, -1.5227e-03,  2.9241e-03,  ..., -1.4098e-03,\n                       -1.0977e-03,  1.0456e-03],\n                      ...,\n                      [ 2.1732e-03,  4.3298e-03,  3.5522e-03,  ...,  2.5167e-03,\n                        1.5922e-05,  5.4845e-04],\n                      [-2.2551e-03,  2.0497e-03,  4.4097e-03,  ...,  1.4916e-03,\n                       -1.5455e-03,  3.8498e-03],\n                      [ 2.0061e-03,  1.0685e-03, -2.3898e-03,  ...,  1.7373e-03,\n                       -2.5606e-04, -8.9928e-04]])),\n             ('fc1.bias',\n              tensor([-4.4968e-04,  3.1310e-03,  1.6352e-03, -3.9867e-03, -2.2258e-03,\n                       2.6301e-04, -1.6015e-03,  4.2839e-03,  4.2929e-03,  1.5505e-03,\n                       9.7621e-04, -1.2121e-03,  1.2334e-03, -4.1667e-03,  3.0519e-04,\n                       1.8934e-03, -1.0369e-03, -3.4767e-03, -3.2580e-03, -3.6072e-03,\n                       2.3602e-03, -3.4876e-03,  2.0439e-03, -4.2066e-03,  2.5821e-03,\n                       3.4594e-03,  2.7211e-03,  2.9007e-03, -3.3142e-03,  4.5057e-03,\n                       2.2601e-03, -2.4345e-03, -7.0395e-04, -1.7463e-03, -2.4585e-03,\n                      -3.1870e-03, -3.8338e-03, -1.5219e-03,  2.5064e-03, -4.5677e-04,\n                       1.7360e-03, -4.5754e-03,  3.3602e-03, -3.6705e-04, -8.3933e-04,\n                      -4.8872e-04,  4.1755e-04,  9.1798e-04,  1.8685e-03, -1.3132e-03,\n                       2.7437e-03,  2.5878e-03, -4.3185e-03, -9.3870e-04, -2.2715e-03,\n                      -1.4576e-03, -1.3924e-03,  2.1425e-03,  1.8700e-03,  4.4870e-03,\n                       1.1560e-03, -1.7569e-04, -3.3712e-03,  1.0817e-03, -2.9530e-03,\n                       2.4212e-04,  4.3750e-04, -4.4602e-03,  3.8460e-04,  1.7981e-03,\n                      -4.3931e-03, -3.0416e-04, -3.8729e-03, -1.7400e-03,  7.7624e-04,\n                      -2.5998e-03, -2.2693e-04,  4.3279e-03, -4.7313e-04, -1.1674e-03,\n                      -8.1517e-04, -3.9582e-05,  4.2577e-03, -4.4303e-03,  3.5883e-03,\n                      -2.8867e-03,  4.0399e-03, -2.5969e-03, -1.9761e-03, -2.6632e-03,\n                       3.6649e-03,  2.6678e-03, -1.2529e-03,  8.7317e-04, -4.3343e-03,\n                       7.4509e-04,  4.5272e-03, -3.7791e-03,  1.3388e-03, -3.0594e-03,\n                      -2.1039e-03, -4.4996e-04,  2.7750e-03, -1.7025e-03,  1.9251e-03,\n                      -2.9506e-03,  2.4093e-03, -1.3630e-03, -4.3059e-03,  2.2901e-03,\n                      -1.3260e-03,  4.5320e-03,  2.3318e-03,  9.6639e-04,  4.1304e-03,\n                       5.1138e-04,  7.9092e-04,  3.9361e-03,  3.1293e-03,  2.9174e-03,\n                       1.9014e-03, -2.9047e-03, -4.3118e-03, -4.0633e-03,  1.3460e-03,\n                       1.3549e-03, -1.6674e-04, -4.1243e-03])),\n             ('fc2.weight',\n              tensor([[-0.0069, -0.0477,  0.0532, -0.0849,  0.0380, -0.0815, -0.0714, -0.0495,\n                        0.0483,  0.0687,  0.0745, -0.0289,  0.0762,  0.0087, -0.0108, -0.0413,\n                        0.0202,  0.0259, -0.0485, -0.0021, -0.0144, -0.0700,  0.0635,  0.0689,\n                       -0.0539,  0.0820, -0.0322,  0.0180, -0.0621,  0.0020,  0.0595, -0.0787,\n                        0.0812, -0.0091,  0.0711,  0.0199,  0.0025, -0.0323,  0.0734,  0.0696,\n                        0.0727, -0.0442, -0.0081, -0.0557, -0.0453, -0.0497, -0.0664,  0.0325,\n                        0.0529, -0.0731,  0.0393, -0.0511,  0.0863, -0.0879, -0.0675, -0.0744,\n                        0.0769,  0.0231, -0.0242,  0.0541, -0.0056, -0.0410, -0.0455, -0.0124,\n                        0.0374, -0.0314, -0.0871,  0.0875, -0.0733,  0.0457,  0.0786,  0.0604,\n                       -0.0159, -0.0115,  0.0788,  0.0261,  0.0799,  0.0124, -0.0417, -0.0037,\n                       -0.0232,  0.0197, -0.0676,  0.0858, -0.0736, -0.0503, -0.0771,  0.0599,\n                       -0.0363,  0.0187, -0.0189, -0.0399, -0.0679,  0.0825, -0.0394,  0.0224,\n                        0.0196, -0.0626, -0.0301, -0.0841, -0.0489, -0.0287, -0.0209,  0.0411,\n                        0.0266,  0.0315, -0.0790, -0.0136, -0.0014,  0.0636, -0.0868,  0.0214,\n                       -0.0622,  0.0481,  0.0088, -0.0223,  0.0468, -0.0132, -0.0164, -0.0109,\n                       -0.0558,  0.0424,  0.0133, -0.0211, -0.0511, -0.0321,  0.0812,  0.0339],\n                      [ 0.0077, -0.0330,  0.0320, -0.0622,  0.0388, -0.0849, -0.0688,  0.0124,\n                        0.0179, -0.0808, -0.0516, -0.0820,  0.0626,  0.0674,  0.0498,  0.0621,\n                        0.0234, -0.0006,  0.0152, -0.0341,  0.0348,  0.0448, -0.0257,  0.0683,\n                       -0.0883, -0.0583, -0.0145, -0.0466, -0.0109, -0.0470,  0.0165,  0.0417,\n                       -0.0598, -0.0606,  0.0297,  0.0372,  0.0162,  0.0482,  0.0653,  0.0527,\n                        0.0614,  0.0010,  0.0062, -0.0776, -0.0096, -0.0762,  0.0504, -0.0086,\n                        0.0035,  0.0146,  0.0089,  0.0537, -0.0154, -0.0626,  0.0351,  0.0131,\n                        0.0155, -0.0181, -0.0670,  0.0128,  0.0754,  0.0262,  0.0142,  0.0240,\n                        0.0176, -0.0287, -0.0821,  0.0164,  0.0472, -0.0411, -0.0105, -0.0598,\n                       -0.0689,  0.0475,  0.0737, -0.0764, -0.0590,  0.0338,  0.0229,  0.0595,\n                        0.0004,  0.0617, -0.0846,  0.0155,  0.0036,  0.0499, -0.0613, -0.0168,\n                       -0.0046, -0.0457, -0.0837,  0.0285,  0.0818, -0.0361, -0.0519, -0.0017,\n                       -0.0073, -0.0849,  0.0267, -0.0248,  0.0513,  0.0609,  0.0422, -0.0126,\n                        0.0681, -0.0166,  0.0428,  0.0719,  0.0013,  0.0483, -0.0452, -0.0087,\n                       -0.0797, -0.0193, -0.0605, -0.0359,  0.0496, -0.0190,  0.0417, -0.0322,\n                       -0.0757,  0.0313, -0.0088, -0.0077,  0.0878, -0.0530, -0.0227,  0.0858],\n                      [-0.0845,  0.0042,  0.0784, -0.0385, -0.0183,  0.0042,  0.0793,  0.0427,\n                        0.0218, -0.0764, -0.0344, -0.0408, -0.0317,  0.0624,  0.0133, -0.0362,\n                       -0.0043,  0.0218, -0.0664,  0.0858, -0.0804, -0.0812, -0.0833, -0.0089,\n                       -0.0398, -0.0706, -0.0639,  0.0758,  0.0191,  0.0147, -0.0859, -0.0377,\n                        0.0391, -0.0232,  0.0114,  0.0515,  0.0152,  0.0483,  0.0725, -0.0420,\n                        0.0656,  0.0554, -0.0648, -0.0397,  0.0164,  0.0006, -0.0169,  0.0271,\n                       -0.0035,  0.0523, -0.0370, -0.0870, -0.0043,  0.0804, -0.0397, -0.0800,\n                        0.0039,  0.0553,  0.0109,  0.0029, -0.0268,  0.0434, -0.0585, -0.0816,\n                        0.0073, -0.0382,  0.0716, -0.0017, -0.0525, -0.0335, -0.0431, -0.0881,\n                        0.0132,  0.0437, -0.0451,  0.0722,  0.0370, -0.0640,  0.0629, -0.0192,\n                       -0.0114,  0.0293,  0.0598,  0.0496,  0.0431, -0.0423,  0.0806, -0.0261,\n                       -0.0396, -0.0481,  0.0454, -0.0342,  0.0461, -0.0517,  0.0418,  0.0299,\n                       -0.0570,  0.0114, -0.0298,  0.0547,  0.0709,  0.0284, -0.0867,  0.0506,\n                        0.0284,  0.0160, -0.0861, -0.0226, -0.0883, -0.0493,  0.0602,  0.0644,\n                       -0.0457, -0.0331,  0.0358,  0.0695,  0.0780, -0.0692, -0.0160,  0.0030,\n                        0.0306, -0.0628, -0.0761,  0.0756, -0.0815,  0.0063,  0.0118, -0.0497],\n                      [ 0.0449,  0.0722,  0.0578,  0.0334, -0.0538, -0.0144, -0.0678,  0.0304,\n                       -0.0123, -0.0240,  0.0652,  0.0136,  0.0846,  0.0673,  0.0683,  0.0857,\n                        0.0223, -0.0851, -0.0243,  0.0338,  0.0046,  0.0043,  0.0124,  0.0791,\n                        0.0446, -0.0409, -0.0163, -0.0426, -0.0404, -0.0753,  0.0514,  0.0418,\n                        0.0623, -0.0025,  0.0699,  0.0597,  0.0292,  0.0072, -0.0570,  0.0001,\n                        0.0527, -0.0651,  0.0838, -0.0165, -0.0460,  0.0737,  0.0849, -0.0422,\n                        0.0439, -0.0808, -0.0039, -0.0049,  0.0106, -0.0421, -0.0845, -0.0838,\n                        0.0822, -0.0637, -0.0298,  0.0383,  0.0591, -0.0188, -0.0687, -0.0609,\n                       -0.0536,  0.0676, -0.0742,  0.0643,  0.0379, -0.0013, -0.0122, -0.0351,\n                        0.0271,  0.0796, -0.0093,  0.0240,  0.0521, -0.0512,  0.0386, -0.0838,\n                       -0.0530,  0.0574, -0.0639, -0.0640,  0.0253, -0.0546, -0.0876,  0.0800,\n                        0.0608, -0.0439,  0.0373,  0.0829, -0.0114,  0.0789,  0.0500,  0.0801,\n                        0.0050,  0.0396,  0.0281, -0.0858,  0.0140,  0.0541, -0.0181,  0.0853,\n                       -0.0454,  0.0585,  0.0140, -0.0029, -0.0117, -0.0196, -0.0048,  0.0100,\n                        0.0005, -0.0396, -0.0666, -0.0267, -0.0838,  0.0424,  0.0120,  0.0191,\n                        0.0157, -0.0167, -0.0451,  0.0626,  0.0097, -0.0527, -0.0533,  0.0649],\n                      [-0.0378, -0.0011, -0.0225, -0.0680, -0.0582,  0.0238,  0.0145, -0.0010,\n                       -0.0860,  0.0830,  0.0208,  0.0223, -0.0790,  0.0691,  0.0782, -0.0770,\n                       -0.0549,  0.0690, -0.0812, -0.0234, -0.0564,  0.0651, -0.0716,  0.0012,\n                       -0.0106,  0.0760,  0.0282, -0.0336,  0.0451, -0.0276,  0.0245, -0.0447,\n                       -0.0592,  0.0262,  0.0752,  0.0690,  0.0744,  0.0090,  0.0272, -0.0626,\n                       -0.0203, -0.0570,  0.0417, -0.0176,  0.0378,  0.0548, -0.0423, -0.0642,\n                       -0.0474, -0.0558,  0.0874, -0.0467,  0.0565, -0.0705, -0.0794, -0.0578,\n                        0.0213,  0.0812,  0.0673, -0.0229,  0.0565, -0.0837,  0.0486,  0.0065,\n                        0.0503, -0.0459, -0.0779,  0.0418,  0.0259,  0.0159,  0.0252, -0.0360,\n                        0.0648,  0.0550,  0.0291, -0.0565, -0.0207, -0.0330,  0.0213,  0.0379,\n                       -0.0599, -0.0418, -0.0335,  0.0023,  0.0148, -0.0320,  0.0283, -0.0783,\n                        0.0874,  0.0254, -0.0869,  0.0548,  0.0675,  0.0840,  0.0302,  0.0049,\n                        0.0259, -0.0371,  0.0850, -0.0576,  0.0460, -0.0318, -0.0765,  0.0855,\n                        0.0636, -0.0463,  0.0515,  0.0466, -0.0485,  0.0204, -0.0023, -0.0597,\n                       -0.0379, -0.0295, -0.0170,  0.0475,  0.0866,  0.0517, -0.0166, -0.0775,\n                       -0.0393,  0.0074,  0.0264,  0.0865, -0.0225, -0.0108,  0.0434,  0.0273],\n                      [-0.0157,  0.0565,  0.0675,  0.0531, -0.0603, -0.0298,  0.0379,  0.0784,\n                        0.0504,  0.0129,  0.0278, -0.0024,  0.0721, -0.0294, -0.0240,  0.0369,\n                        0.0195, -0.0787,  0.0745, -0.0704, -0.0159,  0.0272,  0.0475,  0.0139,\n                        0.0010,  0.0484, -0.0501, -0.0665, -0.0434,  0.0826,  0.0611, -0.0459,\n                       -0.0350, -0.0735, -0.0615, -0.0512,  0.0503, -0.0777,  0.0044,  0.0324,\n                       -0.0529,  0.0870,  0.0553, -0.0696,  0.0860, -0.0516,  0.0271, -0.0647,\n                        0.0587,  0.0742, -0.0551, -0.0058, -0.0179,  0.0617, -0.0221,  0.0055,\n                       -0.0363, -0.0360,  0.0039,  0.0471,  0.0426, -0.0415, -0.0391,  0.0134,\n                       -0.0082,  0.0068, -0.0193,  0.0822,  0.0054,  0.0285,  0.0198,  0.0525,\n                        0.0842, -0.0314, -0.0303,  0.0717,  0.0672, -0.0130, -0.0558,  0.0413,\n                        0.0248, -0.0474, -0.0406, -0.0756,  0.0417, -0.0203, -0.0224, -0.0441,\n                        0.0287, -0.0391, -0.0210, -0.0877, -0.0702, -0.0262,  0.0759,  0.0819,\n                       -0.0826,  0.0700,  0.0586, -0.0085, -0.0549,  0.0336, -0.0637, -0.0469,\n                        0.0684, -0.0596, -0.0366, -0.0246, -0.0078, -0.0192,  0.0053, -0.0253,\n                       -0.0280,  0.0817,  0.0611, -0.0849,  0.0063, -0.0559, -0.0776, -0.0542,\n                       -0.0842,  0.0023, -0.0716, -0.0691,  0.0805,  0.0188,  0.0526,  0.0104]])),\n             ('fc2.bias',\n              tensor([ 0.0775, -0.0174,  0.0639,  0.0270,  0.0197,  0.0344]))])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "save_path = './model_saved/cnn_3000_100e_1.pt'\n",
    "\n",
    "onnx_file_name = \"./cnn_3000_100e.onnx\"\n",
    "torch.load(save_path, map_location=torch.device('cpu'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T21:55:38.136684Z",
     "end_time": "2023-04-24T21:55:38.313234Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is valid!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "# 我们可以使用异常处理的方法进行检验\n",
    "try:\n",
    "    # 当我们的模型不可用时，将会报出异常\n",
    "    onnx.checker.check_model(onnx_file_name)\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(\"The model is invalid: %s\"%e)\n",
    "else:\n",
    "    # 模型可用时，将不会报出异常，并会输出“The model is valid!”\n",
    "    print(\"The model is valid!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T21:55:38.316234Z",
     "end_time": "2023-04-24T21:55:38.806156Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkl\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkl\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "rest\n",
      "walkbase\n",
      "walkbase\n",
      "walkl\n",
      "walkbase\n",
      "walkl\n",
      "walkl\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "read\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "lefthand\n",
      "lefthand\n",
      "read\n",
      "walkbase\n",
      "walkfocus\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkbase\n",
      "walkl\n",
      "walkbase\n",
      "walkbase\n"
     ]
    }
   ],
   "source": [
    "for i in range(windows.shape[2]):\n",
    "    signal = pd.DataFrame(windows[:,:,i])\n",
    "    model = EEGNet()\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "    input = torch.from_numpy(signal.values)\n",
    "    input = input.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input.float())\n",
    "        #print(output)\n",
    "    # probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    # print(probabilities)\n",
    "\n",
    "    _, pred = torch.max(output, dim=1)  # 找到预测分数最大的类别，得到预测类别\n",
    "    label_map = {0: 'lefthand', 1:'read' ,  2:'rest', 3: 'walkbase', 4: 'walkl' ,5: 'walkfocus'}\n",
    "    print(label_map[pred.item()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T21:55:38.798158Z",
     "end_time": "2023-04-24T21:55:51.796312Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T21:55:51.786321Z",
     "end_time": "2023-04-24T21:55:51.807323Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
