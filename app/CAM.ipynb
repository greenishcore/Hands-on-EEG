{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-14T11:20:29.541779300Z",
     "start_time": "2023-05-14T11:20:27.966301700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "#read the model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(2, 1), stride=(2, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout3 = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        # print('x:', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print('conv1:', x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        # print('pool1:', x.shape)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        # print('conv2:', x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        # print('pool2:', x.shape)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        # print('conv3:',x.shape)\n",
    "        x = self.bn3(x)\n",
    "        # x = torch.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        # print('pool3:',x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print('flatten:', x.shape)\n",
    "        x = self.fc1(x)\n",
    "        # print('fc1:', x.shape)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        # print('fc2:', x.shape)\n",
    "        return x\n",
    "model = EEGNet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T12:21:42.980131400Z",
     "start_time": "2023-05-14T12:21:42.878567400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滑窗后信号形状： (32, 128, 155)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sliding_window(signal, window_size, step_size):\n",
    "    n_channels, n_samples = signal.shape\n",
    "    n_windows = int((n_samples - window_size) / step_size) + 1\n",
    "    windows = np.zeros((n_channels, window_size, n_windows))\n",
    "    for i in range(n_windows):\n",
    "        windows[:, :,i ] = signal.iloc[:, i*step_size:i*step_size+window_size]\n",
    "    return windows\n",
    "\n",
    "signal = pd.read_csv(\"C:\\\\Users\\\\a1882\\Desktop\\EEG\\eegdata\\\\raw\\lefthand_zyy_05_epocflex_2023.03.22t16.50.54+08.00.md.bp.csv\")\n",
    "signal = pd.DataFrame(signal)\n",
    "# 定义滑窗大小和滑动步长\n",
    "window_size = 128\n",
    "step_size = 100\n",
    "\n",
    "# 对信号进行滑窗处理\n",
    "windows = sliding_window(signal, window_size, step_size)\n",
    "\n",
    "# 输出滑窗后的信号形状\n",
    "print(\"滑窗后信号形状：\", windows.shape)\n",
    "#print(windows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T12:21:48.397883300Z",
     "start_time": "2023-05-14T12:21:44.177797300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rest\n",
      "rest\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "rest\n",
      "lefthand\n",
      "rest\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n"
     ]
    }
   ],
   "source": [
    "save_path = 'C:\\\\Users\\\\a1882\\Desktop\\EEG\\\\normal\\model\\cnn_conv3_128_100e_97.pt'\n",
    "model = EEGNet()\n",
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "for i in range(windows.shape[2]):\n",
    "    signal = pd.DataFrame(windows[:,:,i])\n",
    "    input = torch.from_numpy(signal.values)\n",
    "    input = input.unsqueeze(0)\n",
    "    output = model(input.float())\n",
    "\n",
    "    # # print(output)\n",
    "    # probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    # print(probabilities)\n",
    "\n",
    "    _, pred = torch.max(output, dim=1)  # 找到预测分数最大的类别，得到预测类别\n",
    "    label_map = {0: 'lefthand', 1:'read' ,  2:'rest', 3: 'walkbase', 4: 'walkl' ,5: 'walkfocus'}\n",
    "    print(label_map[pred.item()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T12:21:54.307308700Z",
     "start_time": "2023-05-14T12:21:52.692427400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def normalize_tensor(tensor):\n",
    "    # Normalize the tensor along the channel dimension\n",
    "    mean = tensor.mean(dim=(1,2), keepdim=True)\n",
    "    std = tensor.std(dim=(1,2), keepdim=True)\n",
    "    normalized_tensor = (tensor - mean) / std\n",
    "    return normalized_tensor\n",
    "\n",
    "def show_normalized_tensor(tensor):\n",
    "    # Normalize the tensor and convert it to a numpy array\n",
    "    normalized_tensor = normalize_tensor(tensor)\n",
    "    normalized_array = normalized_tensor.squeeze().numpy()\n",
    "\n",
    "    # Display the normalized tensor using Matplotlib\n",
    "    plt.imshow(normalized_array, cmap='seismic',aspect='auto')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('for_cam.png')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T12:25:54.708014300Z",
     "start_time": "2023-05-14T12:25:54.670484900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\a1882\\\\Desktop\\\\EEG\\\\normal\\\\data\\\\128_s100\\\\lefthand_zyy_04_epocflex_2023_window_2.csv\", header=None)\n",
    "data = data.values\n",
    "data = torch.from_numpy(data)\n",
    "data = data.unsqueeze(0)\n",
    "print(data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T12:25:56.370215Z",
     "start_time": "2023-05-14T12:25:56.259656700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = EEGNet()\n",
    "input_tensor = data # example batch of input images\n",
    "show_normalized_tensor(input_tensor)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T12:25:59.232306800Z",
     "start_time": "2023-05-14T12:25:57.925777500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# import torchvision.models as models\n",
    "# from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "#\n",
    "# class GradCAM():\n",
    "#     '''\n",
    "#     Grad-cam: Visual explanations from deep networks via gradient-based localization\n",
    "#     Selvaraju R R, Cogswell M, Das A, et al.\n",
    "#     https://openaccess.thecvf.com/content_iccv_2017/html/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.html\n",
    "#     '''\n",
    "#     def __init__(self, model, target_layers, use_cuda=False):\n",
    "#         super(GradCAM).__init__()\n",
    "#         self.use_cuda = use_cuda\n",
    "#         self.model = model\n",
    "#         self.target_layers = target_layers\n",
    "#\n",
    "#         self.target_layers.register_forward_hook(self.forward_hook)\n",
    "#         self.target_layers.register_full_backward_hook(self.backward_hook)\n",
    "#\n",
    "#         self.activations = []\n",
    "#         self.grads = []\n",
    "#\n",
    "#     def forward_hook(self, module, input, output):\n",
    "#         self.activations.append(output[0])\n",
    "#\n",
    "#     def backward_hook(self, module, grad_input, grad_output):\n",
    "#         self.grads.append(grad_output[0].detach())\n",
    "#\n",
    "#     def calculate_cam(self, model_input):\n",
    "#         if self.use_cuda:\n",
    "#             device = torch.device('cuda')\n",
    "#             self.model.to(device)                 # Module.to() is in-place method\n",
    "#             model_input = model_input.to(device)  # Tensor.to() is not a in-place method\n",
    "#         self.model.eval()\n",
    "#\n",
    "#         # forward\n",
    "#         y_hat = self.model(model_input)\n",
    "#         max_class = np.argmax(y_hat.cpu().data.numpy(), axis=1)\n",
    "#\n",
    "#         # backward\n",
    "#         model.zero_grad()\n",
    "#         y_c = y_hat[0, max_class]\n",
    "#         y_c.backward()\n",
    "#\n",
    "#         # get activations and gradients\n",
    "#         activations = self.activations[0].cpu().data.numpy().squeeze()\n",
    "#         grads = self.grads[0].cpu().data.numpy().squeeze()\n",
    "#\n",
    "#         # calculate weights\n",
    "#         weights = np.mean(grads.reshape(grads.shape[0], -1), axis=1)\n",
    "#         weights = weights.reshape(-1, 1, 1)\n",
    "#         cam = (weights * activations).sum(axis=0)\n",
    "#         cam = np.maximum(cam, 0) # ReLU\n",
    "#         cam = cam / cam.max()\n",
    "#         return cam\n",
    "#\n",
    "#     @staticmethod\n",
    "#     def show_cam_on_image(image, cam):\n",
    "#         # image: [H,W,C]\n",
    "#         h, w = image.shape[:2]\n",
    "#\n",
    "#         cam = cv2.resize(cam, (h,w))\n",
    "#         cam = cam / cam.max()\n",
    "#         heatmap = cv2.applyColorMap((255*cam).astype(np.uint8), cv2.COLORMAP_JET) # [H,W,C]\n",
    "#         heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "#\n",
    "#         image = image / image.max()\n",
    "#         heatmap = heatmap / heatmap.max()\n",
    "#\n",
    "#         result = 0.4*heatmap + 0.6*image\n",
    "#         result = result / result.max()\n",
    "#\n",
    "#         plt.figure()\n",
    "#         plt.imshow((result*255).astype(np.uint8))\n",
    "#         plt.colorbar(shrink=0.8)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "#\n",
    "#     @staticmethod\n",
    "#     def preprocess_image(img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
    "#         preprocessing = Compose([\n",
    "#         \tToTensor(),\n",
    "#         \tNormalize(mean=mean, std=std)\n",
    "#             ])\n",
    "#         return preprocessing(img.copy()).unsqueeze(0)\n",
    "#\n",
    "#\n",
    "# if __name__ == '__main__':\n",
    "#     image = cv2.imread('both.png') # (224,224,3)\n",
    "#     input_tensor = GradCAM.preprocess_image(image)\n",
    "#     # model = models.resnet18(pretrained=True)\n",
    "#     model = EEGNet()\n",
    "#     grad_cam = GradCAM(model, model.layer4[-1], 224)\n",
    "#     cam = grad_cam.calculate_cam(input_tensor)\n",
    "#     GradCAM.show_cam_on_image(image, cam)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T15:26:46.893460100Z",
     "start_time": "2023-05-14T15:26:46.842935300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(2, 1), stride=(2, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout3 = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "        self.dropout4 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        features = x.clone()\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        output = self.fc2(x)\n",
    "        return output, features\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-14T15:27:03.317128Z",
     "start_time": "2023-05-14T15:27:03.245274500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T01:22:37.035024400Z",
     "start_time": "2023-05-15T01:22:37.033028Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsqueeze(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m EEGNet()\n\u001B[0;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124ma1882\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDesktop\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mEEG\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mnormal\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mcnn_conv3_128_100e_97.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m----> 3\u001B[0m cam_generator \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_cam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconv3\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124ma1882\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mDesktop\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mEEG\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mnormal\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m128_s100\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mlefthand_zyy_04_epocflex_2023_window_2.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m      5\u001B[0m data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mvalues\n",
      "Cell \u001B[1;32mIn[23], line 6\u001B[0m, in \u001B[0;36mgenerate_cam\u001B[1;34m(model, x)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_cam\u001B[39m(model, x):\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;66;03m# Get the output of the last convolutional layer\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m     features \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconv3(model\u001B[38;5;241m.\u001B[39mdropout2(model\u001B[38;5;241m.\u001B[39mpool2(model\u001B[38;5;241m.\u001B[39mbn2(F\u001B[38;5;241m.\u001B[39mrelu(model\u001B[38;5;241m.\u001B[39mconv2(model\u001B[38;5;241m.\u001B[39mdropout1(model\u001B[38;5;241m.\u001B[39mpool1(model\u001B[38;5;241m.\u001B[39mbn1(F\u001B[38;5;241m.\u001B[39mrelu(model\u001B[38;5;241m.\u001B[39mconv1(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m)))))))))))\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# Get the output of the fully connected layer\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     fc_out \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mfc2(model\u001B[38;5;241m.\u001B[39mdropout3(F\u001B[38;5;241m.\u001B[39mrelu(model\u001B[38;5;241m.\u001B[39mfc1(features\u001B[38;5;241m.\u001B[39mview(features\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)))))\n",
      "\u001B[1;31mTypeError\u001B[0m: unsqueeze(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "model = EEGNet()\n",
    "model.load_state_dict(torch.load(\"C:\\\\Users\\\\a1882\\Desktop\\EEG\\\\normal\\model\\cnn_conv3_128_100e_97.pt\"))\n",
    "cam_generator = generate_cam(model, \"conv3\")\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\a1882\\\\Desktop\\\\EEG\\\\normal\\\\data\\\\128_s100\\\\lefthand_zyy_04_epocflex_2023_window_2.csv\", header=None)\n",
    "data = data.values\n",
    "data = torch.from_numpy(data)\n",
    "data = data.unsqueeze(0)\n",
    "input_sample = data\n",
    "target_class = 0\n",
    "cam = cam_generator.generate_cam(input_sample, target_class)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T01:22:39.716378300Z",
     "start_time": "2023-05-15T01:22:39.523215700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
