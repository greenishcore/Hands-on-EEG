{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-12T03:03:19.604605400Z",
     "start_time": "2023-06-12T03:03:07.822707700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "origin_raw_data_dir = 'C:\\\\Users\\\\a1882\\\\Desktop\\\\EEG\\\\normal\\\\data\\\\128_s100'\n",
    "model_save ='C:\\\\Users\\\\a1882\\\\Desktop\\\\EEG\\\\normal\\\\model'\n",
    "pic_dir = 'C:\\\\Users\\\\a1882\\\\Desktop\\\\EEG\\\\normal\\\\pic'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T03:03:19.620718900Z",
     "start_time": "2023-06-12T03:03:19.604605400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class EEG_Dataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = origin_raw_data_dir\n",
    "        self.file_list = os.listdir(self.data_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        file_path = os.path.join(self.data_dir, file_name)\n",
    "        data = pd.read_csv(file_path, header=None)\n",
    "        data = data.values\n",
    "        data = torch.from_numpy(data)\n",
    "        label_map = {'lefthand': 0, 'read': 1, 'rest': 2, 'walkbase': 3, 'walkl': 4 ,'walkfocus': 5}\n",
    "        data_label = label_map[file_name.split('_')[0]]\n",
    "        return data, data_label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T03:03:19.654772600Z",
     "start_time": "2023-06-12T03:03:19.620718900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataset = EEG_Dataset(origin_raw_data_dir)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T03:03:20.320582400Z",
     "start_time": "2023-06-12T03:03:19.637615700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, input_size=2048, hidden_size=256, num_layers=4, num_classes=6):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(2, 1), stride=(2, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout3 = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        # print('x:', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print('conv1:', x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        # print('pool1:', x.shape)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        # print('conv2:', x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        # print('pool2:', x.shape)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        # print('conv3:',x.shape)\n",
    "        x = self.bn3(x)\n",
    "        # x = torch.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        # print('pool3:',x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print('flatten:', x.shape)\n",
    "        x = torch.unsqueeze(x,0)\n",
    "        # x = self.fc1(x)\n",
    "        # # print('fc1:', x.shape)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.dropout3(x)\n",
    "        # x = self.fc2(x)\n",
    "        # # print('fc2:', x.shape)\n",
    "        # return x\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T03:03:20.371359700Z",
     "start_time": "2023-06-12T03:03:20.337396200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T03:03:20.387514700Z",
     "start_time": "2023-06-12T03:03:20.354537800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=( 1, 32, 128), dtype=torch.float32)\n",
    "model = EEGNet()\n",
    "output = model(X)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T03:03:20.504457900Z",
     "start_time": "2023-06-12T03:03:20.387514700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "num_epochs = 100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T03:03:20.520661300Z",
     "start_time": "2023-06-12T03:03:20.504457900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            global train_loss\n",
    "            train_loss.append(loss)\n",
    "\n",
    "\n",
    "\n",
    "def test(dataloader, model,loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error:\\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    global valid_loss\n",
    "    valid_loss.append(test_loss)\n",
    "    global accuracy\n",
    "    accuracy.append(correct)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T03:03:20.569600200Z",
     "start_time": "2023-06-12T03:03:20.520661300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.819896  [    0/ 3788]\n",
      "loss: 1.795709  [  100/ 3788]\n",
      "loss: 1.815243  [  200/ 3788]\n",
      "loss: 1.815414  [  300/ 3788]\n",
      "loss: 1.814119  [  400/ 3788]\n",
      "loss: 1.813565  [  500/ 3788]\n",
      "loss: 1.811826  [  600/ 3788]\n",
      "loss: 1.724508  [  700/ 3788]\n",
      "loss: 1.764550  [  800/ 3788]\n",
      "loss: 1.806811  [  900/ 3788]\n",
      "loss: 1.802649  [ 1000/ 3788]\n",
      "loss: 1.800436  [ 1100/ 3788]\n",
      "loss: 1.720425  [ 1200/ 3788]\n",
      "loss: 1.793011  [ 1300/ 3788]\n",
      "loss: 1.723442  [ 1400/ 3788]\n",
      "loss: 1.581969  [ 1500/ 3788]\n",
      "loss: 1.779096  [ 1600/ 3788]\n",
      "loss: 1.775646  [ 1700/ 3788]\n",
      "loss: 1.769635  [ 1800/ 3788]\n",
      "loss: 1.656214  [ 1900/ 3788]\n",
      "loss: 0.651560  [ 2000/ 3788]\n",
      "loss: 0.443124  [ 2100/ 3788]\n",
      "loss: 1.643402  [ 2200/ 3788]\n",
      "loss: 1.557106  [ 2300/ 3788]\n",
      "loss: 0.246773  [ 2400/ 3788]\n",
      "loss: 0.226600  [ 2500/ 3788]\n",
      "loss: 0.188256  [ 2600/ 3788]\n",
      "loss: 1.366166  [ 2700/ 3788]\n",
      "loss: 1.506748  [ 2800/ 3788]\n",
      "loss: 0.178058  [ 2900/ 3788]\n",
      "loss: 1.810583  [ 3000/ 3788]\n",
      "loss: 1.201560  [ 3100/ 3788]\n",
      "loss: 0.196538  [ 3200/ 3788]\n",
      "loss: 2.173475  [ 3300/ 3788]\n",
      "loss: 1.079824  [ 3400/ 3788]\n",
      "loss: 1.316251  [ 3500/ 3788]\n",
      "loss: 1.856088  [ 3600/ 3788]\n",
      "loss: 0.936137  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 40.5%, Avg loss: 1.440768 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.970219  [    0/ 3788]\n",
      "loss: 1.322122  [  100/ 3788]\n",
      "loss: 0.814468  [  200/ 3788]\n",
      "loss: 0.726242  [  300/ 3788]\n",
      "loss: 1.129948  [  400/ 3788]\n",
      "loss: 0.862332  [  500/ 3788]\n",
      "loss: 0.063142  [  600/ 3788]\n",
      "loss: 1.821619  [  700/ 3788]\n",
      "loss: 0.949608  [  800/ 3788]\n",
      "loss: 1.913651  [  900/ 3788]\n",
      "loss: 0.916317  [ 1000/ 3788]\n",
      "loss: 1.202238  [ 1100/ 3788]\n",
      "loss: 0.063000  [ 1200/ 3788]\n",
      "loss: 0.039451  [ 1300/ 3788]\n",
      "loss: 1.801744  [ 1400/ 3788]\n",
      "loss: 0.026989  [ 1500/ 3788]\n",
      "loss: 1.450349  [ 1600/ 3788]\n",
      "loss: 0.025209  [ 1700/ 3788]\n",
      "loss: 0.024951  [ 1800/ 3788]\n",
      "loss: 0.998772  [ 1900/ 3788]\n",
      "loss: 1.559671  [ 2000/ 3788]\n",
      "loss: 0.436569  [ 2100/ 3788]\n",
      "loss: 0.747493  [ 2200/ 3788]\n",
      "loss: 0.773643  [ 2300/ 3788]\n",
      "loss: 0.960614  [ 2400/ 3788]\n",
      "loss: 0.879811  [ 2500/ 3788]\n",
      "loss: 0.781345  [ 2600/ 3788]\n",
      "loss: 0.073001  [ 2700/ 3788]\n",
      "loss: 0.717691  [ 2800/ 3788]\n",
      "loss: 0.613517  [ 2900/ 3788]\n",
      "loss: 0.156107  [ 3000/ 3788]\n",
      "loss: 0.234477  [ 3100/ 3788]\n",
      "loss: 0.143700  [ 3200/ 3788]\n",
      "loss: 0.824723  [ 3300/ 3788]\n",
      "loss: 0.147273  [ 3400/ 3788]\n",
      "loss: 0.701362  [ 3500/ 3788]\n",
      "loss: 0.039331  [ 3600/ 3788]\n",
      "loss: 0.011655  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 54.1%, Avg loss: 1.146363 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.650714  [    0/ 3788]\n",
      "loss: 2.443840  [  100/ 3788]\n",
      "loss: 0.135028  [  200/ 3788]\n",
      "loss: 0.568615  [  300/ 3788]\n",
      "loss: 0.757342  [  400/ 3788]\n",
      "loss: 1.782785  [  500/ 3788]\n",
      "loss: 0.173361  [  600/ 3788]\n",
      "loss: 0.009084  [  700/ 3788]\n",
      "loss: 0.250848  [  800/ 3788]\n",
      "loss: 0.165137  [  900/ 3788]\n",
      "loss: 0.371843  [ 1000/ 3788]\n",
      "loss: 0.217590  [ 1100/ 3788]\n",
      "loss: 0.206303  [ 1200/ 3788]\n",
      "loss: 0.787974  [ 1300/ 3788]\n",
      "loss: 0.009744  [ 1400/ 3788]\n",
      "loss: 0.838255  [ 1500/ 3788]\n",
      "loss: 1.702107  [ 1600/ 3788]\n",
      "loss: 0.729660  [ 1700/ 3788]\n",
      "loss: 0.756773  [ 1800/ 3788]\n",
      "loss: 0.715556  [ 1900/ 3788]\n",
      "loss: 0.018512  [ 2000/ 3788]\n",
      "loss: 1.690023  [ 2100/ 3788]\n",
      "loss: 0.007470  [ 2200/ 3788]\n",
      "loss: 0.151093  [ 2300/ 3788]\n",
      "loss: 0.695814  [ 2400/ 3788]\n",
      "loss: 1.626601  [ 2500/ 3788]\n",
      "loss: 0.095514  [ 2600/ 3788]\n",
      "loss: 0.869306  [ 2700/ 3788]\n",
      "loss: 0.638024  [ 2800/ 3788]\n",
      "loss: 0.028891  [ 2900/ 3788]\n",
      "loss: 0.672564  [ 3000/ 3788]\n",
      "loss: 1.020417  [ 3100/ 3788]\n",
      "loss: 1.747212  [ 3200/ 3788]\n",
      "loss: 0.005856  [ 3300/ 3788]\n",
      "loss: 0.082461  [ 3400/ 3788]\n",
      "loss: 2.058402  [ 3500/ 3788]\n",
      "loss: 0.820612  [ 3600/ 3788]\n",
      "loss: 0.088378  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 48.5%, Avg loss: 1.293241 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.859661  [    0/ 3788]\n",
      "loss: 0.145998  [  100/ 3788]\n",
      "loss: 0.005338  [  200/ 3788]\n",
      "loss: 0.167061  [  300/ 3788]\n",
      "loss: 0.004090  [  400/ 3788]\n",
      "loss: 0.224396  [  500/ 3788]\n",
      "loss: 0.579111  [  600/ 3788]\n",
      "loss: 0.071346  [  700/ 3788]\n",
      "loss: 0.008851  [  800/ 3788]\n",
      "loss: 0.656360  [  900/ 3788]\n",
      "loss: 0.625873  [ 1000/ 3788]\n",
      "loss: 0.638401  [ 1100/ 3788]\n",
      "loss: 0.070745  [ 1200/ 3788]\n",
      "loss: 0.600383  [ 1300/ 3788]\n",
      "loss: 0.685712  [ 1400/ 3788]\n",
      "loss: 0.012167  [ 1500/ 3788]\n",
      "loss: 0.003888  [ 1600/ 3788]\n",
      "loss: 1.475538  [ 1700/ 3788]\n",
      "loss: 0.626251  [ 1800/ 3788]\n",
      "loss: 0.536300  [ 1900/ 3788]\n",
      "loss: 0.056921  [ 2000/ 3788]\n",
      "loss: 0.567796  [ 2100/ 3788]\n",
      "loss: 0.069537  [ 2200/ 3788]\n",
      "loss: 0.099834  [ 2300/ 3788]\n",
      "loss: 0.064081  [ 2400/ 3788]\n",
      "loss: 0.004014  [ 2500/ 3788]\n",
      "loss: 0.528468  [ 2600/ 3788]\n",
      "loss: 0.074037  [ 2700/ 3788]\n",
      "loss: 1.654215  [ 2800/ 3788]\n",
      "loss: 0.856605  [ 2900/ 3788]\n",
      "loss: 0.502265  [ 3000/ 3788]\n",
      "loss: 0.645985  [ 3100/ 3788]\n",
      "loss: 0.268486  [ 3200/ 3788]\n",
      "loss: 0.004772  [ 3300/ 3788]\n",
      "loss: 0.287311  [ 3400/ 3788]\n",
      "loss: 0.552642  [ 3500/ 3788]\n",
      "loss: 0.003516  [ 3600/ 3788]\n",
      "loss: 0.004215  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 63.6%, Avg loss: 0.964763 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.575460  [    0/ 3788]\n",
      "loss: 0.004296  [  100/ 3788]\n",
      "loss: 0.230320  [  200/ 3788]\n",
      "loss: 0.003166  [  300/ 3788]\n",
      "loss: 1.459622  [  400/ 3788]\n",
      "loss: 0.484303  [  500/ 3788]\n",
      "loss: 0.224515  [  600/ 3788]\n",
      "loss: 0.046763  [  700/ 3788]\n",
      "loss: 0.003318  [  800/ 3788]\n",
      "loss: 1.044632  [  900/ 3788]\n",
      "loss: 1.046166  [ 1000/ 3788]\n",
      "loss: 0.070582  [ 1100/ 3788]\n",
      "loss: 1.068223  [ 1200/ 3788]\n",
      "loss: 0.083761  [ 1300/ 3788]\n",
      "loss: 1.381227  [ 1400/ 3788]\n",
      "loss: 0.399752  [ 1500/ 3788]\n",
      "loss: 0.635078  [ 1600/ 3788]\n",
      "loss: 0.402604  [ 1700/ 3788]\n",
      "loss: 0.911165  [ 1800/ 3788]\n",
      "loss: 0.002766  [ 1900/ 3788]\n",
      "loss: 0.669209  [ 2000/ 3788]\n",
      "loss: 0.267861  [ 2100/ 3788]\n",
      "loss: 0.994008  [ 2200/ 3788]\n",
      "loss: 0.003978  [ 2300/ 3788]\n",
      "loss: 2.369608  [ 2400/ 3788]\n",
      "loss: 0.002850  [ 2500/ 3788]\n",
      "loss: 0.054953  [ 2600/ 3788]\n",
      "loss: 0.262828  [ 2700/ 3788]\n",
      "loss: 0.110619  [ 2800/ 3788]\n",
      "loss: 0.005795  [ 2900/ 3788]\n",
      "loss: 0.035665  [ 3000/ 3788]\n",
      "loss: 0.551520  [ 3100/ 3788]\n",
      "loss: 0.003648  [ 3200/ 3788]\n",
      "loss: 0.451877  [ 3300/ 3788]\n",
      "loss: 1.856109  [ 3400/ 3788]\n",
      "loss: 0.640522  [ 3500/ 3788]\n",
      "loss: 0.003567  [ 3600/ 3788]\n",
      "loss: 0.326458  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 69.1%, Avg loss: 0.789888 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.391202  [    0/ 3788]\n",
      "loss: 0.549896  [  100/ 3788]\n",
      "loss: 0.586981  [  200/ 3788]\n",
      "loss: 0.413635  [  300/ 3788]\n",
      "loss: 0.004035  [  400/ 3788]\n",
      "loss: 2.876910  [  500/ 3788]\n",
      "loss: 0.050701  [  600/ 3788]\n",
      "loss: 0.537785  [  700/ 3788]\n",
      "loss: 0.905901  [  800/ 3788]\n",
      "loss: 0.042370  [  900/ 3788]\n",
      "loss: 0.049918  [ 1000/ 3788]\n",
      "loss: 0.364170  [ 1100/ 3788]\n",
      "loss: 0.469823  [ 1200/ 3788]\n",
      "loss: 0.855703  [ 1300/ 3788]\n",
      "loss: 0.005461  [ 1400/ 3788]\n",
      "loss: 0.030941  [ 1500/ 3788]\n",
      "loss: 1.281029  [ 1600/ 3788]\n",
      "loss: 0.211656  [ 1700/ 3788]\n",
      "loss: 0.283275  [ 1800/ 3788]\n",
      "loss: 0.041330  [ 1900/ 3788]\n",
      "loss: 1.012451  [ 2000/ 3788]\n",
      "loss: 0.547378  [ 2100/ 3788]\n",
      "loss: 0.330874  [ 2200/ 3788]\n",
      "loss: 0.003270  [ 2300/ 3788]\n",
      "loss: 0.368641  [ 2400/ 3788]\n",
      "loss: 0.387175  [ 2500/ 3788]\n",
      "loss: 0.002467  [ 2600/ 3788]\n",
      "loss: 0.057585  [ 2700/ 3788]\n",
      "loss: 0.338164  [ 2800/ 3788]\n",
      "loss: 0.069315  [ 2900/ 3788]\n",
      "loss: 0.228313  [ 3000/ 3788]\n",
      "loss: 0.467770  [ 3100/ 3788]\n",
      "loss: 0.419173  [ 3200/ 3788]\n",
      "loss: 0.571876  [ 3300/ 3788]\n",
      "loss: 0.210057  [ 3400/ 3788]\n",
      "loss: 0.078035  [ 3500/ 3788]\n",
      "loss: 0.993814  [ 3600/ 3788]\n",
      "loss: 0.296459  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 44.7%, Avg loss: 1.392417 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.002624  [    0/ 3788]\n",
      "loss: 0.002054  [  100/ 3788]\n",
      "loss: 0.026772  [  200/ 3788]\n",
      "loss: 0.248021  [  300/ 3788]\n",
      "loss: 0.024181  [  400/ 3788]\n",
      "loss: 0.029654  [  500/ 3788]\n",
      "loss: 0.126066  [  600/ 3788]\n",
      "loss: 0.033409  [  700/ 3788]\n",
      "loss: 0.398559  [  800/ 3788]\n",
      "loss: 0.130590  [  900/ 3788]\n",
      "loss: 0.181419  [ 1000/ 3788]\n",
      "loss: 0.027860  [ 1100/ 3788]\n",
      "loss: 0.378949  [ 1200/ 3788]\n",
      "loss: 0.004916  [ 1300/ 3788]\n",
      "loss: 0.153772  [ 1400/ 3788]\n",
      "loss: 0.550059  [ 1500/ 3788]\n",
      "loss: 1.587417  [ 1600/ 3788]\n",
      "loss: 0.004603  [ 1700/ 3788]\n",
      "loss: 0.002176  [ 1800/ 3788]\n",
      "loss: 0.160530  [ 1900/ 3788]\n",
      "loss: 0.002018  [ 2000/ 3788]\n",
      "loss: 0.003829  [ 2100/ 3788]\n",
      "loss: 0.172106  [ 2200/ 3788]\n",
      "loss: 0.273149  [ 2300/ 3788]\n",
      "loss: 0.161783  [ 2400/ 3788]\n",
      "loss: 0.080406  [ 2500/ 3788]\n",
      "loss: 0.110404  [ 2600/ 3788]\n",
      "loss: 0.037503  [ 2700/ 3788]\n",
      "loss: 0.001862  [ 2800/ 3788]\n",
      "loss: 0.003571  [ 2900/ 3788]\n",
      "loss: 0.224685  [ 3000/ 3788]\n",
      "loss: 0.197328  [ 3100/ 3788]\n",
      "loss: 0.028297  [ 3200/ 3788]\n",
      "loss: 0.323581  [ 3300/ 3788]\n",
      "loss: 0.123677  [ 3400/ 3788]\n",
      "loss: 0.028108  [ 3500/ 3788]\n",
      "loss: 0.075150  [ 3600/ 3788]\n",
      "loss: 0.003265  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 72.5%, Avg loss: 0.690874 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.206426  [    0/ 3788]\n",
      "loss: 0.001749  [  100/ 3788]\n",
      "loss: 0.001880  [  200/ 3788]\n",
      "loss: 0.001801  [  300/ 3788]\n",
      "loss: 0.675945  [  400/ 3788]\n",
      "loss: 0.040123  [  500/ 3788]\n",
      "loss: 0.359128  [  600/ 3788]\n",
      "loss: 0.089996  [  700/ 3788]\n",
      "loss: 0.265731  [  800/ 3788]\n",
      "loss: 0.231659  [  900/ 3788]\n",
      "loss: 0.006533  [ 1000/ 3788]\n",
      "loss: 0.023720  [ 1100/ 3788]\n",
      "loss: 0.111854  [ 1200/ 3788]\n",
      "loss: 0.547528  [ 1300/ 3788]\n",
      "loss: 0.205885  [ 1400/ 3788]\n",
      "loss: 0.074275  [ 1500/ 3788]\n",
      "loss: 0.095617  [ 1600/ 3788]\n",
      "loss: 0.103502  [ 1700/ 3788]\n",
      "loss: 1.615169  [ 1800/ 3788]\n",
      "loss: 0.055028  [ 1900/ 3788]\n",
      "loss: 0.056005  [ 2000/ 3788]\n",
      "loss: 0.372802  [ 2100/ 3788]\n",
      "loss: 0.062676  [ 2200/ 3788]\n",
      "loss: 0.108934  [ 2300/ 3788]\n",
      "loss: 0.022268  [ 2400/ 3788]\n",
      "loss: 1.034451  [ 2500/ 3788]\n",
      "loss: 0.001479  [ 2600/ 3788]\n",
      "loss: 0.010257  [ 2700/ 3788]\n",
      "loss: 0.001652  [ 2800/ 3788]\n",
      "loss: 0.001701  [ 2900/ 3788]\n",
      "loss: 0.153288  [ 3000/ 3788]\n",
      "loss: 0.002202  [ 3100/ 3788]\n",
      "loss: 0.495538  [ 3200/ 3788]\n",
      "loss: 0.040114  [ 3300/ 3788]\n",
      "loss: 0.066786  [ 3400/ 3788]\n",
      "loss: 0.155548  [ 3500/ 3788]\n",
      "loss: 0.045473  [ 3600/ 3788]\n",
      "loss: 0.001619  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 47.6%, Avg loss: 1.505398 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.329124  [    0/ 3788]\n",
      "loss: 0.002019  [  100/ 3788]\n",
      "loss: 0.068464  [  200/ 3788]\n",
      "loss: 0.033680  [  300/ 3788]\n",
      "loss: 0.043336  [  400/ 3788]\n",
      "loss: 0.036189  [  500/ 3788]\n",
      "loss: 0.002707  [  600/ 3788]\n",
      "loss: 0.290948  [  700/ 3788]\n",
      "loss: 0.019959  [  800/ 3788]\n",
      "loss: 0.027877  [  900/ 3788]\n",
      "loss: 0.039480  [ 1000/ 3788]\n",
      "loss: 0.406023  [ 1100/ 3788]\n",
      "loss: 0.029541  [ 1200/ 3788]\n",
      "loss: 0.001442  [ 1300/ 3788]\n",
      "loss: 0.217420  [ 1400/ 3788]\n",
      "loss: 0.019466  [ 1500/ 3788]\n",
      "loss: 0.262887  [ 1600/ 3788]\n",
      "loss: 0.002389  [ 1700/ 3788]\n",
      "loss: 0.230027  [ 1800/ 3788]\n",
      "loss: 0.552558  [ 1900/ 3788]\n",
      "loss: 0.124653  [ 2000/ 3788]\n",
      "loss: 0.035214  [ 2100/ 3788]\n",
      "loss: 0.016810  [ 2200/ 3788]\n",
      "loss: 0.034472  [ 2300/ 3788]\n",
      "loss: 0.765840  [ 2400/ 3788]\n",
      "loss: 0.619119  [ 2500/ 3788]\n",
      "loss: 0.036141  [ 2600/ 3788]\n",
      "loss: 0.019205  [ 2700/ 3788]\n",
      "loss: 0.036168  [ 2800/ 3788]\n",
      "loss: 0.013985  [ 2900/ 3788]\n",
      "loss: 0.056760  [ 3000/ 3788]\n",
      "loss: 1.488186  [ 3100/ 3788]\n",
      "loss: 0.057586  [ 3200/ 3788]\n",
      "loss: 0.001428  [ 3300/ 3788]\n",
      "loss: 0.060609  [ 3400/ 3788]\n",
      "loss: 0.049418  [ 3500/ 3788]\n",
      "loss: 0.186934  [ 3600/ 3788]\n",
      "loss: 0.027306  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 70.0%, Avg loss: 0.791864 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.309199  [    0/ 3788]\n",
      "loss: 0.028787  [  100/ 3788]\n",
      "loss: 0.001229  [  200/ 3788]\n",
      "loss: 0.001312  [  300/ 3788]\n",
      "loss: 0.024012  [  400/ 3788]\n",
      "loss: 0.052572  [  500/ 3788]\n",
      "loss: 0.027116  [  600/ 3788]\n",
      "loss: 0.075464  [  700/ 3788]\n",
      "loss: 0.028132  [  800/ 3788]\n",
      "loss: 0.474348  [  900/ 3788]\n",
      "loss: 0.012835  [ 1000/ 3788]\n",
      "loss: 0.001341  [ 1100/ 3788]\n",
      "loss: 0.144929  [ 1200/ 3788]\n",
      "loss: 0.426771  [ 1300/ 3788]\n",
      "loss: 0.018888  [ 1400/ 3788]\n",
      "loss: 0.046985  [ 1500/ 3788]\n",
      "loss: 0.049296  [ 1600/ 3788]\n",
      "loss: 0.197186  [ 1700/ 3788]\n",
      "loss: 0.025938  [ 1800/ 3788]\n",
      "loss: 0.001444  [ 1900/ 3788]\n",
      "loss: 0.021557  [ 2000/ 3788]\n",
      "loss: 0.001334  [ 2100/ 3788]\n",
      "loss: 0.025638  [ 2200/ 3788]\n",
      "loss: 0.030302  [ 2300/ 3788]\n",
      "loss: 0.064327  [ 2400/ 3788]\n",
      "loss: 0.030511  [ 2500/ 3788]\n",
      "loss: 0.027436  [ 2600/ 3788]\n",
      "loss: 0.013994  [ 2700/ 3788]\n",
      "loss: 0.262545  [ 2800/ 3788]\n",
      "loss: 0.001199  [ 2900/ 3788]\n",
      "loss: 0.016907  [ 3000/ 3788]\n",
      "loss: 0.015701  [ 3100/ 3788]\n",
      "loss: 0.256942  [ 3200/ 3788]\n",
      "loss: 0.004568  [ 3300/ 3788]\n",
      "loss: 0.020705  [ 3400/ 3788]\n",
      "loss: 0.019179  [ 3500/ 3788]\n",
      "loss: 0.890194  [ 3600/ 3788]\n",
      "loss: 0.768137  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 73.6%, Avg loss: 0.683700 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.001203  [    0/ 3788]\n",
      "loss: 0.019194  [  100/ 3788]\n",
      "loss: 0.164070  [  200/ 3788]\n",
      "loss: 0.001140  [  300/ 3788]\n",
      "loss: 0.506510  [  400/ 3788]\n",
      "loss: 0.074381  [  500/ 3788]\n",
      "loss: 0.160878  [  600/ 3788]\n",
      "loss: 0.214689  [  700/ 3788]\n",
      "loss: 0.001131  [  800/ 3788]\n",
      "loss: 0.304787  [  900/ 3788]\n",
      "loss: 0.001200  [ 1000/ 3788]\n",
      "loss: 0.034339  [ 1100/ 3788]\n",
      "loss: 0.027432  [ 1200/ 3788]\n",
      "loss: 0.030361  [ 1300/ 3788]\n",
      "loss: 0.016064  [ 1400/ 3788]\n",
      "loss: 0.261725  [ 1500/ 3788]\n",
      "loss: 0.031429  [ 1600/ 3788]\n",
      "loss: 0.015604  [ 1700/ 3788]\n",
      "loss: 0.001214  [ 1800/ 3788]\n",
      "loss: 0.019407  [ 1900/ 3788]\n",
      "loss: 0.034593  [ 2000/ 3788]\n",
      "loss: 0.008589  [ 2100/ 3788]\n",
      "loss: 0.034795  [ 2200/ 3788]\n",
      "loss: 0.009795  [ 2300/ 3788]\n",
      "loss: 0.026681  [ 2400/ 3788]\n",
      "loss: 0.088721  [ 2500/ 3788]\n",
      "loss: 0.367275  [ 2600/ 3788]\n",
      "loss: 0.467098  [ 2700/ 3788]\n",
      "loss: 0.025100  [ 2800/ 3788]\n",
      "loss: 0.037718  [ 2900/ 3788]\n",
      "loss: 0.003744  [ 3000/ 3788]\n",
      "loss: 0.784736  [ 3100/ 3788]\n",
      "loss: 0.001313  [ 3200/ 3788]\n",
      "loss: 0.056834  [ 3300/ 3788]\n",
      "loss: 0.482451  [ 3400/ 3788]\n",
      "loss: 0.001599  [ 3500/ 3788]\n",
      "loss: 0.065998  [ 3600/ 3788]\n",
      "loss: 0.047550  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 74.9%, Avg loss: 0.637635 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.001151  [    0/ 3788]\n",
      "loss: 0.003124  [  100/ 3788]\n",
      "loss: 0.001582  [  200/ 3788]\n",
      "loss: 0.006755  [  300/ 3788]\n",
      "loss: 0.001790  [  400/ 3788]\n",
      "loss: 0.020059  [  500/ 3788]\n",
      "loss: 0.011127  [  600/ 3788]\n",
      "loss: 0.011991  [  700/ 3788]\n",
      "loss: 0.013799  [  800/ 3788]\n",
      "loss: 0.072096  [  900/ 3788]\n",
      "loss: 0.011640  [ 1000/ 3788]\n",
      "loss: 0.255469  [ 1100/ 3788]\n",
      "loss: 0.021598  [ 1200/ 3788]\n",
      "loss: 0.001171  [ 1300/ 3788]\n",
      "loss: 0.020807  [ 1400/ 3788]\n",
      "loss: 0.014147  [ 1500/ 3788]\n",
      "loss: 0.156855  [ 1600/ 3788]\n",
      "loss: 0.919855  [ 1700/ 3788]\n",
      "loss: 0.009295  [ 1800/ 3788]\n",
      "loss: 0.010006  [ 1900/ 3788]\n",
      "loss: 0.020439  [ 2000/ 3788]\n",
      "loss: 0.216737  [ 2100/ 3788]\n",
      "loss: 0.002356  [ 2200/ 3788]\n",
      "loss: 0.263230  [ 2300/ 3788]\n",
      "loss: 0.012932  [ 2400/ 3788]\n",
      "loss: 0.027416  [ 2500/ 3788]\n",
      "loss: 0.028240  [ 2600/ 3788]\n",
      "loss: 0.026339  [ 2700/ 3788]\n",
      "loss: 0.099281  [ 2800/ 3788]\n",
      "loss: 0.023895  [ 2900/ 3788]\n",
      "loss: 0.012515  [ 3000/ 3788]\n",
      "loss: 1.180046  [ 3100/ 3788]\n",
      "loss: 0.015981  [ 3200/ 3788]\n",
      "loss: 0.000967  [ 3300/ 3788]\n",
      "loss: 0.001078  [ 3400/ 3788]\n",
      "loss: 0.007563  [ 3500/ 3788]\n",
      "loss: 0.001025  [ 3600/ 3788]\n",
      "loss: 0.000955  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 75.5%, Avg loss: 0.648942 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.105818  [    0/ 3788]\n",
      "loss: 0.001097  [  100/ 3788]\n",
      "loss: 0.016570  [  200/ 3788]\n",
      "loss: 0.001079  [  300/ 3788]\n",
      "loss: 0.126773  [  400/ 3788]\n",
      "loss: 0.010488  [  500/ 3788]\n",
      "loss: 0.000840  [  600/ 3788]\n",
      "loss: 2.784088  [  700/ 3788]\n",
      "loss: 0.030202  [  800/ 3788]\n",
      "loss: 0.009514  [  900/ 3788]\n",
      "loss: 0.009331  [ 1000/ 3788]\n",
      "loss: 0.212647  [ 1100/ 3788]\n",
      "loss: 0.030359  [ 1200/ 3788]\n",
      "loss: 0.015587  [ 1300/ 3788]\n",
      "loss: 0.011448  [ 1400/ 3788]\n",
      "loss: 0.012025  [ 1500/ 3788]\n",
      "loss: 0.134105  [ 1600/ 3788]\n",
      "loss: 0.087017  [ 1700/ 3788]\n",
      "loss: 0.001163  [ 1800/ 3788]\n",
      "loss: 0.013986  [ 1900/ 3788]\n",
      "loss: 0.156988  [ 2000/ 3788]\n",
      "loss: 0.023840  [ 2100/ 3788]\n",
      "loss: 0.007128  [ 2200/ 3788]\n",
      "loss: 0.069202  [ 2300/ 3788]\n",
      "loss: 0.126889  [ 2400/ 3788]\n",
      "loss: 0.592444  [ 2500/ 3788]\n",
      "loss: 1.032443  [ 2600/ 3788]\n",
      "loss: 0.028304  [ 2700/ 3788]\n",
      "loss: 0.019150  [ 2800/ 3788]\n",
      "loss: 0.052073  [ 2900/ 3788]\n",
      "loss: 0.022849  [ 3000/ 3788]\n",
      "loss: 0.167887  [ 3100/ 3788]\n",
      "loss: 0.016387  [ 3200/ 3788]\n",
      "loss: 0.001020  [ 3300/ 3788]\n",
      "loss: 0.025370  [ 3400/ 3788]\n",
      "loss: 0.018127  [ 3500/ 3788]\n",
      "loss: 0.011362  [ 3600/ 3788]\n",
      "loss: 0.016545  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 63.3%, Avg loss: 0.992709 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.001229  [    0/ 3788]\n",
      "loss: 0.007028  [  100/ 3788]\n",
      "loss: 0.001391  [  200/ 3788]\n",
      "loss: 0.005665  [  300/ 3788]\n",
      "loss: 0.006535  [  400/ 3788]\n",
      "loss: 0.001140  [  500/ 3788]\n",
      "loss: 0.000774  [  600/ 3788]\n",
      "loss: 0.006680  [  700/ 3788]\n",
      "loss: 0.018769  [  800/ 3788]\n",
      "loss: 0.009551  [  900/ 3788]\n",
      "loss: 0.004869  [ 1000/ 3788]\n",
      "loss: 0.129581  [ 1100/ 3788]\n",
      "loss: 0.019463  [ 1200/ 3788]\n",
      "loss: 0.000781  [ 1300/ 3788]\n",
      "loss: 0.021759  [ 1400/ 3788]\n",
      "loss: 0.005984  [ 1500/ 3788]\n",
      "loss: 0.006486  [ 1600/ 3788]\n",
      "loss: 0.000794  [ 1700/ 3788]\n",
      "loss: 0.007665  [ 1800/ 3788]\n",
      "loss: 0.001510  [ 1900/ 3788]\n",
      "loss: 0.000759  [ 2000/ 3788]\n",
      "loss: 0.005174  [ 2100/ 3788]\n",
      "loss: 0.005135  [ 2200/ 3788]\n",
      "loss: 0.001193  [ 2300/ 3788]\n",
      "loss: 0.000738  [ 2400/ 3788]\n",
      "loss: 0.000899  [ 2500/ 3788]\n",
      "loss: 0.150249  [ 2600/ 3788]\n",
      "loss: 0.005913  [ 2700/ 3788]\n",
      "loss: 0.001394  [ 2800/ 3788]\n",
      "loss: 0.035194  [ 2900/ 3788]\n",
      "loss: 0.018612  [ 3000/ 3788]\n",
      "loss: 0.016256  [ 3100/ 3788]\n",
      "loss: 0.221654  [ 3200/ 3788]\n",
      "loss: 0.006932  [ 3300/ 3788]\n",
      "loss: 0.062481  [ 3400/ 3788]\n",
      "loss: 0.604144  [ 3500/ 3788]\n",
      "loss: 0.005277  [ 3600/ 3788]\n",
      "loss: 0.174531  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 35.5%, Avg loss: 2.034816 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 1.425035  [    0/ 3788]\n",
      "loss: 0.007491  [  100/ 3788]\n",
      "loss: 0.117080  [  200/ 3788]\n",
      "loss: 0.005654  [  300/ 3788]\n",
      "loss: 0.017364  [  400/ 3788]\n",
      "loss: 0.010372  [  500/ 3788]\n",
      "loss: 0.042418  [  600/ 3788]\n",
      "loss: 0.001915  [  700/ 3788]\n",
      "loss: 0.005378  [  800/ 3788]\n",
      "loss: 0.042084  [  900/ 3788]\n",
      "loss: 0.001505  [ 1000/ 3788]\n",
      "loss: 0.002161  [ 1100/ 3788]\n",
      "loss: 0.089202  [ 1200/ 3788]\n",
      "loss: 0.165853  [ 1300/ 3788]\n",
      "loss: 0.004070  [ 1400/ 3788]\n",
      "loss: 0.031739  [ 1500/ 3788]\n",
      "loss: 0.014387  [ 1600/ 3788]\n",
      "loss: 0.135849  [ 1700/ 3788]\n",
      "loss: 1.353414  [ 1800/ 3788]\n",
      "loss: 0.004181  [ 1900/ 3788]\n",
      "loss: 0.006857  [ 2000/ 3788]\n",
      "loss: 0.000698  [ 2100/ 3788]\n",
      "loss: 0.102345  [ 2200/ 3788]\n",
      "loss: 0.004958  [ 2300/ 3788]\n",
      "loss: 0.004932  [ 2400/ 3788]\n",
      "loss: 0.014180  [ 2500/ 3788]\n",
      "loss: 0.005367  [ 2600/ 3788]\n",
      "loss: 0.040201  [ 2700/ 3788]\n",
      "loss: 0.098097  [ 2800/ 3788]\n",
      "loss: 0.016747  [ 2900/ 3788]\n",
      "loss: 0.019643  [ 3000/ 3788]\n",
      "loss: 0.001281  [ 3100/ 3788]\n",
      "loss: 0.005418  [ 3200/ 3788]\n",
      "loss: 0.004532  [ 3300/ 3788]\n",
      "loss: 0.006691  [ 3400/ 3788]\n",
      "loss: 0.000698  [ 3500/ 3788]\n",
      "loss: 0.079914  [ 3600/ 3788]\n",
      "loss: 0.004902  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 79.9%, Avg loss: 0.469240 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.089075  [    0/ 3788]\n",
      "loss: 0.004224  [  100/ 3788]\n",
      "loss: 0.000641  [  200/ 3788]\n",
      "loss: 0.013293  [  300/ 3788]\n",
      "loss: 0.029734  [  400/ 3788]\n",
      "loss: 0.004368  [  500/ 3788]\n",
      "loss: 0.001570  [  600/ 3788]\n",
      "loss: 0.005037  [  700/ 3788]\n",
      "loss: 0.009801  [  800/ 3788]\n",
      "loss: 0.000730  [  900/ 3788]\n",
      "loss: 0.011338  [ 1000/ 3788]\n",
      "loss: 0.007379  [ 1100/ 3788]\n",
      "loss: 0.004104  [ 1200/ 3788]\n",
      "loss: 0.673425  [ 1300/ 3788]\n",
      "loss: 0.017905  [ 1400/ 3788]\n",
      "loss: 0.006312  [ 1500/ 3788]\n",
      "loss: 0.000595  [ 1600/ 3788]\n",
      "loss: 0.000634  [ 1700/ 3788]\n",
      "loss: 2.853171  [ 1800/ 3788]\n",
      "loss: 0.005173  [ 1900/ 3788]\n",
      "loss: 0.030803  [ 2000/ 3788]\n",
      "loss: 0.005044  [ 2100/ 3788]\n",
      "loss: 0.001068  [ 2200/ 3788]\n",
      "loss: 1.182072  [ 2300/ 3788]\n",
      "loss: 0.028576  [ 2400/ 3788]\n",
      "loss: 0.003667  [ 2500/ 3788]\n",
      "loss: 0.000664  [ 2600/ 3788]\n",
      "loss: 2.518653  [ 2700/ 3788]\n",
      "loss: 0.003321  [ 2800/ 3788]\n",
      "loss: 0.033593  [ 2900/ 3788]\n",
      "loss: 0.003498  [ 3000/ 3788]\n",
      "loss: 0.012737  [ 3100/ 3788]\n",
      "loss: 0.128823  [ 3200/ 3788]\n",
      "loss: 0.004399  [ 3300/ 3788]\n",
      "loss: 0.012432  [ 3400/ 3788]\n",
      "loss: 0.003270  [ 3500/ 3788]\n",
      "loss: 0.014490  [ 3600/ 3788]\n",
      "loss: 0.018516  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 75.3%, Avg loss: 0.607668 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.004618  [    0/ 3788]\n",
      "loss: 0.003178  [  100/ 3788]\n",
      "loss: 0.003223  [  200/ 3788]\n",
      "loss: 0.003006  [  300/ 3788]\n",
      "loss: 0.062533  [  400/ 3788]\n",
      "loss: 0.011837  [  500/ 3788]\n",
      "loss: 0.000692  [  600/ 3788]\n",
      "loss: 0.004027  [  700/ 3788]\n",
      "loss: 0.115360  [  800/ 3788]\n",
      "loss: 0.003310  [  900/ 3788]\n",
      "loss: 0.075882  [ 1000/ 3788]\n",
      "loss: 0.000698  [ 1100/ 3788]\n",
      "loss: 0.007970  [ 1200/ 3788]\n",
      "loss: 0.002245  [ 1300/ 3788]\n",
      "loss: 0.005813  [ 1400/ 3788]\n",
      "loss: 0.131008  [ 1500/ 3788]\n",
      "loss: 0.014556  [ 1600/ 3788]\n",
      "loss: 0.008955  [ 1700/ 3788]\n",
      "loss: 0.001144  [ 1800/ 3788]\n",
      "loss: 0.016121  [ 1900/ 3788]\n",
      "loss: 0.000840  [ 2000/ 3788]\n",
      "loss: 0.274952  [ 2100/ 3788]\n",
      "loss: 0.003374  [ 2200/ 3788]\n",
      "loss: 0.002892  [ 2300/ 3788]\n",
      "loss: 0.004928  [ 2400/ 3788]\n",
      "loss: 0.000593  [ 2500/ 3788]\n",
      "loss: 0.004050  [ 2600/ 3788]\n",
      "loss: 0.033863  [ 2700/ 3788]\n",
      "loss: 0.003723  [ 2800/ 3788]\n",
      "loss: 0.011481  [ 2900/ 3788]\n",
      "loss: 0.000494  [ 3000/ 3788]\n",
      "loss: 0.003343  [ 3100/ 3788]\n",
      "loss: 0.010295  [ 3200/ 3788]\n",
      "loss: 0.012837  [ 3300/ 3788]\n",
      "loss: 0.005873  [ 3400/ 3788]\n",
      "loss: 0.006876  [ 3500/ 3788]\n",
      "loss: 0.035917  [ 3600/ 3788]\n",
      "loss: 0.003710  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 66.6%, Avg loss: 0.864648 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.006760  [    0/ 3788]\n",
      "loss: 0.046202  [  100/ 3788]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mt\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m-------------------------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 10\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     13\u001B[0m test(test_loader, model, loss_fn)\n",
      "Cell \u001B[1;32mIn[9], line 10\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(dataloader, model, loss_fn, optimizer)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[0;32m      9\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 10\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "# model = model = EEG_LSTM(input_size=31000, hidden_size=128, num_layers=2,num_classes=train_class_number).to(device)\n",
    "#model = model.cuda()\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "accuracy = []\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    model.train(True)\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "\n",
    "    model.train(False)\n",
    "    test(test_loader, model, loss_fn)\n",
    "\n",
    "    if t > num_epochs-5:\n",
    "        torch.save(model.state_dict(), os.path.join(model_save,'cnn+lstm_1000_100e_'+str(t)+'.pt'))\n",
    "\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-12T05:13:43.882871Z",
     "start_time": "2023-06-12T03:03:20.553974500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loss=pd.DataFrame(train_loss)\n",
    "valid_loss=pd.DataFrame(valid_loss)\n",
    "accuracy=pd.DataFrame(accuracy)\n",
    "\n",
    "train_loss.to_csv(os.path.join(pic_dir,'_tloss.csv'))\n",
    "valid_loss.to_csv(os.path.join(pic_dir,'_vloss.csv'))\n",
    "accuracy.to_csv(os.path.join(pic_dir,'_acc.csv'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Done!\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "hex_d1 = '#552a28'\n",
    "hex_d2 = '#005943'\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, hex_d1, label='train loss')\n",
    "plt.ylabel('train loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(valid_loss, hex_d2, label='valid loss')\n",
    "plt.ylabel('valid loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(os.path.join(pic_dir, 'cnn_3_conv+lstm_128_s100_loss.png'))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(accuracy, color = '#550300', label='accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(pic_dir,'cnn_3_conv+lstm_128_s100_accuracy.png'))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
