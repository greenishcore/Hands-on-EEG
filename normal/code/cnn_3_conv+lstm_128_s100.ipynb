{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-26T13:16:54.467055600Z",
     "start_time": "2023-05-26T13:16:39.333878Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "origin_raw_data_dir = 'C:\\\\Users\\\\a1882\\\\Desktop\\\\EEG\\\\normal\\\\data\\\\128_s100'\n",
    "model_save ='C:\\\\Users\\\\a1882\\\\Desktop\\\\EEG\\\\normal\\\\model'\n",
    "pic_dir = 'C:\\\\Users\\\\a1882\\\\Desktop\\\\EEG\\\\normal\\\\pic'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T13:16:54.479055900Z",
     "start_time": "2023-05-26T13:16:54.469058900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EEG_Dataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = origin_raw_data_dir\n",
    "        self.file_list = os.listdir(self.data_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        file_path = os.path.join(self.data_dir, file_name)\n",
    "        data = pd.read_csv(file_path, header=None)\n",
    "        data = data.values\n",
    "        data = torch.from_numpy(data)\n",
    "        label_map = {'lefthand': 0, 'read': 1, 'rest': 2, 'walkbase': 3, 'walkl': 4 ,'walkfocus': 5}\n",
    "        data_label = label_map[file_name.split('_')[0]]\n",
    "        return data, data_label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T13:16:54.521593600Z",
     "start_time": "2023-05-26T13:16:54.488615200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = EEG_Dataset(origin_raw_data_dir)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T13:16:54.953738900Z",
     "start_time": "2023-05-26T13:16:54.514598400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, input_size=2048, hidden_size=128, num_layers=3, num_classes=6):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(2, 1), stride=(2, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.dropout3 = nn.Dropout(p=0.25)\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        # print('x:', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print('conv1:', x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        # print('pool1:', x.shape)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        # print('conv2:', x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        # print('pool2:', x.shape)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        # print('conv3:',x.shape)\n",
    "        x = self.bn3(x)\n",
    "        # x = torch.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        # print('pool3:',x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print('flatten:', x.shape)\n",
    "        x = torch.unsqueeze(x,0)\n",
    "        # x = self.fc1(x)\n",
    "        # # print('fc1:', x.shape)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.dropout3(x)\n",
    "        # x = self.fc2(x)\n",
    "        # # print('fc2:', x.shape)\n",
    "        # return x\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T13:16:55.011311Z",
     "start_time": "2023-05-26T13:16:54.954741700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T13:18:14.830781200Z",
     "start_time": "2023-05-26T13:18:14.807769400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(size=( 1, 32, 128), dtype=torch.float32)\n",
    "model = EEGNet()\n",
    "output = model(X)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T13:18:16.771851300Z",
     "start_time": "2023-05-26T13:18:15.556405500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T13:18:17.410572800Z",
     "start_time": "2023-05-26T13:18:17.381556600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            global train_loss\n",
    "            train_loss.append(loss)\n",
    "\n",
    "\n",
    "\n",
    "def test(dataloader, model,loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error:\\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    global valid_loss\n",
    "    valid_loss.append(test_loss)\n",
    "    global accuracy\n",
    "    accuracy.append(correct)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T13:18:17.942781300Z",
     "start_time": "2023-05-26T13:18:17.891782200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.844248  [    0/ 3788]\n",
      "loss: 1.829766  [  100/ 3788]\n",
      "loss: 1.789028  [  200/ 3788]\n",
      "loss: 1.604908  [  300/ 3788]\n",
      "loss: 1.686073  [  400/ 3788]\n",
      "loss: 1.299647  [  500/ 3788]\n",
      "loss: 2.601411  [  600/ 3788]\n",
      "loss: 0.969588  [  700/ 3788]\n",
      "loss: 0.881017  [  800/ 3788]\n",
      "loss: 0.477150  [  900/ 3788]\n",
      "loss: 0.338089  [ 1000/ 3788]\n",
      "loss: 0.650373  [ 1100/ 3788]\n",
      "loss: 0.565302  [ 1200/ 3788]\n",
      "loss: 1.036354  [ 1300/ 3788]\n",
      "loss: 0.823851  [ 1400/ 3788]\n",
      "loss: 0.576621  [ 1500/ 3788]\n",
      "loss: 0.023842  [ 1600/ 3788]\n",
      "loss: 0.237979  [ 1700/ 3788]\n",
      "loss: 0.758031  [ 1800/ 3788]\n",
      "loss: 0.016432  [ 1900/ 3788]\n",
      "loss: 0.960991  [ 2000/ 3788]\n",
      "loss: 0.045310  [ 2100/ 3788]\n",
      "loss: 0.019403  [ 2200/ 3788]\n",
      "loss: 0.478211  [ 2300/ 3788]\n",
      "loss: 1.393528  [ 2400/ 3788]\n",
      "loss: 0.009193  [ 2500/ 3788]\n",
      "loss: 0.456142  [ 2600/ 3788]\n",
      "loss: 0.057547  [ 2700/ 3788]\n",
      "loss: 0.009590  [ 2800/ 3788]\n",
      "loss: 0.328961  [ 2900/ 3788]\n",
      "loss: 0.455251  [ 3000/ 3788]\n",
      "loss: 0.607145  [ 3100/ 3788]\n",
      "loss: 0.317920  [ 3200/ 3788]\n",
      "loss: 0.350232  [ 3300/ 3788]\n",
      "loss: 1.078422  [ 3400/ 3788]\n",
      "loss: 0.145862  [ 3500/ 3788]\n",
      "loss: 0.692116  [ 3600/ 3788]\n",
      "loss: 0.636148  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 71.5%, Avg loss: 1.039517 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.039289  [    0/ 3788]\n",
      "loss: 0.044208  [  100/ 3788]\n",
      "loss: 0.285782  [  200/ 3788]\n",
      "loss: 0.407868  [  300/ 3788]\n",
      "loss: 0.165930  [  400/ 3788]\n",
      "loss: 0.547493  [  500/ 3788]\n",
      "loss: 0.109206  [  600/ 3788]\n",
      "loss: 0.159726  [  700/ 3788]\n",
      "loss: 0.053142  [  800/ 3788]\n",
      "loss: 0.259803  [  900/ 3788]\n",
      "loss: 0.096985  [ 1000/ 3788]\n",
      "loss: 0.076206  [ 1100/ 3788]\n",
      "loss: 0.032176  [ 1200/ 3788]\n",
      "loss: 0.052309  [ 1300/ 3788]\n",
      "loss: 0.408668  [ 1400/ 3788]\n",
      "loss: 0.195628  [ 1500/ 3788]\n",
      "loss: 0.042086  [ 1600/ 3788]\n",
      "loss: 0.432355  [ 1700/ 3788]\n",
      "loss: 0.038975  [ 1800/ 3788]\n",
      "loss: 0.216512  [ 1900/ 3788]\n",
      "loss: 0.145154  [ 2000/ 3788]\n",
      "loss: 0.356173  [ 2100/ 3788]\n",
      "loss: 0.038904  [ 2200/ 3788]\n",
      "loss: 0.028361  [ 2300/ 3788]\n",
      "loss: 0.135041  [ 2400/ 3788]\n",
      "loss: 0.005528  [ 2500/ 3788]\n",
      "loss: 0.044645  [ 2600/ 3788]\n",
      "loss: 0.002735  [ 2700/ 3788]\n",
      "loss: 0.003585  [ 2800/ 3788]\n",
      "loss: 0.343238  [ 2900/ 3788]\n",
      "loss: 0.039905  [ 3000/ 3788]\n",
      "loss: 0.049569  [ 3100/ 3788]\n",
      "loss: 0.001959  [ 3200/ 3788]\n",
      "loss: 1.731620  [ 3300/ 3788]\n",
      "loss: 0.001993  [ 3400/ 3788]\n",
      "loss: 0.338105  [ 3500/ 3788]\n",
      "loss: 0.784893  [ 3600/ 3788]\n",
      "loss: 0.026215  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 71.8%, Avg loss: 0.991994 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.002007  [    0/ 3788]\n",
      "loss: 0.001944  [  100/ 3788]\n",
      "loss: 0.010157  [  200/ 3788]\n",
      "loss: 0.621825  [  300/ 3788]\n",
      "loss: 0.061235  [  400/ 3788]\n",
      "loss: 0.027918  [  500/ 3788]\n",
      "loss: 0.017640  [  600/ 3788]\n",
      "loss: 0.001841  [  700/ 3788]\n",
      "loss: 0.021985  [  800/ 3788]\n",
      "loss: 0.051614  [  900/ 3788]\n",
      "loss: 0.010393  [ 1000/ 3788]\n",
      "loss: 0.001210  [ 1100/ 3788]\n",
      "loss: 0.081865  [ 1200/ 3788]\n",
      "loss: 0.020982  [ 1300/ 3788]\n",
      "loss: 0.028453  [ 1400/ 3788]\n",
      "loss: 0.024073  [ 1500/ 3788]\n",
      "loss: 0.001481  [ 1600/ 3788]\n",
      "loss: 0.001427  [ 1700/ 3788]\n",
      "loss: 0.008097  [ 1800/ 3788]\n",
      "loss: 0.016073  [ 1900/ 3788]\n",
      "loss: 0.077148  [ 2000/ 3788]\n",
      "loss: 0.008653  [ 2100/ 3788]\n",
      "loss: 0.056058  [ 2200/ 3788]\n",
      "loss: 0.004108  [ 2300/ 3788]\n",
      "loss: 0.023697  [ 2400/ 3788]\n",
      "loss: 0.001149  [ 2500/ 3788]\n",
      "loss: 0.961296  [ 2600/ 3788]\n",
      "loss: 0.006252  [ 2700/ 3788]\n",
      "loss: 0.002693  [ 2800/ 3788]\n",
      "loss: 0.001444  [ 2900/ 3788]\n",
      "loss: 0.004691  [ 3000/ 3788]\n",
      "loss: 0.001054  [ 3100/ 3788]\n",
      "loss: 0.001817  [ 3200/ 3788]\n",
      "loss: 0.018961  [ 3300/ 3788]\n",
      "loss: 0.001341  [ 3400/ 3788]\n",
      "loss: 0.034274  [ 3500/ 3788]\n",
      "loss: 0.004860  [ 3600/ 3788]\n",
      "loss: 0.002671  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 71.6%, Avg loss: 1.029909 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.001634  [    0/ 3788]\n",
      "loss: 0.258364  [  100/ 3788]\n",
      "loss: 0.081431  [  200/ 3788]\n",
      "loss: 0.004345  [  300/ 3788]\n",
      "loss: 0.002137  [  400/ 3788]\n",
      "loss: 0.173582  [  500/ 3788]\n",
      "loss: 0.133778  [  600/ 3788]\n",
      "loss: 0.003201  [  700/ 3788]\n",
      "loss: 0.001382  [  800/ 3788]\n",
      "loss: 0.001812  [  900/ 3788]\n",
      "loss: 0.002062  [ 1000/ 3788]\n",
      "loss: 0.003311  [ 1100/ 3788]\n",
      "loss: 0.002407  [ 1200/ 3788]\n",
      "loss: 0.006575  [ 1300/ 3788]\n",
      "loss: 0.018013  [ 1400/ 3788]\n",
      "loss: 0.003891  [ 1500/ 3788]\n",
      "loss: 0.002085  [ 1600/ 3788]\n",
      "loss: 0.097621  [ 1700/ 3788]\n",
      "loss: 0.003516  [ 1800/ 3788]\n",
      "loss: 0.007105  [ 1900/ 3788]\n",
      "loss: 0.002014  [ 2000/ 3788]\n",
      "loss: 0.001148  [ 2100/ 3788]\n",
      "loss: 0.018586  [ 2200/ 3788]\n",
      "loss: 0.006216  [ 2300/ 3788]\n",
      "loss: 0.013046  [ 2400/ 3788]\n",
      "loss: 0.001431  [ 2500/ 3788]\n",
      "loss: 0.004359  [ 2600/ 3788]\n",
      "loss: 0.000514  [ 2700/ 3788]\n",
      "loss: 0.001882  [ 2800/ 3788]\n",
      "loss: 0.002675  [ 2900/ 3788]\n",
      "loss: 0.005690  [ 3000/ 3788]\n",
      "loss: 0.000599  [ 3100/ 3788]\n",
      "loss: 0.050271  [ 3200/ 3788]\n",
      "loss: 0.008578  [ 3300/ 3788]\n",
      "loss: 0.002726  [ 3400/ 3788]\n",
      "loss: 0.004682  [ 3500/ 3788]\n",
      "loss: 0.047850  [ 3600/ 3788]\n",
      "loss: 0.002775  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 61.5%, Avg loss: 1.186722 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.002848  [    0/ 3788]\n",
      "loss: 0.000538  [  100/ 3788]\n",
      "loss: 0.024737  [  200/ 3788]\n",
      "loss: 0.001242  [  300/ 3788]\n",
      "loss: 0.003837  [  400/ 3788]\n",
      "loss: 0.001389  [  500/ 3788]\n",
      "loss: 0.001879  [  600/ 3788]\n",
      "loss: 0.004243  [  700/ 3788]\n",
      "loss: 0.018707  [  800/ 3788]\n",
      "loss: 0.005909  [  900/ 3788]\n",
      "loss: 0.000890  [ 1000/ 3788]\n",
      "loss: 0.020555  [ 1100/ 3788]\n",
      "loss: 0.000708  [ 1200/ 3788]\n",
      "loss: 0.003451  [ 1300/ 3788]\n",
      "loss: 0.001976  [ 1400/ 3788]\n",
      "loss: 0.147486  [ 1500/ 3788]\n",
      "loss: 0.038126  [ 1600/ 3788]\n",
      "loss: 0.002111  [ 1700/ 3788]\n",
      "loss: 0.039919  [ 1800/ 3788]\n",
      "loss: 0.005297  [ 1900/ 3788]\n",
      "loss: 0.003492  [ 2000/ 3788]\n",
      "loss: 0.093511  [ 2100/ 3788]\n",
      "loss: 0.000973  [ 2200/ 3788]\n",
      "loss: 0.011244  [ 2300/ 3788]\n",
      "loss: 5.583694  [ 2400/ 3788]\n",
      "loss: 0.000868  [ 2500/ 3788]\n",
      "loss: 0.003878  [ 2600/ 3788]\n",
      "loss: 0.001067  [ 2700/ 3788]\n",
      "loss: 0.003360  [ 2800/ 3788]\n",
      "loss: 0.144681  [ 2900/ 3788]\n",
      "loss: 0.007132  [ 3000/ 3788]\n",
      "loss: 0.003044  [ 3100/ 3788]\n",
      "loss: 0.063684  [ 3200/ 3788]\n",
      "loss: 0.003405  [ 3300/ 3788]\n",
      "loss: 0.341885  [ 3400/ 3788]\n",
      "loss: 0.000285  [ 3500/ 3788]\n",
      "loss: 0.017354  [ 3600/ 3788]\n",
      "loss: 0.000233  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 75.7%, Avg loss: 1.007266 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.001079  [    0/ 3788]\n",
      "loss: 0.000301  [  100/ 3788]\n",
      "loss: 0.033869  [  200/ 3788]\n",
      "loss: 0.000828  [  300/ 3788]\n",
      "loss: 0.000909  [  400/ 3788]\n",
      "loss: 0.000567  [  500/ 3788]\n",
      "loss: 0.001646  [  600/ 3788]\n",
      "loss: 0.416549  [  700/ 3788]\n",
      "loss: 0.007418  [  800/ 3788]\n",
      "loss: 0.001126  [  900/ 3788]\n",
      "loss: 0.004142  [ 1000/ 3788]\n",
      "loss: 0.000257  [ 1100/ 3788]\n",
      "loss: 0.002940  [ 1200/ 3788]\n",
      "loss: 0.000217  [ 1300/ 3788]\n",
      "loss: 0.000663  [ 1400/ 3788]\n",
      "loss: 0.000219  [ 1500/ 3788]\n",
      "loss: 0.000625  [ 1600/ 3788]\n",
      "loss: 0.001234  [ 1700/ 3788]\n",
      "loss: 0.000218  [ 1800/ 3788]\n",
      "loss: 0.583976  [ 1900/ 3788]\n",
      "loss: 0.001302  [ 2000/ 3788]\n",
      "loss: 0.001306  [ 2100/ 3788]\n",
      "loss: 0.001234  [ 2200/ 3788]\n",
      "loss: 0.000541  [ 2300/ 3788]\n",
      "loss: 0.000213  [ 2400/ 3788]\n",
      "loss: 0.000725  [ 2500/ 3788]\n",
      "loss: 0.043862  [ 2600/ 3788]\n",
      "loss: 0.002084  [ 2700/ 3788]\n",
      "loss: 0.001206  [ 2800/ 3788]\n",
      "loss: 0.000724  [ 2900/ 3788]\n",
      "loss: 0.000178  [ 3000/ 3788]\n",
      "loss: 0.001441  [ 3100/ 3788]\n",
      "loss: 0.189919  [ 3200/ 3788]\n",
      "loss: 0.000649  [ 3300/ 3788]\n",
      "loss: 0.000188  [ 3400/ 3788]\n",
      "loss: 0.036634  [ 3500/ 3788]\n",
      "loss: 0.004496  [ 3600/ 3788]\n",
      "loss: 0.001517  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 74.1%, Avg loss: 1.003496 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.000194  [    0/ 3788]\n",
      "loss: 0.002571  [  100/ 3788]\n",
      "loss: 0.002721  [  200/ 3788]\n",
      "loss: 0.001189  [  300/ 3788]\n",
      "loss: 0.001388  [  400/ 3788]\n",
      "loss: 0.013553  [  500/ 3788]\n",
      "loss: 0.001115  [  600/ 3788]\n",
      "loss: 0.000569  [  700/ 3788]\n",
      "loss: 0.000472  [  800/ 3788]\n",
      "loss: 0.056208  [  900/ 3788]\n",
      "loss: 0.019914  [ 1000/ 3788]\n",
      "loss: 0.000144  [ 1100/ 3788]\n",
      "loss: 0.020211  [ 1200/ 3788]\n",
      "loss: 0.000863  [ 1300/ 3788]\n",
      "loss: 0.000185  [ 1400/ 3788]\n",
      "loss: 0.000980  [ 1500/ 3788]\n",
      "loss: 0.002866  [ 1600/ 3788]\n",
      "loss: 0.005570  [ 1700/ 3788]\n",
      "loss: 0.001615  [ 1800/ 3788]\n",
      "loss: 0.000267  [ 1900/ 3788]\n",
      "loss: 0.001318  [ 2000/ 3788]\n",
      "loss: 0.000132  [ 2100/ 3788]\n",
      "loss: 0.000118  [ 2200/ 3788]\n",
      "loss: 0.000332  [ 2300/ 3788]\n",
      "loss: 0.001294  [ 2400/ 3788]\n",
      "loss: 0.000379  [ 2500/ 3788]\n",
      "loss: 0.000382  [ 2600/ 3788]\n",
      "loss: 0.001292  [ 2700/ 3788]\n",
      "loss: 0.000218  [ 2800/ 3788]\n",
      "loss: 0.003558  [ 2900/ 3788]\n",
      "loss: 0.001238  [ 3000/ 3788]\n",
      "loss: 0.005312  [ 3100/ 3788]\n",
      "loss: 0.000399  [ 3200/ 3788]\n",
      "loss: 0.000832  [ 3300/ 3788]\n",
      "loss: 0.000255  [ 3400/ 3788]\n",
      "loss: 0.000346  [ 3500/ 3788]\n",
      "loss: 0.000882  [ 3600/ 3788]\n",
      "loss: 0.000161  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 67.4%, Avg loss: 1.050567 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.000114  [    0/ 3788]\n",
      "loss: 0.000303  [  100/ 3788]\n",
      "loss: 0.011530  [  200/ 3788]\n",
      "loss: 0.000455  [  300/ 3788]\n",
      "loss: 0.000229  [  400/ 3788]\n",
      "loss: 0.000260  [  500/ 3788]\n",
      "loss: 0.000498  [  600/ 3788]\n",
      "loss: 0.000388  [  700/ 3788]\n",
      "loss: 0.000128  [  800/ 3788]\n",
      "loss: 0.000280  [  900/ 3788]\n",
      "loss: 0.000143  [ 1000/ 3788]\n",
      "loss: 0.000227  [ 1100/ 3788]\n",
      "loss: 0.000658  [ 1200/ 3788]\n",
      "loss: 0.000396  [ 1300/ 3788]\n",
      "loss: 0.001003  [ 1400/ 3788]\n",
      "loss: 0.000297  [ 1500/ 3788]\n",
      "loss: 0.000289  [ 1600/ 3788]\n",
      "loss: 0.005090  [ 1700/ 3788]\n",
      "loss: 0.000229  [ 1800/ 3788]\n",
      "loss: 0.000339  [ 1900/ 3788]\n",
      "loss: 0.000199  [ 2000/ 3788]\n",
      "loss: 0.000095  [ 2100/ 3788]\n",
      "loss: 0.002021  [ 2200/ 3788]\n",
      "loss: 0.001708  [ 2300/ 3788]\n",
      "loss: 0.764915  [ 2400/ 3788]\n",
      "loss: 0.000455  [ 2500/ 3788]\n",
      "loss: 0.000190  [ 2600/ 3788]\n",
      "loss: 0.013868  [ 2700/ 3788]\n",
      "loss: 0.000122  [ 2800/ 3788]\n",
      "loss: 0.000289  [ 2900/ 3788]\n",
      "loss: 0.000289  [ 3000/ 3788]\n",
      "loss: 0.000119  [ 3100/ 3788]\n",
      "loss: 0.000469  [ 3200/ 3788]\n",
      "loss: 0.000968  [ 3300/ 3788]\n",
      "loss: 0.000191  [ 3400/ 3788]\n",
      "loss: 0.111562  [ 3500/ 3788]\n",
      "loss: 0.000255  [ 3600/ 3788]\n",
      "loss: 0.008150  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 36.9%, Avg loss: 1.612943 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.000062  [    0/ 3788]\n",
      "loss: 0.000228  [  100/ 3788]\n",
      "loss: 0.001931  [  200/ 3788]\n",
      "loss: 0.000074  [  300/ 3788]\n",
      "loss: 0.000413  [  400/ 3788]\n",
      "loss: 0.000058  [  500/ 3788]\n",
      "loss: 0.000194  [  600/ 3788]\n",
      "loss: 0.000289  [  700/ 3788]\n",
      "loss: 0.000260  [  800/ 3788]\n",
      "loss: 0.000442  [  900/ 3788]\n",
      "loss: 0.000102  [ 1000/ 3788]\n",
      "loss: 0.000451  [ 1100/ 3788]\n",
      "loss: 0.000218  [ 1200/ 3788]\n",
      "loss: 0.000186  [ 1300/ 3788]\n",
      "loss: 0.000342  [ 1400/ 3788]\n",
      "loss: 0.000223  [ 1500/ 3788]\n",
      "loss: 0.000155  [ 1600/ 3788]\n",
      "loss: 0.000281  [ 1700/ 3788]\n",
      "loss: 0.005067  [ 1800/ 3788]\n",
      "loss: 0.000171  [ 1900/ 3788]\n",
      "loss: 0.000215  [ 2000/ 3788]\n",
      "loss: 0.001938  [ 2100/ 3788]\n",
      "loss: 0.000072  [ 2200/ 3788]\n",
      "loss: 0.006221  [ 2300/ 3788]\n",
      "loss: 0.000076  [ 2400/ 3788]\n",
      "loss: 0.000060  [ 2500/ 3788]\n",
      "loss: 0.000374  [ 2600/ 3788]\n",
      "loss: 0.000669  [ 2700/ 3788]\n",
      "loss: 0.000182  [ 2800/ 3788]\n",
      "loss: 0.000314  [ 2900/ 3788]\n",
      "loss: 0.000185  [ 3000/ 3788]\n",
      "loss: 0.000068  [ 3100/ 3788]\n",
      "loss: 0.000229  [ 3200/ 3788]\n",
      "loss: 0.000355  [ 3300/ 3788]\n",
      "loss: 0.000105  [ 3400/ 3788]\n",
      "loss: 0.000139  [ 3500/ 3788]\n",
      "loss: 0.000059  [ 3600/ 3788]\n",
      "loss: 0.000802  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 67.5%, Avg loss: 1.093139 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.000065  [    0/ 3788]\n",
      "loss: 0.000458  [  100/ 3788]\n",
      "loss: 0.000978  [  200/ 3788]\n",
      "loss: 0.000273  [  300/ 3788]\n",
      "loss: 0.000074  [  400/ 3788]\n",
      "loss: 0.001346  [  500/ 3788]\n",
      "loss: 0.000114  [  600/ 3788]\n",
      "loss: 0.000291  [  700/ 3788]\n",
      "loss: 0.002775  [  800/ 3788]\n",
      "loss: 0.009342  [  900/ 3788]\n",
      "loss: 0.000191  [ 1000/ 3788]\n",
      "loss: 0.000262  [ 1100/ 3788]\n",
      "loss: 0.000326  [ 1200/ 3788]\n",
      "loss: 0.000640  [ 1300/ 3788]\n",
      "loss: 0.000176  [ 1400/ 3788]\n",
      "loss: 0.004000  [ 1500/ 3788]\n",
      "loss: 0.001259  [ 1600/ 3788]\n",
      "loss: 0.000318  [ 1700/ 3788]\n",
      "loss: 0.001357  [ 1800/ 3788]\n",
      "loss: 0.001276  [ 1900/ 3788]\n",
      "loss: 0.000562  [ 2000/ 3788]\n",
      "loss: 0.001221  [ 2100/ 3788]\n",
      "loss: 0.000779  [ 2200/ 3788]\n",
      "loss: 0.000082  [ 2300/ 3788]\n",
      "loss: 0.001757  [ 2400/ 3788]\n",
      "loss: 0.000128  [ 2500/ 3788]\n",
      "loss: 0.005328  [ 2600/ 3788]\n",
      "loss: 0.000569  [ 2700/ 3788]\n",
      "loss: 0.000142  [ 2800/ 3788]\n",
      "loss: 0.072231  [ 2900/ 3788]\n",
      "loss: 0.000125  [ 3000/ 3788]\n",
      "loss: 0.000810  [ 3100/ 3788]\n",
      "loss: 0.000699  [ 3200/ 3788]\n",
      "loss: 0.000087  [ 3300/ 3788]\n",
      "loss: 0.000060  [ 3400/ 3788]\n",
      "loss: 0.018605  [ 3500/ 3788]\n",
      "loss: 0.000045  [ 3600/ 3788]\n",
      "loss: 0.000144  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 64.4%, Avg loss: 1.307991 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.000090  [    0/ 3788]\n",
      "loss: 0.002394  [  100/ 3788]\n",
      "loss: 0.000056  [  200/ 3788]\n",
      "loss: 0.000235  [  300/ 3788]\n",
      "loss: 0.000286  [  400/ 3788]\n",
      "loss: 0.000131  [  500/ 3788]\n",
      "loss: 0.000099  [  600/ 3788]\n",
      "loss: 0.002790  [  700/ 3788]\n",
      "loss: 0.000058  [  800/ 3788]\n",
      "loss: 0.001484  [  900/ 3788]\n",
      "loss: 0.001013  [ 1000/ 3788]\n",
      "loss: 0.000057  [ 1100/ 3788]\n",
      "loss: 0.000338  [ 1200/ 3788]\n",
      "loss: 0.000075  [ 1300/ 3788]\n",
      "loss: 0.000394  [ 1400/ 3788]\n",
      "loss: 0.000158  [ 1500/ 3788]\n",
      "loss: 0.000720  [ 1600/ 3788]\n",
      "loss: 0.002217  [ 1700/ 3788]\n",
      "loss: 0.000060  [ 1800/ 3788]\n",
      "loss: 0.000774  [ 1900/ 3788]\n",
      "loss: 0.000829  [ 2000/ 3788]\n",
      "loss: 0.000249  [ 2100/ 3788]\n",
      "loss: 0.000040  [ 2200/ 3788]\n",
      "loss: 0.000047  [ 2300/ 3788]\n",
      "loss: 0.000364  [ 2400/ 3788]\n",
      "loss: 0.000302  [ 2500/ 3788]\n",
      "loss: 0.000045  [ 2600/ 3788]\n",
      "loss: 0.000467  [ 2700/ 3788]\n",
      "loss: 0.000166  [ 2800/ 3788]\n",
      "loss: 0.000265  [ 2900/ 3788]\n",
      "loss: 0.000043  [ 3000/ 3788]\n",
      "loss: 0.000089  [ 3100/ 3788]\n",
      "loss: 0.000033  [ 3200/ 3788]\n",
      "loss: 0.000235  [ 3300/ 3788]\n",
      "loss: 0.000992  [ 3400/ 3788]\n",
      "loss: 0.006057  [ 3500/ 3788]\n",
      "loss: 0.000126  [ 3600/ 3788]\n",
      "loss: 0.000050  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 42.9%, Avg loss: 1.533151 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.000038  [    0/ 3788]\n",
      "loss: 0.000385  [  100/ 3788]\n",
      "loss: 0.000076  [  200/ 3788]\n",
      "loss: 0.000297  [  300/ 3788]\n",
      "loss: 0.000225  [  400/ 3788]\n",
      "loss: 0.000038  [  500/ 3788]\n",
      "loss: 0.000167  [  600/ 3788]\n",
      "loss: 0.000201  [  700/ 3788]\n",
      "loss: 0.000468  [  800/ 3788]\n",
      "loss: 0.000033  [  900/ 3788]\n",
      "loss: 0.000164  [ 1000/ 3788]\n",
      "loss: 0.003316  [ 1100/ 3788]\n",
      "loss: 0.000086  [ 1200/ 3788]\n",
      "loss: 0.000077  [ 1300/ 3788]\n",
      "loss: 0.000110  [ 1400/ 3788]\n",
      "loss: 0.000109  [ 1500/ 3788]\n",
      "loss: 0.000559  [ 1600/ 3788]\n",
      "loss: 0.001014  [ 1700/ 3788]\n",
      "loss: 0.000209  [ 1800/ 3788]\n",
      "loss: 0.000138  [ 1900/ 3788]\n",
      "loss: 0.000085  [ 2000/ 3788]\n",
      "loss: 0.001833  [ 2100/ 3788]\n",
      "loss: 0.000021  [ 2200/ 3788]\n",
      "loss: 0.000029  [ 2300/ 3788]\n",
      "loss: 0.000104  [ 2400/ 3788]\n",
      "loss: 0.000131  [ 2500/ 3788]\n",
      "loss: 0.000527  [ 2600/ 3788]\n",
      "loss: 0.001474  [ 2700/ 3788]\n",
      "loss: 0.000113  [ 2800/ 3788]\n",
      "loss: 0.000030  [ 2900/ 3788]\n",
      "loss: 0.000097  [ 3000/ 3788]\n",
      "loss: 0.004634  [ 3100/ 3788]\n",
      "loss: 0.010892  [ 3200/ 3788]\n",
      "loss: 0.000138  [ 3300/ 3788]\n",
      "loss: 0.000125  [ 3400/ 3788]\n",
      "loss: 0.000033  [ 3500/ 3788]\n",
      "loss: 0.000073  [ 3600/ 3788]\n",
      "loss: 0.000104  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 64.3%, Avg loss: 1.211187 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.004309  [    0/ 3788]\n",
      "loss: 0.000058  [  100/ 3788]\n",
      "loss: 0.000165  [  200/ 3788]\n",
      "loss: 0.000223  [  300/ 3788]\n",
      "loss: 0.000075  [  400/ 3788]\n",
      "loss: 0.000047  [  500/ 3788]\n",
      "loss: 0.000806  [  600/ 3788]\n",
      "loss: 0.000115  [  700/ 3788]\n",
      "loss: 0.010324  [  800/ 3788]\n",
      "loss: 0.000889  [  900/ 3788]\n",
      "loss: 0.000064  [ 1000/ 3788]\n",
      "loss: 0.000127  [ 1100/ 3788]\n",
      "loss: 0.000263  [ 1200/ 3788]\n",
      "loss: 0.000032  [ 1300/ 3788]\n",
      "loss: 0.000029  [ 1400/ 3788]\n",
      "loss: 0.000051  [ 1500/ 3788]\n",
      "loss: 0.000034  [ 1600/ 3788]\n",
      "loss: 0.000421  [ 1700/ 3788]\n",
      "loss: 0.000298  [ 1800/ 3788]\n",
      "loss: 0.000156  [ 1900/ 3788]\n",
      "loss: 0.000122  [ 2000/ 3788]\n",
      "loss: 0.000232  [ 2100/ 3788]\n",
      "loss: 0.000104  [ 2200/ 3788]\n",
      "loss: 0.000056  [ 2300/ 3788]\n",
      "loss: 0.000107  [ 2400/ 3788]\n",
      "loss: 0.000037  [ 2500/ 3788]\n",
      "loss: 0.000180  [ 2600/ 3788]\n",
      "loss: 0.000176  [ 2700/ 3788]\n",
      "loss: 0.000104  [ 2800/ 3788]\n",
      "loss: 0.000111  [ 2900/ 3788]\n",
      "loss: 0.000039  [ 3000/ 3788]\n",
      "loss: 0.790974  [ 3100/ 3788]\n",
      "loss: 0.000060  [ 3200/ 3788]\n",
      "loss: 0.000270  [ 3300/ 3788]\n",
      "loss: 0.000463  [ 3400/ 3788]\n",
      "loss: 0.009810  [ 3500/ 3788]\n",
      "loss: 0.000102  [ 3600/ 3788]\n",
      "loss: 0.000046  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 75.3%, Avg loss: 0.927323 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.005639  [    0/ 3788]\n",
      "loss: 0.000405  [  100/ 3788]\n",
      "loss: 0.000125  [  200/ 3788]\n",
      "loss: 0.000065  [  300/ 3788]\n",
      "loss: 0.000117  [  400/ 3788]\n",
      "loss: 0.000114  [  500/ 3788]\n",
      "loss: 0.000055  [  600/ 3788]\n",
      "loss: 0.000144  [  700/ 3788]\n",
      "loss: 0.000038  [  800/ 3788]\n",
      "loss: 0.000160  [  900/ 3788]\n",
      "loss: 0.000208  [ 1000/ 3788]\n",
      "loss: 0.000126  [ 1100/ 3788]\n",
      "loss: 0.000060  [ 1200/ 3788]\n",
      "loss: 0.000168  [ 1300/ 3788]\n",
      "loss: 0.000139  [ 1400/ 3788]\n",
      "loss: 0.000139  [ 1500/ 3788]\n",
      "loss: 0.000067  [ 1600/ 3788]\n",
      "loss: 0.000135  [ 1700/ 3788]\n",
      "loss: 0.000090  [ 1800/ 3788]\n",
      "loss: 0.000024  [ 1900/ 3788]\n",
      "loss: 0.000018  [ 2000/ 3788]\n",
      "loss: 0.000153  [ 2100/ 3788]\n",
      "loss: 0.000603  [ 2200/ 3788]\n",
      "loss: 0.000019  [ 2300/ 3788]\n",
      "loss: 0.000311  [ 2400/ 3788]\n",
      "loss: 0.000024  [ 2500/ 3788]\n",
      "loss: 0.000127  [ 2600/ 3788]\n",
      "loss: 0.000218  [ 2700/ 3788]\n",
      "loss: 0.000213  [ 2800/ 3788]\n",
      "loss: 0.000065  [ 2900/ 3788]\n",
      "loss: 0.000200  [ 3000/ 3788]\n",
      "loss: 0.000052  [ 3100/ 3788]\n",
      "loss: 0.000934  [ 3200/ 3788]\n",
      "loss: 0.000099  [ 3300/ 3788]\n",
      "loss: 0.000080  [ 3400/ 3788]\n",
      "loss: 0.000061  [ 3500/ 3788]\n",
      "loss: 0.000140  [ 3600/ 3788]\n",
      "loss: 0.000046  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 47.0%, Avg loss: 1.488072 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.000041  [    0/ 3788]\n",
      "loss: 0.000100  [  100/ 3788]\n",
      "loss: 0.000159  [  200/ 3788]\n",
      "loss: 0.000034  [  300/ 3788]\n",
      "loss: 0.000055  [  400/ 3788]\n",
      "loss: 0.000609  [  500/ 3788]\n",
      "loss: 0.000017  [  600/ 3788]\n",
      "loss: 0.000106  [  700/ 3788]\n",
      "loss: 0.000309  [  800/ 3788]\n",
      "loss: 0.000079  [  900/ 3788]\n",
      "loss: 0.000075  [ 1000/ 3788]\n",
      "loss: 0.000085  [ 1100/ 3788]\n",
      "loss: 0.000089  [ 1200/ 3788]\n",
      "loss: 0.000531  [ 1300/ 3788]\n",
      "loss: 0.000098  [ 1400/ 3788]\n",
      "loss: 0.000016  [ 1500/ 3788]\n",
      "loss: 0.000078  [ 1600/ 3788]\n",
      "loss: 0.000035  [ 1700/ 3788]\n",
      "loss: 0.000278  [ 1800/ 3788]\n",
      "loss: 0.000178  [ 1900/ 3788]\n",
      "loss: 0.000060  [ 2000/ 3788]\n",
      "loss: 0.000932  [ 2100/ 3788]\n",
      "loss: 0.000034  [ 2200/ 3788]\n",
      "loss: 0.000056  [ 2300/ 3788]\n",
      "loss: 0.000085  [ 2400/ 3788]\n",
      "loss: 0.000038  [ 2500/ 3788]\n",
      "loss: 0.001254  [ 2600/ 3788]\n",
      "loss: 0.000095  [ 2700/ 3788]\n",
      "loss: 0.004130  [ 2800/ 3788]\n",
      "loss: 0.000015  [ 2900/ 3788]\n",
      "loss: 0.000575  [ 3000/ 3788]\n",
      "loss: 0.029665  [ 3100/ 3788]\n",
      "loss: 0.000266  [ 3200/ 3788]\n",
      "loss: 0.000036  [ 3300/ 3788]\n",
      "loss: 0.000017  [ 3400/ 3788]\n",
      "loss: 0.000405  [ 3500/ 3788]\n",
      "loss: 0.000093  [ 3600/ 3788]\n",
      "loss: 0.000055  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 48.9%, Avg loss: 1.428676 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.000014  [    0/ 3788]\n",
      "loss: 0.000055  [  100/ 3788]\n",
      "loss: 0.000018  [  200/ 3788]\n",
      "loss: 0.000028  [  300/ 3788]\n",
      "loss: 0.000384  [  400/ 3788]\n",
      "loss: 0.000024  [  500/ 3788]\n",
      "loss: 0.000012  [  600/ 3788]\n",
      "loss: 0.000007  [  700/ 3788]\n",
      "loss: 0.000008  [  800/ 3788]\n",
      "loss: 0.000027  [  900/ 3788]\n",
      "loss: 0.000460  [ 1000/ 3788]\n",
      "loss: 0.000007  [ 1100/ 3788]\n",
      "loss: 0.000008  [ 1200/ 3788]\n",
      "loss: 0.000231  [ 1300/ 3788]\n",
      "loss: 0.000110  [ 1400/ 3788]\n",
      "loss: 0.000077  [ 1500/ 3788]\n",
      "loss: 0.000004  [ 1600/ 3788]\n",
      "loss: 0.000005  [ 1700/ 3788]\n",
      "loss: 0.000081  [ 1800/ 3788]\n",
      "loss: 0.000072  [ 1900/ 3788]\n",
      "loss: 0.000007  [ 2000/ 3788]\n",
      "loss: 0.001946  [ 2100/ 3788]\n",
      "loss: 0.000118  [ 2200/ 3788]\n",
      "loss: 0.000043  [ 2300/ 3788]\n",
      "loss: 0.000932  [ 2400/ 3788]\n",
      "loss: 0.000016  [ 2500/ 3788]\n",
      "loss: 0.000879  [ 2600/ 3788]\n",
      "loss: 0.000004  [ 2700/ 3788]\n",
      "loss: 0.000009  [ 2800/ 3788]\n",
      "loss: 0.004307  [ 2900/ 3788]\n",
      "loss: 0.000054  [ 3000/ 3788]\n",
      "loss: 0.000734  [ 3100/ 3788]\n",
      "loss: 0.000200  [ 3200/ 3788]\n",
      "loss: 0.000072  [ 3300/ 3788]\n",
      "loss: 0.000039  [ 3400/ 3788]\n",
      "loss: 0.000077  [ 3500/ 3788]\n",
      "loss: 0.000072  [ 3600/ 3788]\n",
      "loss: 0.000083  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 49.4%, Avg loss: 1.353691 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.002564  [    0/ 3788]\n",
      "loss: 0.000510  [  100/ 3788]\n",
      "loss: 0.000028  [  200/ 3788]\n",
      "loss: 0.000053  [  300/ 3788]\n",
      "loss: 0.000066  [  400/ 3788]\n",
      "loss: 0.000378  [  500/ 3788]\n",
      "loss: 0.000268  [  600/ 3788]\n",
      "loss: 0.000057  [  700/ 3788]\n",
      "loss: 0.002235  [  800/ 3788]\n",
      "loss: 0.000707  [  900/ 3788]\n",
      "loss: 0.000326  [ 1000/ 3788]\n",
      "loss: 0.000087  [ 1100/ 3788]\n",
      "loss: 0.000074  [ 1200/ 3788]\n",
      "loss: 0.000053  [ 1300/ 3788]\n",
      "loss: 0.000100  [ 1400/ 3788]\n",
      "loss: 0.000174  [ 1500/ 3788]\n",
      "loss: 0.006971  [ 1600/ 3788]\n",
      "loss: 0.000031  [ 1700/ 3788]\n",
      "loss: 0.000079  [ 1800/ 3788]\n",
      "loss: 0.000110  [ 1900/ 3788]\n",
      "loss: 0.000608  [ 2000/ 3788]\n",
      "loss: 0.000399  [ 2100/ 3788]\n",
      "loss: 0.000166  [ 2200/ 3788]\n",
      "loss: 0.000084  [ 2300/ 3788]\n",
      "loss: 0.000006  [ 2400/ 3788]\n",
      "loss: 0.000004  [ 2500/ 3788]\n",
      "loss: 0.000032  [ 2600/ 3788]\n",
      "loss: 0.000297  [ 2700/ 3788]\n",
      "loss: 0.000243  [ 2800/ 3788]\n",
      "loss: 0.000056  [ 2900/ 3788]\n",
      "loss: 0.000054  [ 3000/ 3788]\n",
      "loss: 0.000033  [ 3100/ 3788]\n",
      "loss: 0.000040  [ 3200/ 3788]\n",
      "loss: 0.000107  [ 3300/ 3788]\n",
      "loss: 0.000031  [ 3400/ 3788]\n",
      "loss: 0.000154  [ 3500/ 3788]\n",
      "loss: 0.000032  [ 3600/ 3788]\n",
      "loss: 0.000031  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 58.6%, Avg loss: 1.564862 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.000105  [    0/ 3788]\n",
      "loss: 0.000043  [  100/ 3788]\n",
      "loss: 0.000471  [  200/ 3788]\n",
      "loss: 0.000026  [  300/ 3788]\n",
      "loss: 0.000035  [  400/ 3788]\n",
      "loss: 0.000231  [  500/ 3788]\n",
      "loss: 0.000049  [  600/ 3788]\n",
      "loss: 0.000100  [  700/ 3788]\n",
      "loss: 0.000661  [  800/ 3788]\n",
      "loss: 0.000013  [  900/ 3788]\n",
      "loss: 0.000091  [ 1000/ 3788]\n",
      "loss: 0.000006  [ 1100/ 3788]\n",
      "loss: 0.000004  [ 1200/ 3788]\n",
      "loss: 0.000220  [ 1300/ 3788]\n",
      "loss: 0.000096  [ 1400/ 3788]\n",
      "loss: 0.000094  [ 1500/ 3788]\n",
      "loss: 0.000137  [ 1600/ 3788]\n",
      "loss: 0.000004  [ 1700/ 3788]\n",
      "loss: 0.000050  [ 1800/ 3788]\n",
      "loss: 0.000049  [ 1900/ 3788]\n",
      "loss: 0.000061  [ 2000/ 3788]\n",
      "loss: 0.000054  [ 2100/ 3788]\n",
      "loss: 0.000007  [ 2200/ 3788]\n",
      "loss: 0.000062  [ 2300/ 3788]\n",
      "loss: 0.000043  [ 2400/ 3788]\n",
      "loss: 0.000135  [ 2500/ 3788]\n",
      "loss: 0.000282  [ 2600/ 3788]\n",
      "loss: 0.000014  [ 2700/ 3788]\n",
      "loss: 0.000039  [ 2800/ 3788]\n",
      "loss: 0.000062  [ 2900/ 3788]\n",
      "loss: 0.000717  [ 3000/ 3788]\n",
      "loss: 0.000141  [ 3100/ 3788]\n",
      "loss: 0.000085  [ 3200/ 3788]\n",
      "loss: 0.000560  [ 3300/ 3788]\n",
      "loss: 0.000044  [ 3400/ 3788]\n",
      "loss: 0.000067  [ 3500/ 3788]\n",
      "loss: 0.000231  [ 3600/ 3788]\n",
      "loss: 0.000022  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 78.5%, Avg loss: 1.174219 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.002595  [    0/ 3788]\n",
      "loss: 0.000117  [  100/ 3788]\n",
      "loss: 0.000004  [  200/ 3788]\n",
      "loss: 0.000203  [  300/ 3788]\n",
      "loss: 0.000058  [  400/ 3788]\n",
      "loss: 0.000021  [  500/ 3788]\n",
      "loss: 0.000018  [  600/ 3788]\n",
      "loss: 0.000043  [  700/ 3788]\n",
      "loss: 0.000022  [  800/ 3788]\n",
      "loss: 0.000079  [  900/ 3788]\n",
      "loss: 0.000098  [ 1000/ 3788]\n",
      "loss: 0.000121  [ 1100/ 3788]\n",
      "loss: 0.000044  [ 1200/ 3788]\n",
      "loss: 0.000204  [ 1300/ 3788]\n",
      "loss: 0.000028  [ 1400/ 3788]\n",
      "loss: 0.000027  [ 1500/ 3788]\n",
      "loss: 0.000042  [ 1600/ 3788]\n",
      "loss: 0.000038  [ 1700/ 3788]\n",
      "loss: 0.000032  [ 1800/ 3788]\n",
      "loss: 0.000082  [ 1900/ 3788]\n",
      "loss: 0.000003  [ 2000/ 3788]\n",
      "loss: 0.000024  [ 2100/ 3788]\n",
      "loss: 0.000074  [ 2200/ 3788]\n",
      "loss: 0.000014  [ 2300/ 3788]\n",
      "loss: 0.000008  [ 2400/ 3788]\n",
      "loss: 0.000021  [ 2500/ 3788]\n",
      "loss: 0.000023  [ 2600/ 3788]\n",
      "loss: 0.000059  [ 2700/ 3788]\n",
      "loss: 0.000035  [ 2800/ 3788]\n",
      "loss: 0.000023  [ 2900/ 3788]\n",
      "loss: 0.000016  [ 3000/ 3788]\n",
      "loss: 0.000021  [ 3100/ 3788]\n",
      "loss: 0.000055  [ 3200/ 3788]\n",
      "loss: 0.000040  [ 3300/ 3788]\n",
      "loss: 0.000027  [ 3400/ 3788]\n",
      "loss: 0.000119  [ 3500/ 3788]\n",
      "loss: 0.006062  [ 3600/ 3788]\n",
      "loss: 0.000086  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 58.9%, Avg loss: 1.221072 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.000027  [    0/ 3788]\n",
      "loss: 0.000037  [  100/ 3788]\n",
      "loss: 0.000052  [  200/ 3788]\n",
      "loss: 0.000024  [  300/ 3788]\n",
      "loss: 0.000021  [  400/ 3788]\n",
      "loss: 0.000507  [  500/ 3788]\n",
      "loss: 0.000014  [  600/ 3788]\n",
      "loss: 0.000012  [  700/ 3788]\n",
      "loss: 0.000385  [  800/ 3788]\n",
      "loss: 0.000036  [  900/ 3788]\n",
      "loss: 0.000005  [ 1000/ 3788]\n",
      "loss: 0.000177  [ 1100/ 3788]\n",
      "loss: 0.000027  [ 1200/ 3788]\n",
      "loss: 0.000004  [ 1300/ 3788]\n",
      "loss: 0.000012  [ 1400/ 3788]\n",
      "loss: 0.000007  [ 1500/ 3788]\n",
      "loss: 0.000081  [ 1600/ 3788]\n",
      "loss: 0.000033  [ 1700/ 3788]\n",
      "loss: 0.000024  [ 1800/ 3788]\n",
      "loss: 0.000023  [ 1900/ 3788]\n",
      "loss: 0.000006  [ 2000/ 3788]\n",
      "loss: 0.000011  [ 2100/ 3788]\n",
      "loss: 0.000041  [ 2200/ 3788]\n",
      "loss: 0.000014  [ 2300/ 3788]\n",
      "loss: 0.000130  [ 2400/ 3788]\n",
      "loss: 0.000014  [ 2500/ 3788]\n",
      "loss: 1.941697  [ 2600/ 3788]\n",
      "loss: 0.000041  [ 2700/ 3788]\n",
      "loss: 0.000035  [ 2800/ 3788]\n",
      "loss: 0.000141  [ 2900/ 3788]\n",
      "loss: 0.000015  [ 3000/ 3788]\n",
      "loss: 0.000016  [ 3100/ 3788]\n",
      "loss: 0.000004  [ 3200/ 3788]\n",
      "loss: 0.000004  [ 3300/ 3788]\n",
      "loss: 0.000040  [ 3400/ 3788]\n",
      "loss: 0.000110  [ 3500/ 3788]\n",
      "loss: 0.000005  [ 3600/ 3788]\n",
      "loss: 0.000041  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 42.1%, Avg loss: 1.502088 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.000134  [    0/ 3788]\n",
      "loss: 0.000015  [  100/ 3788]\n",
      "loss: 0.000046  [  200/ 3788]\n",
      "loss: 0.000012  [  300/ 3788]\n",
      "loss: 0.000004  [  400/ 3788]\n",
      "loss: 0.000117  [  500/ 3788]\n",
      "loss: 0.000028  [  600/ 3788]\n",
      "loss: 0.000176  [  700/ 3788]\n",
      "loss: 0.000011  [  800/ 3788]\n",
      "loss: 0.000008  [  900/ 3788]\n",
      "loss: 0.000038  [ 1000/ 3788]\n",
      "loss: 0.000014  [ 1100/ 3788]\n",
      "loss: 0.000067  [ 1200/ 3788]\n",
      "loss: 0.000112  [ 1300/ 3788]\n",
      "loss: 0.000016  [ 1400/ 3788]\n",
      "loss: 0.000013  [ 1500/ 3788]\n",
      "loss: 0.011403  [ 1600/ 3788]\n",
      "loss: 0.000185  [ 1700/ 3788]\n",
      "loss: 0.000022  [ 1800/ 3788]\n",
      "loss: 0.000058  [ 1900/ 3788]\n",
      "loss: 0.000008  [ 2000/ 3788]\n",
      "loss: 0.000219  [ 2100/ 3788]\n",
      "loss: 0.000038  [ 2200/ 3788]\n",
      "loss: 0.000049  [ 2300/ 3788]\n",
      "loss: 0.000098  [ 2400/ 3788]\n",
      "loss: 0.000051  [ 2500/ 3788]\n",
      "loss: 0.000004  [ 2600/ 3788]\n",
      "loss: 0.000030  [ 2700/ 3788]\n",
      "loss: 0.000009  [ 2800/ 3788]\n",
      "loss: 0.000270  [ 2900/ 3788]\n",
      "loss: 0.000243  [ 3000/ 3788]\n",
      "loss: 0.001114  [ 3100/ 3788]\n",
      "loss: 0.000037  [ 3200/ 3788]\n",
      "loss: 0.000343  [ 3300/ 3788]\n",
      "loss: 0.000118  [ 3400/ 3788]\n",
      "loss: 0.000013  [ 3500/ 3788]\n",
      "loss: 0.000042  [ 3600/ 3788]\n",
      "loss: 0.000156  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 65.7%, Avg loss: 1.094254 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.000069  [    0/ 3788]\n",
      "loss: 0.000220  [  100/ 3788]\n",
      "loss: 0.000140  [  200/ 3788]\n",
      "loss: 0.000054  [  300/ 3788]\n",
      "loss: 0.000003  [  400/ 3788]\n",
      "loss: 0.000104  [  500/ 3788]\n",
      "loss: 0.000180  [  600/ 3788]\n",
      "loss: 0.000037  [  700/ 3788]\n",
      "loss: 0.000045  [  800/ 3788]\n",
      "loss: 0.000011  [  900/ 3788]\n",
      "loss: 0.000008  [ 1000/ 3788]\n",
      "loss: 0.000003  [ 1100/ 3788]\n",
      "loss: 0.000003  [ 1200/ 3788]\n",
      "loss: 0.000024  [ 1300/ 3788]\n",
      "loss: 0.000182  [ 1400/ 3788]\n",
      "loss: 0.000019  [ 1500/ 3788]\n",
      "loss: 0.000011  [ 1600/ 3788]\n",
      "loss: 0.000007  [ 1700/ 3788]\n",
      "loss: 0.000003  [ 1800/ 3788]\n",
      "loss: 0.000062  [ 1900/ 3788]\n",
      "loss: 0.000051  [ 2000/ 3788]\n",
      "loss: 0.000031  [ 2100/ 3788]\n",
      "loss: 0.000023  [ 2200/ 3788]\n",
      "loss: 0.000011  [ 2300/ 3788]\n",
      "loss: 0.000008  [ 2400/ 3788]\n",
      "loss: 0.000011  [ 2500/ 3788]\n",
      "loss: 0.000063  [ 2600/ 3788]\n",
      "loss: 0.000004  [ 2700/ 3788]\n",
      "loss: 0.000094  [ 2800/ 3788]\n",
      "loss: 0.000050  [ 2900/ 3788]\n",
      "loss: 0.000003  [ 3000/ 3788]\n",
      "loss: 0.000049  [ 3100/ 3788]\n",
      "loss: 0.000007  [ 3200/ 3788]\n",
      "loss: 0.000575  [ 3300/ 3788]\n",
      "loss: 0.000042  [ 3400/ 3788]\n",
      "loss: 0.000010  [ 3500/ 3788]\n",
      "loss: 0.000012  [ 3600/ 3788]\n",
      "loss: 0.000254  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 53.4%, Avg loss: 1.353670 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 3788]\n",
      "loss: 0.000056  [  100/ 3788]\n",
      "loss: 0.000095  [  200/ 3788]\n",
      "loss: 0.000049  [  300/ 3788]\n",
      "loss: 0.000066  [  400/ 3788]\n",
      "loss: 0.000008  [  500/ 3788]\n",
      "loss: 0.000052  [  600/ 3788]\n",
      "loss: 0.000068  [  700/ 3788]\n",
      "loss: 0.000004  [  800/ 3788]\n",
      "loss: 0.000008  [  900/ 3788]\n",
      "loss: 0.000006  [ 1000/ 3788]\n",
      "loss: 0.000055  [ 1100/ 3788]\n",
      "loss: 0.000053  [ 1200/ 3788]\n",
      "loss: 0.000009  [ 1300/ 3788]\n",
      "loss: 0.000004  [ 1400/ 3788]\n",
      "loss: 0.000043  [ 1500/ 3788]\n",
      "loss: 0.000104  [ 1600/ 3788]\n",
      "loss: 0.000002  [ 1700/ 3788]\n",
      "loss: 0.000056  [ 1800/ 3788]\n",
      "loss: 0.000223  [ 1900/ 3788]\n",
      "loss: 0.000046  [ 2000/ 3788]\n",
      "loss: 0.000008  [ 2100/ 3788]\n",
      "loss: 0.000086  [ 2200/ 3788]\n",
      "loss: 0.000006  [ 2300/ 3788]\n",
      "loss: 0.000023  [ 2400/ 3788]\n",
      "loss: 0.000009  [ 2500/ 3788]\n",
      "loss: 0.000021  [ 2600/ 3788]\n",
      "loss: 0.000046  [ 2700/ 3788]\n",
      "loss: 0.000002  [ 2800/ 3788]\n",
      "loss: 0.000013  [ 2900/ 3788]\n",
      "loss: 0.000026  [ 3000/ 3788]\n",
      "loss: 0.001979  [ 3100/ 3788]\n",
      "loss: 0.000092  [ 3200/ 3788]\n",
      "loss: 0.000008  [ 3300/ 3788]\n",
      "loss: 0.000020  [ 3400/ 3788]\n",
      "loss: 0.000081  [ 3500/ 3788]\n",
      "loss: 0.000006  [ 3600/ 3788]\n",
      "loss: 0.000013  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 65.5%, Avg loss: 1.284796 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.000028  [    0/ 3788]\n",
      "loss: 0.000019  [  100/ 3788]\n",
      "loss: 0.000007  [  200/ 3788]\n",
      "loss: 0.000011  [  300/ 3788]\n",
      "loss: 0.000013  [  400/ 3788]\n",
      "loss: 0.000005  [  500/ 3788]\n",
      "loss: 0.000037  [  600/ 3788]\n",
      "loss: 0.000009  [  700/ 3788]\n",
      "loss: 0.000022  [  800/ 3788]\n",
      "loss: 0.000022  [  900/ 3788]\n",
      "loss: 0.000072  [ 1000/ 3788]\n",
      "loss: 0.000034  [ 1100/ 3788]\n",
      "loss: 0.000146  [ 1200/ 3788]\n",
      "loss: 0.000046  [ 1300/ 3788]\n",
      "loss: 0.000009  [ 1400/ 3788]\n",
      "loss: 0.000069  [ 1500/ 3788]\n",
      "loss: 0.000010  [ 1600/ 3788]\n",
      "loss: 0.000008  [ 1700/ 3788]\n",
      "loss: 0.000003  [ 1800/ 3788]\n",
      "loss: 0.000904  [ 1900/ 3788]\n",
      "loss: 0.000019  [ 2000/ 3788]\n",
      "loss: 0.000005  [ 2100/ 3788]\n",
      "loss: 0.000028  [ 2200/ 3788]\n",
      "loss: 0.000056  [ 2300/ 3788]\n",
      "loss: 0.000046  [ 2400/ 3788]\n",
      "loss: 0.000014  [ 2500/ 3788]\n",
      "loss: 0.000001  [ 2600/ 3788]\n",
      "loss: 0.000003  [ 2700/ 3788]\n",
      "loss: 0.000009  [ 2800/ 3788]\n",
      "loss: 0.000019  [ 2900/ 3788]\n",
      "loss: 0.000042  [ 3000/ 3788]\n",
      "loss: 0.000145  [ 3100/ 3788]\n",
      "loss: 0.000011  [ 3200/ 3788]\n",
      "loss: 0.000018  [ 3300/ 3788]\n",
      "loss: 0.000025  [ 3400/ 3788]\n",
      "loss: 0.000006  [ 3500/ 3788]\n",
      "loss: 0.000049  [ 3600/ 3788]\n",
      "loss: 0.000018  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 45.7%, Avg loss: 1.460507 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.205479  [    0/ 3788]\n",
      "loss: 0.000077  [  100/ 3788]\n",
      "loss: 0.000437  [  200/ 3788]\n",
      "loss: 0.000012  [  300/ 3788]\n",
      "loss: 0.000033  [  400/ 3788]\n",
      "loss: 0.000020  [  500/ 3788]\n",
      "loss: 0.000081  [  600/ 3788]\n",
      "loss: 0.000003  [  700/ 3788]\n",
      "loss: 0.000024  [  800/ 3788]\n",
      "loss: 0.000012  [  900/ 3788]\n",
      "loss: 0.000007  [ 1000/ 3788]\n",
      "loss: 0.000001  [ 1100/ 3788]\n",
      "loss: 0.000313  [ 1200/ 3788]\n",
      "loss: 0.000012  [ 1300/ 3788]\n",
      "loss: 0.000018  [ 1400/ 3788]\n",
      "loss: 0.000005  [ 1500/ 3788]\n",
      "loss: 0.000068  [ 1600/ 3788]\n",
      "loss: 0.000015  [ 1700/ 3788]\n",
      "loss: 0.000005  [ 1800/ 3788]\n",
      "loss: 0.000002  [ 1900/ 3788]\n",
      "loss: 0.000143  [ 2000/ 3788]\n",
      "loss: 0.000093  [ 2100/ 3788]\n",
      "loss: 0.000013  [ 2200/ 3788]\n",
      "loss: 0.000011  [ 2300/ 3788]\n",
      "loss: 0.000252  [ 2400/ 3788]\n",
      "loss: 0.000104  [ 2500/ 3788]\n",
      "loss: 0.000003  [ 2600/ 3788]\n",
      "loss: 0.000049  [ 2700/ 3788]\n",
      "loss: 0.000009  [ 2800/ 3788]\n",
      "loss: 0.000023  [ 2900/ 3788]\n",
      "loss: 0.000050  [ 3000/ 3788]\n",
      "loss: 0.000296  [ 3100/ 3788]\n",
      "loss: 0.000026  [ 3200/ 3788]\n",
      "loss: 0.000011  [ 3300/ 3788]\n",
      "loss: 0.000013  [ 3400/ 3788]\n",
      "loss: 0.000021  [ 3500/ 3788]\n",
      "loss: 0.000017  [ 3600/ 3788]\n",
      "loss: 0.000010  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 66.0%, Avg loss: 1.242302 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.005895  [    0/ 3788]\n",
      "loss: 0.000011  [  100/ 3788]\n",
      "loss: 0.000004  [  200/ 3788]\n",
      "loss: 0.000014  [  300/ 3788]\n",
      "loss: 0.000070  [  400/ 3788]\n",
      "loss: 0.000039  [  500/ 3788]\n",
      "loss: 0.000017  [  600/ 3788]\n",
      "loss: 0.000006  [  700/ 3788]\n",
      "loss: 0.000009  [  800/ 3788]\n",
      "loss: 0.000002  [  900/ 3788]\n",
      "loss: 0.000014  [ 1000/ 3788]\n",
      "loss: 0.000011  [ 1100/ 3788]\n",
      "loss: 0.000020  [ 1200/ 3788]\n",
      "loss: 0.000008  [ 1300/ 3788]\n",
      "loss: 0.000012  [ 1400/ 3788]\n",
      "loss: 0.000349  [ 1500/ 3788]\n",
      "loss: 0.000003  [ 1600/ 3788]\n",
      "loss: 0.000023  [ 1700/ 3788]\n",
      "loss: 0.000002  [ 1800/ 3788]\n",
      "loss: 0.000011  [ 1900/ 3788]\n",
      "loss: 0.000009  [ 2000/ 3788]\n",
      "loss: 0.000023  [ 2100/ 3788]\n",
      "loss: 0.000027  [ 2200/ 3788]\n",
      "loss: 0.000021  [ 2300/ 3788]\n",
      "loss: 0.000005  [ 2400/ 3788]\n",
      "loss: 0.000117  [ 2500/ 3788]\n",
      "loss: 0.000016  [ 2600/ 3788]\n",
      "loss: 0.000013  [ 2700/ 3788]\n",
      "loss: 0.000001  [ 2800/ 3788]\n",
      "loss: 0.000005  [ 2900/ 3788]\n",
      "loss: 0.000014  [ 3000/ 3788]\n",
      "loss: 0.000020  [ 3100/ 3788]\n",
      "loss: 0.000004  [ 3200/ 3788]\n",
      "loss: 0.000003  [ 3300/ 3788]\n",
      "loss: 0.002198  [ 3400/ 3788]\n",
      "loss: 0.000011  [ 3500/ 3788]\n",
      "loss: 0.000003  [ 3600/ 3788]\n",
      "loss: 0.000016  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 66.4%, Avg loss: 1.206561 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.000172  [    0/ 3788]\n",
      "loss: 0.000008  [  100/ 3788]\n",
      "loss: 0.000031  [  200/ 3788]\n",
      "loss: 0.000026  [  300/ 3788]\n",
      "loss: 0.000018  [  400/ 3788]\n",
      "loss: 0.000016  [  500/ 3788]\n",
      "loss: 0.000007  [  600/ 3788]\n",
      "loss: 0.000467  [  700/ 3788]\n",
      "loss: 0.000006  [  800/ 3788]\n",
      "loss: 0.000016  [  900/ 3788]\n",
      "loss: 0.002823  [ 1000/ 3788]\n",
      "loss: 0.000010  [ 1100/ 3788]\n",
      "loss: 0.000006  [ 1200/ 3788]\n",
      "loss: 0.000012  [ 1300/ 3788]\n",
      "loss: 0.000004  [ 1400/ 3788]\n",
      "loss: 0.000198  [ 1500/ 3788]\n",
      "loss: 0.000008  [ 1600/ 3788]\n",
      "loss: 0.000006  [ 1700/ 3788]\n",
      "loss: 0.000006  [ 1800/ 3788]\n",
      "loss: 0.000001  [ 1900/ 3788]\n",
      "loss: 0.000005  [ 2000/ 3788]\n",
      "loss: 0.000043  [ 2100/ 3788]\n",
      "loss: 0.000021  [ 2200/ 3788]\n",
      "loss: 0.000088  [ 2300/ 3788]\n",
      "loss: 0.000009  [ 2400/ 3788]\n",
      "loss: 0.000015  [ 2500/ 3788]\n",
      "loss: 0.000008  [ 2600/ 3788]\n",
      "loss: 0.000005  [ 2700/ 3788]\n",
      "loss: 0.000002  [ 2800/ 3788]\n",
      "loss: 0.000004  [ 2900/ 3788]\n",
      "loss: 0.000055  [ 3000/ 3788]\n",
      "loss: 0.000006  [ 3100/ 3788]\n",
      "loss: 0.000015  [ 3200/ 3788]\n",
      "loss: 0.000011  [ 3300/ 3788]\n",
      "loss: 0.000003  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000021  [ 3600/ 3788]\n",
      "loss: 0.000001  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 50.9%, Avg loss: 1.444232 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.000014  [    0/ 3788]\n",
      "loss: 0.005591  [  100/ 3788]\n",
      "loss: 0.000001  [  200/ 3788]\n",
      "loss: 0.000007  [  300/ 3788]\n",
      "loss: 0.000002  [  400/ 3788]\n",
      "loss: 0.000009  [  500/ 3788]\n",
      "loss: 0.000018  [  600/ 3788]\n",
      "loss: 0.000002  [  700/ 3788]\n",
      "loss: 0.000015  [  800/ 3788]\n",
      "loss: 0.000005  [  900/ 3788]\n",
      "loss: 0.000006  [ 1000/ 3788]\n",
      "loss: 0.000004  [ 1100/ 3788]\n",
      "loss: 0.000032  [ 1200/ 3788]\n",
      "loss: 0.000002  [ 1300/ 3788]\n",
      "loss: 0.000007  [ 1400/ 3788]\n",
      "loss: 0.000013  [ 1500/ 3788]\n",
      "loss: 0.000011  [ 1600/ 3788]\n",
      "loss: 0.000002  [ 1700/ 3788]\n",
      "loss: 0.000003  [ 1800/ 3788]\n",
      "loss: 0.000005  [ 1900/ 3788]\n",
      "loss: 0.000031  [ 2000/ 3788]\n",
      "loss: 0.000042  [ 2100/ 3788]\n",
      "loss: 0.000059  [ 2200/ 3788]\n",
      "loss: 0.000010  [ 2300/ 3788]\n",
      "loss: 0.000004  [ 2400/ 3788]\n",
      "loss: 0.000009  [ 2500/ 3788]\n",
      "loss: 0.000002  [ 2600/ 3788]\n",
      "loss: 0.000001  [ 2700/ 3788]\n",
      "loss: 0.000003  [ 2800/ 3788]\n",
      "loss: 0.000017  [ 2900/ 3788]\n",
      "loss: 0.000017  [ 3000/ 3788]\n",
      "loss: 0.000011  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000056  [ 3300/ 3788]\n",
      "loss: 0.000006  [ 3400/ 3788]\n",
      "loss: 0.000008  [ 3500/ 3788]\n",
      "loss: 0.000015  [ 3600/ 3788]\n",
      "loss: 0.000004  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 75.9%, Avg loss: 1.335583 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 3788]\n",
      "loss: 0.000017  [  100/ 3788]\n",
      "loss: 0.000016  [  200/ 3788]\n",
      "loss: 0.000005  [  300/ 3788]\n",
      "loss: 0.000016  [  400/ 3788]\n",
      "loss: 0.000004  [  500/ 3788]\n",
      "loss: 0.000007  [  600/ 3788]\n",
      "loss: 0.000002  [  700/ 3788]\n",
      "loss: 0.000020  [  800/ 3788]\n",
      "loss: 0.753855  [  900/ 3788]\n",
      "loss: 0.000002  [ 1000/ 3788]\n",
      "loss: 0.000004  [ 1100/ 3788]\n",
      "loss: 0.000003  [ 1200/ 3788]\n",
      "loss: 0.000003  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000001  [ 1500/ 3788]\n",
      "loss: 0.000001  [ 1600/ 3788]\n",
      "loss: 0.000159  [ 1700/ 3788]\n",
      "loss: 0.000016  [ 1800/ 3788]\n",
      "loss: 0.000004  [ 1900/ 3788]\n",
      "loss: 0.000001  [ 2000/ 3788]\n",
      "loss: 0.000014  [ 2100/ 3788]\n",
      "loss: 0.000005  [ 2200/ 3788]\n",
      "loss: 0.000004  [ 2300/ 3788]\n",
      "loss: 0.000067  [ 2400/ 3788]\n",
      "loss: 0.000015  [ 2500/ 3788]\n",
      "loss: 0.000076  [ 2600/ 3788]\n",
      "loss: 0.000041  [ 2700/ 3788]\n",
      "loss: 0.000001  [ 2800/ 3788]\n",
      "loss: 0.000013  [ 2900/ 3788]\n",
      "loss: 0.000009  [ 3000/ 3788]\n",
      "loss: 0.000057  [ 3100/ 3788]\n",
      "loss: 0.000009  [ 3200/ 3788]\n",
      "loss: 0.000014  [ 3300/ 3788]\n",
      "loss: 0.000009  [ 3400/ 3788]\n",
      "loss: 0.000056  [ 3500/ 3788]\n",
      "loss: 0.000004  [ 3600/ 3788]\n",
      "loss: 0.000009  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 62.0%, Avg loss: 1.403898 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 3788]\n",
      "loss: 0.000011  [  100/ 3788]\n",
      "loss: 0.000001  [  200/ 3788]\n",
      "loss: 0.000002  [  300/ 3788]\n",
      "loss: 0.000004  [  400/ 3788]\n",
      "loss: 0.000008  [  500/ 3788]\n",
      "loss: 0.000005  [  600/ 3788]\n",
      "loss: 0.000008  [  700/ 3788]\n",
      "loss: 0.000019  [  800/ 3788]\n",
      "loss: 0.000005  [  900/ 3788]\n",
      "loss: 0.000021  [ 1000/ 3788]\n",
      "loss: 0.000003  [ 1100/ 3788]\n",
      "loss: 0.000009  [ 1200/ 3788]\n",
      "loss: 0.000010  [ 1300/ 3788]\n",
      "loss: 0.000009  [ 1400/ 3788]\n",
      "loss: 0.000001  [ 1500/ 3788]\n",
      "loss: 0.000081  [ 1600/ 3788]\n",
      "loss: 0.000002  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000007  [ 1900/ 3788]\n",
      "loss: 0.000001  [ 2000/ 3788]\n",
      "loss: 0.000020  [ 2100/ 3788]\n",
      "loss: 0.000039  [ 2200/ 3788]\n",
      "loss: 0.000007  [ 2300/ 3788]\n",
      "loss: 0.000093  [ 2400/ 3788]\n",
      "loss: 0.000011  [ 2500/ 3788]\n",
      "loss: 0.000011  [ 2600/ 3788]\n",
      "loss: 0.000039  [ 2700/ 3788]\n",
      "loss: 0.000066  [ 2800/ 3788]\n",
      "loss: 0.000003  [ 2900/ 3788]\n",
      "loss: 0.000015  [ 3000/ 3788]\n",
      "loss: 0.000006  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000002  [ 3300/ 3788]\n",
      "loss: 0.000127  [ 3400/ 3788]\n",
      "loss: 0.000002  [ 3500/ 3788]\n",
      "loss: 0.000049  [ 3600/ 3788]\n",
      "loss: 0.000013  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 57.3%, Avg loss: 1.356220 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 3788]\n",
      "loss: 0.000055  [  100/ 3788]\n",
      "loss: 0.000010  [  200/ 3788]\n",
      "loss: 0.000034  [  300/ 3788]\n",
      "loss: 0.000003  [  400/ 3788]\n",
      "loss: 0.000011  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000004  [  700/ 3788]\n",
      "loss: 0.000050  [  800/ 3788]\n",
      "loss: 0.000001  [  900/ 3788]\n",
      "loss: 0.000008  [ 1000/ 3788]\n",
      "loss: 0.000032  [ 1100/ 3788]\n",
      "loss: 0.000003  [ 1200/ 3788]\n",
      "loss: 0.000004  [ 1300/ 3788]\n",
      "loss: 0.000006  [ 1400/ 3788]\n",
      "loss: 0.000007  [ 1500/ 3788]\n",
      "loss: 0.000009  [ 1600/ 3788]\n",
      "loss: 0.000004  [ 1700/ 3788]\n",
      "loss: 0.000007  [ 1800/ 3788]\n",
      "loss: 0.000007  [ 1900/ 3788]\n",
      "loss: 0.000204  [ 2000/ 3788]\n",
      "loss: 0.000001  [ 2100/ 3788]\n",
      "loss: 0.000001  [ 2200/ 3788]\n",
      "loss: 0.000005  [ 2300/ 3788]\n",
      "loss: 0.001119  [ 2400/ 3788]\n",
      "loss: 0.000001  [ 2500/ 3788]\n",
      "loss: 0.000006  [ 2600/ 3788]\n",
      "loss: 0.000002  [ 2700/ 3788]\n",
      "loss: 0.000014  [ 2800/ 3788]\n",
      "loss: 0.000020  [ 2900/ 3788]\n",
      "loss: 0.000001  [ 3000/ 3788]\n",
      "loss: 0.000005  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000022  [ 3300/ 3788]\n",
      "loss: 0.000005  [ 3400/ 3788]\n",
      "loss: 0.000022  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000004  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 38.1%, Avg loss: 1.605933 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.000190  [    0/ 3788]\n",
      "loss: 0.000005  [  100/ 3788]\n",
      "loss: 0.000004  [  200/ 3788]\n",
      "loss: 0.000025  [  300/ 3788]\n",
      "loss: 0.000004  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000010  [  600/ 3788]\n",
      "loss: 0.000006  [  700/ 3788]\n",
      "loss: 0.000003  [  800/ 3788]\n",
      "loss: 0.000004  [  900/ 3788]\n",
      "loss: 0.000304  [ 1000/ 3788]\n",
      "loss: 0.000078  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000001  [ 1300/ 3788]\n",
      "loss: 0.000026  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000007  [ 1700/ 3788]\n",
      "loss: 0.000036  [ 1800/ 3788]\n",
      "loss: 0.000010  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000022  [ 2100/ 3788]\n",
      "loss: 0.000004  [ 2200/ 3788]\n",
      "loss: 0.000017  [ 2300/ 3788]\n",
      "loss: 0.000013  [ 2400/ 3788]\n",
      "loss: 0.000017  [ 2500/ 3788]\n",
      "loss: 0.000031  [ 2600/ 3788]\n",
      "loss: 0.000003  [ 2700/ 3788]\n",
      "loss: 0.000114  [ 2800/ 3788]\n",
      "loss: 0.000029  [ 2900/ 3788]\n",
      "loss: 0.000012  [ 3000/ 3788]\n",
      "loss: 0.000006  [ 3100/ 3788]\n",
      "loss: 0.000004  [ 3200/ 3788]\n",
      "loss: 0.000008  [ 3300/ 3788]\n",
      "loss: 0.000037  [ 3400/ 3788]\n",
      "loss: 0.000156  [ 3500/ 3788]\n",
      "loss: 0.000047  [ 3600/ 3788]\n",
      "loss: 0.000003  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 58.1%, Avg loss: 1.626849 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 3788]\n",
      "loss: 0.000004  [  100/ 3788]\n",
      "loss: 0.000005  [  200/ 3788]\n",
      "loss: 0.000005  [  300/ 3788]\n",
      "loss: 0.000024  [  400/ 3788]\n",
      "loss: 0.000006  [  500/ 3788]\n",
      "loss: 0.000001  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000004  [  800/ 3788]\n",
      "loss: 0.000002  [  900/ 3788]\n",
      "loss: 0.000002  [ 1000/ 3788]\n",
      "loss: 0.000001  [ 1100/ 3788]\n",
      "loss: 0.000001  [ 1200/ 3788]\n",
      "loss: 0.000003  [ 1300/ 3788]\n",
      "loss: 0.000037  [ 1400/ 3788]\n",
      "loss: 0.000003  [ 1500/ 3788]\n",
      "loss: 0.000004  [ 1600/ 3788]\n",
      "loss: 0.000017  [ 1700/ 3788]\n",
      "loss: 0.000041  [ 1800/ 3788]\n",
      "loss: 0.000001  [ 1900/ 3788]\n",
      "loss: 0.000072  [ 2000/ 3788]\n",
      "loss: 0.000003  [ 2100/ 3788]\n",
      "loss: 0.000002  [ 2200/ 3788]\n",
      "loss: 0.000035  [ 2300/ 3788]\n",
      "loss: 0.000017  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000409  [ 2600/ 3788]\n",
      "loss: 0.000012  [ 2700/ 3788]\n",
      "loss: 0.000008  [ 2800/ 3788]\n",
      "loss: 0.000013  [ 2900/ 3788]\n",
      "loss: 0.000272  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000013  [ 3200/ 3788]\n",
      "loss: 0.000003  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000009  [ 3600/ 3788]\n",
      "loss: 0.000001  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 60.1%, Avg loss: 1.625508 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 3788]\n",
      "loss: 0.000008  [  100/ 3788]\n",
      "loss: 0.000023  [  200/ 3788]\n",
      "loss: 0.000015  [  300/ 3788]\n",
      "loss: 0.000009  [  400/ 3788]\n",
      "loss: 0.000015  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000003  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000013  [  900/ 3788]\n",
      "loss: 0.000003  [ 1000/ 3788]\n",
      "loss: 0.000001  [ 1100/ 3788]\n",
      "loss: 0.000003  [ 1200/ 3788]\n",
      "loss: 0.000002  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000002  [ 1500/ 3788]\n",
      "loss: 0.000011  [ 1600/ 3788]\n",
      "loss: 0.000007  [ 1700/ 3788]\n",
      "loss: 0.000003  [ 1800/ 3788]\n",
      "loss: 0.000001  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000002  [ 2100/ 3788]\n",
      "loss: 0.000004  [ 2200/ 3788]\n",
      "loss: 0.000004  [ 2300/ 3788]\n",
      "loss: 0.000009  [ 2400/ 3788]\n",
      "loss: 0.000003  [ 2500/ 3788]\n",
      "loss: 0.000033  [ 2600/ 3788]\n",
      "loss: 0.000013  [ 2700/ 3788]\n",
      "loss: 0.000009  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000002  [ 3000/ 3788]\n",
      "loss: 0.000003  [ 3100/ 3788]\n",
      "loss: 0.000020  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000002  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000003  [ 3600/ 3788]\n",
      "loss: 0.000003  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 61.9%, Avg loss: 1.284259 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 3788]\n",
      "loss: 0.000004  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000001  [  300/ 3788]\n",
      "loss: 0.000003  [  400/ 3788]\n",
      "loss: 0.000011  [  500/ 3788]\n",
      "loss: 0.000002  [  600/ 3788]\n",
      "loss: 0.000003  [  700/ 3788]\n",
      "loss: 0.000020  [  800/ 3788]\n",
      "loss: 0.000006  [  900/ 3788]\n",
      "loss: 0.000003  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000012  [ 1200/ 3788]\n",
      "loss: 0.000010  [ 1300/ 3788]\n",
      "loss: 0.000024  [ 1400/ 3788]\n",
      "loss: 0.000003  [ 1500/ 3788]\n",
      "loss: 0.000002  [ 1600/ 3788]\n",
      "loss: 0.000005  [ 1700/ 3788]\n",
      "loss: 0.000002  [ 1800/ 3788]\n",
      "loss: 0.000003  [ 1900/ 3788]\n",
      "loss: 0.000020  [ 2000/ 3788]\n",
      "loss: 0.000002  [ 2100/ 3788]\n",
      "loss: 0.000002  [ 2200/ 3788]\n",
      "loss: 0.000005  [ 2300/ 3788]\n",
      "loss: 0.000002  [ 2400/ 3788]\n",
      "loss: 0.000021  [ 2500/ 3788]\n",
      "loss: 0.000003  [ 2600/ 3788]\n",
      "loss: 0.000006  [ 2700/ 3788]\n",
      "loss: 0.000011  [ 2800/ 3788]\n",
      "loss: 0.000001  [ 2900/ 3788]\n",
      "loss: 0.000010  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000005  [ 3200/ 3788]\n",
      "loss: 0.000003  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000006  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 55.9%, Avg loss: 1.815486 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000014  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000007  [  400/ 3788]\n",
      "loss: 0.000005  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000002  [  800/ 3788]\n",
      "loss: 0.000036  [  900/ 3788]\n",
      "loss: 0.000102  [ 1000/ 3788]\n",
      "loss: 0.000056  [ 1100/ 3788]\n",
      "loss: 0.000001  [ 1200/ 3788]\n",
      "loss: 0.000005  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000003  [ 1500/ 3788]\n",
      "loss: 0.000007  [ 1600/ 3788]\n",
      "loss: 0.000004  [ 1700/ 3788]\n",
      "loss: 0.000008  [ 1800/ 3788]\n",
      "loss: 0.000003  [ 1900/ 3788]\n",
      "loss: 0.000005  [ 2000/ 3788]\n",
      "loss: 0.000004  [ 2100/ 3788]\n",
      "loss: 0.000006  [ 2200/ 3788]\n",
      "loss: 0.000006  [ 2300/ 3788]\n",
      "loss: 0.000004  [ 2400/ 3788]\n",
      "loss: 0.000017  [ 2500/ 3788]\n",
      "loss: 0.000009  [ 2600/ 3788]\n",
      "loss: 0.000006  [ 2700/ 3788]\n",
      "loss: 0.000009  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000034  [ 3000/ 3788]\n",
      "loss: 0.000005  [ 3100/ 3788]\n",
      "loss: 0.000004  [ 3200/ 3788]\n",
      "loss: 0.000006  [ 3300/ 3788]\n",
      "loss: 0.000007  [ 3400/ 3788]\n",
      "loss: 0.000002  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000006  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 56.5%, Avg loss: 1.326159 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.000001  [    0/ 3788]\n",
      "loss: 0.000004  [  100/ 3788]\n",
      "loss: 0.000028  [  200/ 3788]\n",
      "loss: 0.000005  [  300/ 3788]\n",
      "loss: 0.000003  [  400/ 3788]\n",
      "loss: 0.000002  [  500/ 3788]\n",
      "loss: 0.000001  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000005  [  800/ 3788]\n",
      "loss: 0.000002  [  900/ 3788]\n",
      "loss: 0.000008  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000019  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000005  [ 1400/ 3788]\n",
      "loss: 0.000002  [ 1500/ 3788]\n",
      "loss: 0.000004  [ 1600/ 3788]\n",
      "loss: 0.000001  [ 1700/ 3788]\n",
      "loss: 0.000002  [ 1800/ 3788]\n",
      "loss: 0.000001  [ 1900/ 3788]\n",
      "loss: 0.000068  [ 2000/ 3788]\n",
      "loss: 0.000004  [ 2100/ 3788]\n",
      "loss: 0.000002  [ 2200/ 3788]\n",
      "loss: 0.000004  [ 2300/ 3788]\n",
      "loss: 0.000005  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000004  [ 2700/ 3788]\n",
      "loss: 0.000047  [ 2800/ 3788]\n",
      "loss: 0.000026  [ 2900/ 3788]\n",
      "loss: 0.000019  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000004  [ 3200/ 3788]\n",
      "loss: 0.000003  [ 3300/ 3788]\n",
      "loss: 0.000004  [ 3400/ 3788]\n",
      "loss: 0.000006  [ 3500/ 3788]\n",
      "loss: 0.000020  [ 3600/ 3788]\n",
      "loss: 0.000004  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 49.2%, Avg loss: 1.696929 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 3788]\n",
      "loss: 0.000002  [  100/ 3788]\n",
      "loss: 0.000002  [  200/ 3788]\n",
      "loss: 0.000003  [  300/ 3788]\n",
      "loss: 0.000003  [  400/ 3788]\n",
      "loss: 0.000004  [  500/ 3788]\n",
      "loss: 0.000003  [  600/ 3788]\n",
      "loss: 0.000003  [  700/ 3788]\n",
      "loss: 0.000005  [  800/ 3788]\n",
      "loss: 0.000003  [  900/ 3788]\n",
      "loss: 0.000004  [ 1000/ 3788]\n",
      "loss: 0.000003  [ 1100/ 3788]\n",
      "loss: 0.000004  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000004  [ 1400/ 3788]\n",
      "loss: 0.000002  [ 1500/ 3788]\n",
      "loss: 0.000003  [ 1600/ 3788]\n",
      "loss: 0.000003  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000068  [ 2000/ 3788]\n",
      "loss: 0.000003  [ 2100/ 3788]\n",
      "loss: 0.000002  [ 2200/ 3788]\n",
      "loss: 0.000005  [ 2300/ 3788]\n",
      "loss: 0.000020  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000040  [ 2600/ 3788]\n",
      "loss: 0.000006  [ 2700/ 3788]\n",
      "loss: 0.000005  [ 2800/ 3788]\n",
      "loss: 0.000003  [ 2900/ 3788]\n",
      "loss: 0.000003  [ 3000/ 3788]\n",
      "loss: 0.000006  [ 3100/ 3788]\n",
      "loss: 0.000006  [ 3200/ 3788]\n",
      "loss: 0.000009  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000003  [ 3500/ 3788]\n",
      "loss: 0.000002  [ 3600/ 3788]\n",
      "loss: 0.000002  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 55.5%, Avg loss: 1.505174 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 3788]\n",
      "loss: 0.000001  [  100/ 3788]\n",
      "loss: 0.033226  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000001  [  400/ 3788]\n",
      "loss: 0.000012  [  500/ 3788]\n",
      "loss: 0.000001  [  600/ 3788]\n",
      "loss: 0.000008  [  700/ 3788]\n",
      "loss: 0.000005  [  800/ 3788]\n",
      "loss: 0.000003  [  900/ 3788]\n",
      "loss: 0.000001  [ 1000/ 3788]\n",
      "loss: 0.000002  [ 1100/ 3788]\n",
      "loss: 0.000034  [ 1200/ 3788]\n",
      "loss: 0.000018  [ 1300/ 3788]\n",
      "loss: 0.000003  [ 1400/ 3788]\n",
      "loss: 0.000005  [ 1500/ 3788]\n",
      "loss: 0.000053  [ 1600/ 3788]\n",
      "loss: 0.000002  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000006  [ 1900/ 3788]\n",
      "loss: 0.000624  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000002  [ 2500/ 3788]\n",
      "loss: 0.000001  [ 2600/ 3788]\n",
      "loss: 0.000004  [ 2700/ 3788]\n",
      "loss: 0.000002  [ 2800/ 3788]\n",
      "loss: 0.000002  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000003  [ 3200/ 3788]\n",
      "loss: 0.000003  [ 3300/ 3788]\n",
      "loss: 0.000007  [ 3400/ 3788]\n",
      "loss: 0.000004  [ 3500/ 3788]\n",
      "loss: 0.000001  [ 3600/ 3788]\n",
      "loss: 0.000002  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 56.4%, Avg loss: 1.416369 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000011  [  100/ 3788]\n",
      "loss: 0.000005  [  200/ 3788]\n",
      "loss: 0.000001  [  300/ 3788]\n",
      "loss: 0.000001  [  400/ 3788]\n",
      "loss: 0.000003  [  500/ 3788]\n",
      "loss: 0.000004  [  600/ 3788]\n",
      "loss: 0.000002  [  700/ 3788]\n",
      "loss: 0.000031  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000005  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000004  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000002  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000004  [ 1800/ 3788]\n",
      "loss: 0.000003  [ 1900/ 3788]\n",
      "loss: 0.000001  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000001  [ 2200/ 3788]\n",
      "loss: 0.000001  [ 2300/ 3788]\n",
      "loss: 0.000001  [ 2400/ 3788]\n",
      "loss: 0.000003  [ 2500/ 3788]\n",
      "loss: 0.000001  [ 2600/ 3788]\n",
      "loss: 0.000044  [ 2700/ 3788]\n",
      "loss: 0.000004  [ 2800/ 3788]\n",
      "loss: 0.000049  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000003  [ 3100/ 3788]\n",
      "loss: 0.000005  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000008  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000001  [ 3600/ 3788]\n",
      "loss: 0.000003  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 58.9%, Avg loss: 1.563402 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.000027  [    0/ 3788]\n",
      "loss: 0.000001  [  100/ 3788]\n",
      "loss: 0.000003  [  200/ 3788]\n",
      "loss: 0.000002  [  300/ 3788]\n",
      "loss: 0.000002  [  400/ 3788]\n",
      "loss: 0.000003  [  500/ 3788]\n",
      "loss: 0.000002  [  600/ 3788]\n",
      "loss: 0.000001  [  700/ 3788]\n",
      "loss: 0.000011  [  800/ 3788]\n",
      "loss: 0.000003  [  900/ 3788]\n",
      "loss: 0.000004  [ 1000/ 3788]\n",
      "loss: 0.000001  [ 1100/ 3788]\n",
      "loss: 0.000002  [ 1200/ 3788]\n",
      "loss: 0.000003  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000019  [ 1500/ 3788]\n",
      "loss: 0.000026  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000010  [ 1800/ 3788]\n",
      "loss: 0.000004  [ 1900/ 3788]\n",
      "loss: 0.000003  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000003  [ 2200/ 3788]\n",
      "loss: 0.000003  [ 2300/ 3788]\n",
      "loss: 0.000003  [ 2400/ 3788]\n",
      "loss: 0.000006  [ 2500/ 3788]\n",
      "loss: 0.000003  [ 2600/ 3788]\n",
      "loss: 0.000011  [ 2700/ 3788]\n",
      "loss: 0.000001  [ 2800/ 3788]\n",
      "loss: 0.000003  [ 2900/ 3788]\n",
      "loss: 0.000016  [ 3000/ 3788]\n",
      "loss: 0.000004  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000015  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000056  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 62.2%, Avg loss: 1.287346 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000001  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000002  [  400/ 3788]\n",
      "loss: 0.000002  [  500/ 3788]\n",
      "loss: 0.000003  [  600/ 3788]\n",
      "loss: 0.000004  [  700/ 3788]\n",
      "loss: 0.000015  [  800/ 3788]\n",
      "loss: 0.000003  [  900/ 3788]\n",
      "loss: 0.000004  [ 1000/ 3788]\n",
      "loss: 0.000021  [ 1100/ 3788]\n",
      "loss: 0.000003  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000472  [ 1400/ 3788]\n",
      "loss: 0.000191  [ 1500/ 3788]\n",
      "loss: 0.000007  [ 1600/ 3788]\n",
      "loss: 0.000006  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000003  [ 1900/ 3788]\n",
      "loss: 0.000003  [ 2000/ 3788]\n",
      "loss: 0.000002  [ 2100/ 3788]\n",
      "loss: 0.000002  [ 2200/ 3788]\n",
      "loss: 0.000007  [ 2300/ 3788]\n",
      "loss: 0.000001  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000012  [ 2800/ 3788]\n",
      "loss: 0.000004  [ 2900/ 3788]\n",
      "loss: 0.000002  [ 3000/ 3788]\n",
      "loss: 0.000003  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000003  [ 3300/ 3788]\n",
      "loss: 0.000003  [ 3400/ 3788]\n",
      "loss: 0.000010  [ 3500/ 3788]\n",
      "loss: 0.000002  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.675114 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000019  [  200/ 3788]\n",
      "loss: 0.000004  [  300/ 3788]\n",
      "loss: 0.000001  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000015  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000005  [ 1100/ 3788]\n",
      "loss: 0.000011  [ 1200/ 3788]\n",
      "loss: 0.000003  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000001  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000001  [ 1800/ 3788]\n",
      "loss: 0.000021  [ 1900/ 3788]\n",
      "loss: 0.000075  [ 2000/ 3788]\n",
      "loss: 0.000043  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000011  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000157  [ 2700/ 3788]\n",
      "loss: 0.000003  [ 2800/ 3788]\n",
      "loss: 0.000003  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000007  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000028  [ 3300/ 3788]\n",
      "loss: 0.000001  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000016  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 45.1%, Avg loss: 1.575619 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.000033  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000012  [  200/ 3788]\n",
      "loss: 0.000001  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000004  [  600/ 3788]\n",
      "loss: 0.000005  [  700/ 3788]\n",
      "loss: 0.000007  [  800/ 3788]\n",
      "loss: 0.000011  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000001  [ 1200/ 3788]\n",
      "loss: 0.000003  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000002  [ 1500/ 3788]\n",
      "loss: 0.000006  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000397  [ 1800/ 3788]\n",
      "loss: 0.000003  [ 1900/ 3788]\n",
      "loss: 0.000004  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000013  [ 2200/ 3788]\n",
      "loss: 0.000003  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000004  [ 2500/ 3788]\n",
      "loss: 0.000016  [ 2600/ 3788]\n",
      "loss: 0.000011  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000003  [ 2900/ 3788]\n",
      "loss: 0.000001  [ 3000/ 3788]\n",
      "loss: 0.000010  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000021  [ 3300/ 3788]\n",
      "loss: 0.000003  [ 3400/ 3788]\n",
      "loss: 0.000004  [ 3500/ 3788]\n",
      "loss: 0.000004  [ 3600/ 3788]\n",
      "loss: 0.000009  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 65.2%, Avg loss: 1.449563 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000047  [  300/ 3788]\n",
      "loss: 0.000001  [  400/ 3788]\n",
      "loss: 0.000001  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000005  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000008  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000009  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000007  [ 1300/ 3788]\n",
      "loss: 0.000006  [ 1400/ 3788]\n",
      "loss: 0.000009  [ 1500/ 3788]\n",
      "loss: 0.000001  [ 1600/ 3788]\n",
      "loss: 0.000003  [ 1700/ 3788]\n",
      "loss: 0.000010  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000007  [ 2300/ 3788]\n",
      "loss: 0.000003  [ 2400/ 3788]\n",
      "loss: 0.000006  [ 2500/ 3788]\n",
      "loss: 0.000009  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000017  [ 2800/ 3788]\n",
      "loss: 0.000003  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000006  [ 3100/ 3788]\n",
      "loss: 0.000005  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000001  [ 3400/ 3788]\n",
      "loss: 0.000018  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000004  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 66.0%, Avg loss: 1.265931 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000008  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000002  [  500/ 3788]\n",
      "loss: 0.000002  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000004  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000002  [ 1000/ 3788]\n",
      "loss: 0.000014  [ 1100/ 3788]\n",
      "loss: 0.000002  [ 1200/ 3788]\n",
      "loss: 0.000003  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000002  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000002  [ 1800/ 3788]\n",
      "loss: 0.000016  [ 1900/ 3788]\n",
      "loss: 0.000003  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000002  [ 2500/ 3788]\n",
      "loss: 0.000001  [ 2600/ 3788]\n",
      "loss: 0.000002  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000001  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000002  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000011  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000001  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 57.1%, Avg loss: 1.300093 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000004  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000102  [  400/ 3788]\n",
      "loss: 0.000001  [  500/ 3788]\n",
      "loss: 0.000001  [  600/ 3788]\n",
      "loss: 0.000002  [  700/ 3788]\n",
      "loss: 0.000011  [  800/ 3788]\n",
      "loss: 0.000003  [  900/ 3788]\n",
      "loss: 0.000001  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000007  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000001  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000001  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000001  [ 2800/ 3788]\n",
      "loss: 0.000001  [ 2900/ 3788]\n",
      "loss: 0.000001  [ 3000/ 3788]\n",
      "loss: 0.000002  [ 3100/ 3788]\n",
      "loss: 0.000007  [ 3200/ 3788]\n",
      "loss: 0.000001  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 70.2%, Avg loss: 1.703502 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.000001  [    0/ 3788]\n",
      "loss: 0.000464  [  100/ 3788]\n",
      "loss: 0.000022  [  200/ 3788]\n",
      "loss: 0.000006  [  300/ 3788]\n",
      "loss: 0.000003  [  400/ 3788]\n",
      "loss: 0.000001  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000001  [  700/ 3788]\n",
      "loss: 0.000001  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000003  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000001  [ 1200/ 3788]\n",
      "loss: 0.000002  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000001  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000002  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000001  [ 2300/ 3788]\n",
      "loss: 0.000001  [ 2400/ 3788]\n",
      "loss: 0.000001  [ 2500/ 3788]\n",
      "loss: 0.000008  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000002  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000051  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000001  [ 3600/ 3788]\n",
      "loss: 0.000001  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 42.7%, Avg loss: 1.658378 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000003  [  300/ 3788]\n",
      "loss: 0.000005  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000003  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000001  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000001  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000001  [ 1300/ 3788]\n",
      "loss: 0.000002  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000003  [ 1900/ 3788]\n",
      "loss: 0.000003  [ 2000/ 3788]\n",
      "loss: 0.000008  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000001  [ 2300/ 3788]\n",
      "loss: 0.000003  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000001  [ 2800/ 3788]\n",
      "loss: 0.000016  [ 2900/ 3788]\n",
      "loss: 0.000001  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000002  [ 3300/ 3788]\n",
      "loss: 0.000002  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000001  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 20.5%, Avg loss: 1.767595 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000038  [  400/ 3788]\n",
      "loss: 0.000001  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000002  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000004  [  900/ 3788]\n",
      "loss: 0.000005  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.500701  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000001  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000002  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000001  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000001  [ 2700/ 3788]\n",
      "loss: 0.000003  [ 2800/ 3788]\n",
      "loss: 0.000001  [ 2900/ 3788]\n",
      "loss: 0.000001  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000002  [ 3200/ 3788]\n",
      "loss: 0.000001  [ 3300/ 3788]\n",
      "loss: 0.000003  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000001  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 50.5%, Avg loss: 1.574676 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 3788]\n",
      "loss: 0.000001  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000001  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000094  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000001  [ 1000/ 3788]\n",
      "loss: 0.000001  [ 1100/ 3788]\n",
      "loss: 0.000001  [ 1200/ 3788]\n",
      "loss: 0.000001  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000003  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000003  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000001  [ 2100/ 3788]\n",
      "loss: 0.000001  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000001  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000001  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000004  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000005  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 28.9%, Avg loss: 1.656278 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000002  [  100/ 3788]\n",
      "loss: 0.000001  [  200/ 3788]\n",
      "loss: 0.000004  [  300/ 3788]\n",
      "loss: 0.000009  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000002  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000002  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000001  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000002  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000017  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000005  [ 3400/ 3788]\n",
      "loss: 0.000002  [ 3500/ 3788]\n",
      "loss: 0.000002  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 64.9%, Avg loss: 1.648428 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.000001  [    0/ 3788]\n",
      "loss: 0.000002  [  100/ 3788]\n",
      "loss: 0.000003  [  200/ 3788]\n",
      "loss: 0.000002  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000001  [  600/ 3788]\n",
      "loss: 0.000011  [  700/ 3788]\n",
      "loss: 0.000001  [  800/ 3788]\n",
      "loss: 0.000001  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000002  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000003  [ 1800/ 3788]\n",
      "loss: 0.000002  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000001  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000003  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000015  [ 2800/ 3788]\n",
      "loss: 0.000006  [ 2900/ 3788]\n",
      "loss: 0.000004  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000002  [ 3200/ 3788]\n",
      "loss: 0.000065  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000003  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000001  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 62.2%, Avg loss: 1.555284 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000004  [  100/ 3788]\n",
      "loss: 0.000003  [  200/ 3788]\n",
      "loss: 0.000002  [  300/ 3788]\n",
      "loss: 0.000004  [  400/ 3788]\n",
      "loss: 0.000001  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000001  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000005  [ 1000/ 3788]\n",
      "loss: 0.000002  [ 1100/ 3788]\n",
      "loss: 0.000005  [ 1200/ 3788]\n",
      "loss: 0.000003  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000001  [ 1500/ 3788]\n",
      "loss: 0.000003  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000001  [ 2000/ 3788]\n",
      "loss: 0.000006  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000002  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000002  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000001  [ 3100/ 3788]\n",
      "loss: 0.000013  [ 3200/ 3788]\n",
      "loss: 0.000004  [ 3300/ 3788]\n",
      "loss: 0.000002  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 75.0%, Avg loss: 1.595736 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000001  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000002  [  500/ 3788]\n",
      "loss: 0.000006  [  600/ 3788]\n",
      "loss: 0.000055  [  700/ 3788]\n",
      "loss: 0.000001  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000002  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000004  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000001  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000028  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000001  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000002  [ 2700/ 3788]\n",
      "loss: 0.000003  [ 2800/ 3788]\n",
      "loss: 0.000001  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000016  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000010  [ 3300/ 3788]\n",
      "loss: 0.000002  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 66.7%, Avg loss: 1.489005 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000003  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000002  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000078  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000006  [  900/ 3788]\n",
      "loss: 0.000003  [ 1000/ 3788]\n",
      "loss: 0.000003  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000002  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000002  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000001  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000006  [ 2300/ 3788]\n",
      "loss: 0.000011  [ 2400/ 3788]\n",
      "loss: 0.000003  [ 2500/ 3788]\n",
      "loss: 0.000002  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000002  [ 2800/ 3788]\n",
      "loss: 0.000002  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000001  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000002  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000003  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 68.7%, Avg loss: 1.628210 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.000001  [    0/ 3788]\n",
      "loss: 0.000012  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000002  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000001  [  500/ 3788]\n",
      "loss: 0.000006  [  600/ 3788]\n",
      "loss: 0.000001  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000002  [  900/ 3788]\n",
      "loss: 0.000001  [ 1000/ 3788]\n",
      "loss: 0.000002  [ 1100/ 3788]\n",
      "loss: 0.000004  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000001  [ 1500/ 3788]\n",
      "loss: 0.000003  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000001  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000001  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000002  [ 2600/ 3788]\n",
      "loss: 0.000019  [ 2700/ 3788]\n",
      "loss: 0.000004  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000004  [ 3100/ 3788]\n",
      "loss: 0.000003  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000029  [ 3500/ 3788]\n",
      "loss: 0.000014  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 68.4%, Avg loss: 1.269122 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000002  [  100/ 3788]\n",
      "loss: 0.000002  [  200/ 3788]\n",
      "loss: 0.000001  [  300/ 3788]\n",
      "loss: 0.000001  [  400/ 3788]\n",
      "loss: 0.000011  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000004  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000001  [ 1200/ 3788]\n",
      "loss: 0.000010  [ 1300/ 3788]\n",
      "loss: 0.000002  [ 1400/ 3788]\n",
      "loss: 0.000003  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000002  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000001  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000001  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000003  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000002  [ 2600/ 3788]\n",
      "loss: 0.000015  [ 2700/ 3788]\n",
      "loss: 0.000002  [ 2800/ 3788]\n",
      "loss: 0.000002  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000008  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000009  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000001  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 68.0%, Avg loss: 1.414378 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000003  [  100/ 3788]\n",
      "loss: 0.000043  [  200/ 3788]\n",
      "loss: 0.000001  [  300/ 3788]\n",
      "loss: 0.000001  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000001  [  600/ 3788]\n",
      "loss: 0.000011  [  700/ 3788]\n",
      "loss: 0.000002  [  800/ 3788]\n",
      "loss: 0.000001  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000001  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000001  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000002  [ 2200/ 3788]\n",
      "loss: 0.000001  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000001  [ 2600/ 3788]\n",
      "loss: 0.000001  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000001  [ 3300/ 3788]\n",
      "loss: 0.000001  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000007  [ 3600/ 3788]\n",
      "loss: 0.000003  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 56.6%, Avg loss: 1.536692 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 3788]\n",
      "loss: 0.000003  [  100/ 3788]\n",
      "loss: 0.000004  [  200/ 3788]\n",
      "loss: 0.000015  [  300/ 3788]\n",
      "loss: 0.000001  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000017  [  800/ 3788]\n",
      "loss: 0.000006  [  900/ 3788]\n",
      "loss: 0.000001  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000038  [ 1200/ 3788]\n",
      "loss: 0.000001  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000001  [ 1600/ 3788]\n",
      "loss: 0.000003  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000018  [ 2000/ 3788]\n",
      "loss: 0.000001  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000001  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000001  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000002  [ 3100/ 3788]\n",
      "loss: 0.026378  [ 3200/ 3788]\n",
      "loss: 0.000004  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 66.8%, Avg loss: 1.480975 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000001  [  100/ 3788]\n",
      "loss: 0.000001  [  200/ 3788]\n",
      "loss: 0.000003  [  300/ 3788]\n",
      "loss: 0.000020  [  400/ 3788]\n",
      "loss: 0.000002  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000002  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000001  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000001  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000004  [ 1600/ 3788]\n",
      "loss: 0.000001  [ 1700/ 3788]\n",
      "loss: 0.000010  [ 1800/ 3788]\n",
      "loss: 0.000013  [ 1900/ 3788]\n",
      "loss: 0.000002  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000001  [ 2200/ 3788]\n",
      "loss: 0.000003  [ 2300/ 3788]\n",
      "loss: 0.000001  [ 2400/ 3788]\n",
      "loss: 0.000002  [ 2500/ 3788]\n",
      "loss: 0.000001  [ 2600/ 3788]\n",
      "loss: 0.000002  [ 2700/ 3788]\n",
      "loss: 0.000003  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000001  [ 3000/ 3788]\n",
      "loss: 0.000006  [ 3100/ 3788]\n",
      "loss: 0.000002  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000001  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 63.3%, Avg loss: 1.390859 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000001  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000002  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000001  [  600/ 3788]\n",
      "loss: 0.000003  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000002  [ 1100/ 3788]\n",
      "loss: 0.000001  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000003  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000002  [ 1700/ 3788]\n",
      "loss: 0.000002  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000006  [ 2100/ 3788]\n",
      "loss: 0.000001  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000001  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000002  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000001  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000001  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000001  [ 3600/ 3788]\n",
      "loss: 0.000002  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 58.1%, Avg loss: 1.456154 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000001  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000008  [  300/ 3788]\n",
      "loss: 0.000001  [  400/ 3788]\n",
      "loss: 0.000005  [  500/ 3788]\n",
      "loss: 0.000002  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000002  [  800/ 3788]\n",
      "loss: 0.000090  [  900/ 3788]\n",
      "loss: 0.000002  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000001  [ 1200/ 3788]\n",
      "loss: 0.000002  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000002  [ 1700/ 3788]\n",
      "loss: 0.000001  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000004  [ 2000/ 3788]\n",
      "loss: 0.000154  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000002  [ 2300/ 3788]\n",
      "loss: 0.000001  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000007  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000007  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 57.1%, Avg loss: 1.645456 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000009  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000001  [  800/ 3788]\n",
      "loss: 0.000001  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000001  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000003  [ 1400/ 3788]\n",
      "loss: 0.000004  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000003  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000012  [ 1900/ 3788]\n",
      "loss: 0.000003  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000001  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000001  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000006  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000005  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 28.5%, Avg loss: 1.635597 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000001  [  200/ 3788]\n",
      "loss: 0.000001  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000003  [  700/ 3788]\n",
      "loss: 0.000001  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000001  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000002  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000001  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000001  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000001  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000008  [ 2500/ 3788]\n",
      "loss: 0.000024  [ 2600/ 3788]\n",
      "loss: 0.000001  [ 2700/ 3788]\n",
      "loss: 0.000001  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000001  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 62.6%, Avg loss: 1.425658 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000001  [  100/ 3788]\n",
      "loss: 0.000002  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000001  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000001  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000001  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000001  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000005  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000001  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000001  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000002  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000038  [ 3200/ 3788]\n",
      "loss: 0.000003  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 42.3%, Avg loss: 1.623513 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000007  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000003  [  500/ 3788]\n",
      "loss: 0.000001  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000006  [  900/ 3788]\n",
      "loss: 0.000001  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000006  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000002  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000001  [ 1800/ 3788]\n",
      "loss: 0.000003  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000001  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000001  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000001  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000004  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000001  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000002  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 45.5%, Avg loss: 1.772917 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000011  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.001010  [  900/ 3788]\n",
      "loss: 0.000001  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000045  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000001  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000002  [ 2200/ 3788]\n",
      "loss: 0.000001  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000005  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 63.1%, Avg loss: 1.601124 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000001  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000005  [  700/ 3788]\n",
      "loss: 0.000008  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000002  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000001  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000001  [ 2000/ 3788]\n",
      "loss: 0.000103  [ 2100/ 3788]\n",
      "loss: 0.000004  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000001  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000001  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000001  [ 3600/ 3788]\n",
      "loss: 0.000001  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 56.3%, Avg loss: 1.569021 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.000001  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000002  [  300/ 3788]\n",
      "loss: 0.000005  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000003  [  700/ 3788]\n",
      "loss: 0.000001  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000001  [ 1200/ 3788]\n",
      "loss: 0.000001  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000002  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000004  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000003  [ 2800/ 3788]\n",
      "loss: 0.000003  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000001  [ 3500/ 3788]\n",
      "loss: 0.000001  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 68.1%, Avg loss: 1.417501 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000007  [  400/ 3788]\n",
      "loss: 0.000003  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000001  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000001  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000001  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000004  [ 3100/ 3788]\n",
      "loss: 0.000001  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000003  [ 3600/ 3788]\n",
      "loss: 0.000001  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 66.6%, Avg loss: 1.655953 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000003  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000004  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000001  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000001  [ 1400/ 3788]\n",
      "loss: 0.000003  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000008  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000002  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000001  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000001  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000038  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 58.6%, Avg loss: 1.575220 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000002  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000001  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000006  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000004  [ 1600/ 3788]\n",
      "loss: 0.000001  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000001  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000005  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000003  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000002  [ 3200/ 3788]\n",
      "loss: 0.000001  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000001  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 70.7%, Avg loss: 1.577017 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000001  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000001  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000001  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000001  [ 2800/ 3788]\n",
      "loss: 0.000002  [ 2900/ 3788]\n",
      "loss: 0.000001  [ 3000/ 3788]\n",
      "loss: 0.000001  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000001  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 30.4%, Avg loss: 1.731881 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000001  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 42.6%, Avg loss: 1.757475 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000003  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 44.1%, Avg loss: 1.659348 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 66.4%, Avg loss: 1.780276 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 59.1%, Avg loss: 1.514051 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 60.3%, Avg loss: 1.989576 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 37.8%, Avg loss: 1.822333 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 29.5%, Avg loss: 1.901765 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 53.6%, Avg loss: 1.847935 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 33.5%, Avg loss: 1.865990 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 64.4%, Avg loss: 2.074608 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 25.4%, Avg loss: 1.859303 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 33.5%, Avg loss: 2.155806 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 67.2%, Avg loss: 1.898892 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 24.5%, Avg loss: 1.911361 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 51.0%, Avg loss: 2.086092 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 46.1%, Avg loss: 2.086974 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 76.5%, Avg loss: 1.899561 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 45.2%, Avg loss: 1.903827 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 17.2%, Avg loss: 1.950154 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 64.7%, Avg loss: 1.792022 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000001  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 42.4%, Avg loss: 2.122432 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 59.7%, Avg loss: 1.712023 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 64.0%, Avg loss: 2.100959 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 70.3%, Avg loss: 1.915183 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 44.8%, Avg loss: 1.927948 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.000000  [    0/ 3788]\n",
      "loss: 0.000000  [  100/ 3788]\n",
      "loss: 0.000000  [  200/ 3788]\n",
      "loss: 0.000000  [  300/ 3788]\n",
      "loss: 0.000000  [  400/ 3788]\n",
      "loss: 0.000000  [  500/ 3788]\n",
      "loss: 0.000000  [  600/ 3788]\n",
      "loss: 0.000000  [  700/ 3788]\n",
      "loss: 0.000000  [  800/ 3788]\n",
      "loss: 0.000000  [  900/ 3788]\n",
      "loss: 0.000000  [ 1000/ 3788]\n",
      "loss: 0.000000  [ 1100/ 3788]\n",
      "loss: 0.000000  [ 1200/ 3788]\n",
      "loss: 0.000000  [ 1300/ 3788]\n",
      "loss: 0.000000  [ 1400/ 3788]\n",
      "loss: 0.000000  [ 1500/ 3788]\n",
      "loss: 0.000000  [ 1600/ 3788]\n",
      "loss: 0.000000  [ 1700/ 3788]\n",
      "loss: 0.000000  [ 1800/ 3788]\n",
      "loss: 0.000000  [ 1900/ 3788]\n",
      "loss: 0.000000  [ 2000/ 3788]\n",
      "loss: 0.000000  [ 2100/ 3788]\n",
      "loss: 0.000000  [ 2200/ 3788]\n",
      "loss: 0.000000  [ 2300/ 3788]\n",
      "loss: 0.000000  [ 2400/ 3788]\n",
      "loss: 0.000000  [ 2500/ 3788]\n",
      "loss: 0.000000  [ 2600/ 3788]\n",
      "loss: 0.000000  [ 2700/ 3788]\n",
      "loss: 0.000000  [ 2800/ 3788]\n",
      "loss: 0.000000  [ 2900/ 3788]\n",
      "loss: 0.000000  [ 3000/ 3788]\n",
      "loss: 0.000000  [ 3100/ 3788]\n",
      "loss: 0.000000  [ 3200/ 3788]\n",
      "loss: 0.000000  [ 3300/ 3788]\n",
      "loss: 0.000000  [ 3400/ 3788]\n",
      "loss: 0.000000  [ 3500/ 3788]\n",
      "loss: 0.000000  [ 3600/ 3788]\n",
      "loss: 0.000000  [ 3700/ 3788]\n",
      "Test Error:\n",
      " Accuracy: 42.4%, Avg loss: 2.113248 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "# model = model = EEG_LSTM(input_size=31000, hidden_size=128, num_layers=2,num_classes=train_class_number).to(device)\n",
    "#model = model.cuda()\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "accuracy = []\n",
    "for t in range(num_epochs):\n",
    "    print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "    model.train(True)\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "\n",
    "    model.train(False)\n",
    "    test(test_loader, model, loss_fn)\n",
    "\n",
    "    if t > num_epochs-5:\n",
    "        torch.save(model.state_dict(), os.path.join(model_save,'cnn+lstm_1000_100e_'+str(t)+'.pt'))\n",
    "\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:40:27.215094100Z",
     "start_time": "2023-05-26T13:18:18.230810800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGwCAYAAACHJU4LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA480lEQVR4nO3deVyVZf7/8fcB4SAgi6moiVsugYrmklGN1Yi5NKbWlKnf1BabGm0zrbRfpTVFM5WtastUjk2llZmWZpmJlbkrikqaioLjgiuIIijn+v3heMaToBw5h/sGXs/H4zyGc9/Xue/PxU1z3l73fV+3wxhjBAAAYEMBVhcAAABQEoIKAACwLYIKAACwLYIKAACwLYIKAACwLYIKAACwLYIKAACwrWpWF1AWLpdLu3btUo0aNeRwOKwuBwAAlIIxRkeOHFH9+vUVEHDuMZMKHVR27dql2NhYq8sAAAAXICsrSw0aNDhnmwodVGrUqCHpVEcjIiIsrgYAAJRGbm6uYmNj3d/j51Khg8rp0z0REREEFQAAKpjSXLbBxbQAAMC2CCoAAMC2CCoAAMC2KvQ1KgCAysvlcqmwsNDqMnABgoKCFBgY6JNtEVQAALZTWFiojIwMuVwuq0vBBYqKilLdunXLPM8ZQQUAYCvGGO3evVuBgYGKjY0974RgsBdjjI4dO6bs7GxJUr169cq0PYIKAMBWTp48qWPHjql+/foKDQ21uhxcgOrVq0uSsrOzVadOnTKdBiKmAgBspaioSJIUHBxscSUoi9Mh88SJE2XaDkEFAGBLPMOtYvPV8SOoAAAA2yKoAAAA2yKoAABgQ40bN9arr75q+Tasxl0/5aggP1/O/14JDQCoXK699lq1a9fOZ8FgxYoVCgsL88m2KjJGVMrJyoULdXvH9pr57jtWlwIAsIgxRidPnixV29q1a3N7tggq5ebtp5+UJH3y6isWVwIAFYsxRsePHbPkZYwpVY1Dhw7VokWL9Nprr8nhcMjhcGj79u1KSUmRw+HQN998ow4dOsjpdOrnn3/W1q1b1adPH8XExCg8PFydOnXS999/77HN35+2cTgc+uc//6l+/fopNDRUzZs31+zZs736XWZmZqpPnz4KDw9XRESEbr31Vu3du9e9fu3atbruuutUo0YNRUREqEOHDlq5cqUkaceOHerdu7eio6MVFhamVq1aae7cuV7t/0Jw6gcAYGsF+fka3KmDJfueumKVQkoxqvHaa69p8+bNat26tZ555hlJp0ZEtm/fLkl6/PHH9dJLL6lp06aKjo5WVlaWevXqpeeee05Op1NTp05V7969tWnTJjVs2LDE/YwfP17/+Mc/9OKLL+qNN97QoEGDtGPHDtWsWfO8NbpcLndIWbRokU6ePKnhw4erf//+SklJkSQNGjRIl112mSZPnqzAwEClpqYqKChIkjR8+HAVFhbqxx9/VFhYmDZu3Kjw8PDz7resCCoAAJRRZGSkgoODFRoaqrp16561/plnnlG3bt3c72vWrKm2bdu63z/77LOaOXOmZs+erREjRpS4n6FDh2rAgAGSpOeff16vv/66li9frh49epy3xgULFigtLU0ZGRmKjY2VJE2dOlWtWrXSihUr1KlTJ2VmZmr06NG69NJLJUnNmzd3fz4zM1M333yz2rRpI0lq2rTpeffpCwQVAICtOatX19QVqyzbty907NjR431eXp7GjRunOXPmaPfu3Tp58qTy8/OVmZl5zu0kJCS4fw4LC1NERIT7mTrnk56ertjYWHdIkaT4+HhFRUUpPT1dnTp10siRI3X33Xfrww8/VFJSkm655RZdcsklkqQHHnhA9913n7777jslJSXp5ptv9qjHX7hGBQBgaw6HQyGhoZa8fDW76u/v3hk1apRmzpyp559/Xj/99JNSU1PVpk0bFRYWnnM7p0/DnPm78eUTpseNG6cNGzbohhtu0A8//KD4+HjNnDlTknT33Xdr27Ztuv3225WWlqaOHTvqjTfe8Nm+S0JQAQDAB4KDg93PKTqfxYsXa+jQoerXr5/atGmjunXruq9n8Ze4uDhlZWUpKyvLvWzjxo06fPiw4uPj3ctatGihhx9+WN99951uuukmffDBB+51sbGxuvfee/XFF1/okUce0bvvvuvXmiWCCgAAPtG4cWMtW7ZM27dv1/79+8850tG8eXN98cUXSk1N1dq1azVw4ECfjowUJykpSW3atNGgQYO0evVqLV++XIMHD9Y111yjjh07Kj8/XyNGjFBKSop27NihxYsXa8WKFYqLi5MkPfTQQ/r222+VkZGh1atXa+HChe51/kRQAQDAB0aNGqXAwEDFx8erdu3a57zeZMKECYqOjtaVV16p3r17q3v37mrfvr1f63M4HJo1a5aio6PVpUsXJSUlqWnTppo+fbokKTAwUAcOHNDgwYPVokUL3XrrrerZs6fGjx8v6dRTrYcPH664uDj16NFDLVq00KRJk/xasyQ5TGlvEreh3NxcRUZGKicnRxEREVaXc07DulytnAMHJEmfbki3uBoAsK/jx48rIyNDTZo0UUhIiNXl4AKd6zh68/3NiAoAALAtgko5qcADVwAAWIagAgAAbIugUk58dS8+AABVCUEFAADYFkEFAADYFkEFAADYFkEFAADYFkEFAACbaNy4sV599VX3e4fDoS+//LLE9tu3b5fD4VBqamqpt1nRVLO6AAAAULzdu3crOjra6jIsRVABAMCm6tata3UJluPUDwAAZfTOO++ofv36Zz0BuU+fPrrzzjslSVu3blWfPn0UExOj8PBwderUSd9///05t/v7Uz/Lly/XZZddppCQEHXs2FFr1qzxutbMzEz16dNH4eHhioiI0K233qq9e/e6169du1bXXXedatSooYiICHXo0EErV66UJO3YsUO9e/dWdHS0wsLC1KpVK82dO9frGrzBiEo5YQp9ALgwxhgdKzhuyb5DnSGlmrDzlltu0f3336+FCxeqa9eukqSDBw9q3rx57i/yvLw89erVS88995ycTqemTp2q3r17a9OmTWrYsOF595GXl6c//elP6tatm/79738rIyNDDz74oFf9cblc7pCyaNEinTx5UsOHD1f//v2VkpIiSRo0aJAuu+wyTZ48WYGBgUpNTVVQUJAkafjw4SosLNSPP/6osLAwbdy4UeHh4V7V4C2CCgDA1o4VHFf4nb0t2Xfe+18pLKT6edtFR0erZ8+e+vjjj91B5fPPP1etWrV03XXXSZLatm2rtm3buj/z7LPPaubMmZo9e7ZGjBhx3n18/PHHcrlceu+99xQSEqJWrVpp586duu+++0rdnwULFigtLU0ZGRmKjY2VJE2dOlWtWrXSihUr1KlTJ2VmZmr06NG69NJLJUnNmzd3fz4zM1M333yz2rRpI0lq2rRpqfd9oTj1U06YQh8AKrdBgwZpxowZKigokCR99NFHuu222xQQcOqrNi8vT6NGjVJcXJyioqIUHh6u9PR0ZWZmlmr76enpSkhIUEhIiHtZYmKiVzWmp6crNjbWHVIkKT4+XlFRUUpPT5ckjRw5UnfffbeSkpL0wgsvaOvWre62DzzwgP72t7/pqquu0tNPP61169Z5tf8LwYgKAMDWQp0hynv/K8v2XVq9e/eWMUZz5sxRp06d9NNPP+mVV15xrx81apTmz5+vl156Sc2aNVP16tX15z//WYWFhf4o/YKNGzdOAwcO1Jw5c/TNN9/o6aef1rRp09SvXz/dfffd6t69u+bMmaPvvvtOycnJevnll3X//ff7rR6CCgDA1hwOR6lOv1gtJCREN910kz766CNt2bJFLVu2VPv27d3rFy9erKFDh6pfv36STo2wbN++vdTbj4uL04cffqjjx4+7R1WWLl3qVY1xcXHKyspSVlaWe1Rl48aNOnz4sOLj493tWrRooRYtWujhhx/WgAED9MEHH7jrjo2N1b333qt7771XY8aM0bvvvuvXoMKpHwAAfGTQoEGaM2eO3n//fQ0aNMhjXfPmzfXFF18oNTVVa9eu1cCBA8+6S+hcBg4cKIfDoWHDhmnjxo2aO3euXnrpJa/qS0pKUps2bTRo0CCtXr1ay5cv1+DBg3XNNdeoY8eOys/P14gRI5SSkqIdO3Zo8eLFWrFiheLi4iRJDz30kL799ltlZGRo9erVWrhwoXudvxBUAADwkT/+8Y+qWbOmNm3apIEDB3qsmzBhgqKjo3XllVeqd+/e6t69u8eIy/mEh4frq6++Ulpami677DI98cQT+vvf/+5VfQ6HQ7NmzVJ0dLS6dOmipKQkNW3aVNOnT5ckBQYG6sCBAxo8eLBatGihW2+9VT179tT48eMlSUVFRRo+fLji4uLUo0cPtWjRQpMmTfKqBm85TAW+bzY3N1eRkZHKyclRRESE1eWc07AuVyvnwAFJ0qcb0i2uBgDs6/jx48rIyFCTJk08LhxFxXKu4+jN9zcjKgAAwLYIKgAAwLYsDSrjxo2Tw+HweJ2eYAYAAMDy25NbtWrl8ayDatUsL8kvKvClQAAAWMbyVFCtWjWeDgkAOAv/wKvYfHX8LL9G5bffflP9+vXVtGlTDRo06JxTCRcUFCg3N9fjVVEwhT4AlE5gYKAk2W7GVnjn2LFjkuR+oOGFsnREpXPnzpoyZYpatmyp3bt3a/z48frDH/6g9evXq0aNGme1T05Odt/LDQConKpVq6bQ0FDt27dPQUFB7mfloGIwxujYsWPKzs5WVFSUO3heKFvNo3L48GE1atRIEyZM0F133XXW+oKCAvfDnqRT92HHxsYyjwoAVDKFhYXKyMjwauZW2EtUVJTq1q1b7BkFb+ZRsfwalTNFRUWpRYsW2rJlS7HrnU6nnE5nOVcFAChvwcHBat68Oad/KqigoKAyj6ScZqugkpeXp61bt+r222+3uhQAgMUCAgKYmRbWXkw7atQoLVq0SNu3b9cvv/yifv36KTAwUAMGDLCyLAAAYBOWjqjs3LlTAwYM0IEDB1S7dm1dffXVWrp0qWrXrm1lWQAAwCYsDSrTpk2zcvcAAMDmuOcLAADYFkGlnNjoLnAAACoMggoAALAtgko5YQp9AAC8R1ABAAC2RVABAAC2RVABAAC2RVABAAC2RVABAAC2RVABAAC2RVABAAC2RVApJ8xMCwCA9wgqAADAtggqAADAtggq5YQp9AEA8B5BBQAA2BZBBQAA2BZBBQAA2BZBBQAA2BZBBQAA2BZBBQAA2BZBBQAA2BZBpZwwhT4AAN4jqAAAANsiqAAAANsiqAAAANsiqJQTnvUDAID3CCoAAMC2CCoAAMC2CCoAAMC2CCoAAMC2CCoAAMC2CCoAAMC2CCrlhCn0AQDwHkEFAADYFkEFAADYFkEFAADYFkGlnDCFPgAA3iOoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKoAAAA2yKolBOm0AcAwHsEFQAAYFsEFQAAYFsEFQAAYFsElXLCFPoAAHiPoAIAAGzLNkHlhRdekMPh0EMPPWR1KQAAwCZsEVRWrFiht99+WwkJCVaXAgAAbMTyoJKXl6dBgwbp3XffVXR09DnbFhQUKDc31+MFAAAqL8uDyvDhw3XDDTcoKSnpvG2Tk5MVGRnpfsXGxpZDhQAAwCqWBpVp06Zp9erVSk5OLlX7MWPGKCcnx/3Kysryc4UAAMBK1azacVZWlh588EHNnz9fISEhpfqM0+mU0+n0c2X+wRT6AAB4z7KgsmrVKmVnZ6t9+/buZUVFRfrxxx/15ptvqqCgQIGBgVaVBwAAbMCyoNK1a1elpaV5LLvjjjt06aWX6rHHHiOkAAAA64JKjRo11Lp1a49lYWFhuuiii85aDgAAqibL7/qpKphCHwAA71k2olKclJQUq0sAAAA2wogKAACwLYIKAACwLYIKAACwLYIKAACwLYIKAACwLYJKOWEKfQAAvEdQAQAAtkVQAQAAtkVQAQAAtkVQKSdMoQ8AgPcIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKgAAwLYIKuWEmWkBAPAeQQUAANgWQQUAANgWQQUAANgWQQUAANgWQQUAANgWQaWc8KwfAAC8R1ABAAC2RVABAAC2RVABAAC2RVABAAC2RVApJ0yhDwCA9wgqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggq5YQp9AEA8B5BBQAA2BZBBQAA2JbXQWXevHn6+eef3e8nTpyodu3aaeDAgTp06JBPiwMAAFWb10Fl9OjRys3NlSSlpaXpkUceUa9evZSRkaGRI0f6vEAAAFB1VfP2AxkZGYqPj5ckzZgxQ3/605/0/PPPa/Xq1erVq5fPC6wsmEIfAADveT2iEhwcrGPHjkmSvv/+e11//fWSpJo1a7pHWgAAAHzB6xGVq6++WiNHjtRVV12l5cuXa/r06ZKkzZs3q0GDBj4vEAAAVF1ej6i8+eabqlatmj7//HNNnjxZF198sSTpm2++UY8ePXxeIAAAqLq8HlFp2LChvv7667OWv/LKKz4pCAAA4DSvR1RWr16ttLQ09/tZs2apb9++Gjt2rAoLC31aHAAAqNq8Dip/+ctftHnzZknStm3bdNtttyk0NFSfffaZHn30UZ8XWFkwhT4AAN7zOqhs3rxZ7dq1kyR99tln6tKliz7++GNNmTJFM2bM8HV9AACgCvM6qBhj5HK5JJ26Pfn03CmxsbHav3+/b6sDAABVmtdBpWPHjvrb3/6mDz/8UIsWLdINN9wg6dREcDExMT4vEAAAVF1eB5VXX31Vq1ev1ogRI/TEE0+oWbNmkqTPP/9cV155pc8LBAAAVZfXtycnJCR43PVz2osvvqjAwECfFFUZMYU+AADe83pE5bRVq1bp3//+t/79739r9erVCgkJUVBQkFfbmDx5shISEhQREaGIiAglJibqm2++udCSAABAJeP1iEp2drb69++vRYsWKSoqSpJ0+PBhXXfddZo2bZpq165d6m01aNBAL7zwgpo3by5jjP71r3+pT58+WrNmjVq1auVtaQAAoJLxekTl/vvvV15enjZs2KCDBw/q4MGDWr9+vXJzc/XAAw94ta3evXurV69eat68uVq0aKHnnntO4eHhWrp0abHtCwoKlJub6/ECAACVl9dBZd68eZo0aZLi4uLcy+Lj4zVx4sQynbYpKirStGnTdPToUSUmJhbbJjk5WZGRke5XbGzsBe8PAADYn9dBxeVyFXstSlBQkHt+FW+kpaUpPDxcTqdT9957r2bOnKn4+Phi244ZM0Y5OTnuV1ZWltf7AwAAFYfXQeWPf/yjHnzwQe3atcu97D//+Y8efvhhde3a1esCWrZsqdTUVC1btkz33XefhgwZoo0bNxbb1ul0ui+8Pf2qKJhCHwAA73kdVN58803l5uaqcePGuuSSS3TJJZeoSZMmys3N1RtvvOF1AcHBwWrWrJk6dOig5ORktW3bVq+99prX2wEAAJWP13f9xMbGavXq1fr+++/166+/SpLi4uKUlJTkk4JcLpcKCgp8si0AAFCxeR1UpFOnMbp166Zu3bqVaedjxoxRz5491bBhQx05ckQff/yxUlJS9O2335ZpuwAAoHIoVVB5/fXXS71Bb25Rzs7O1uDBg7V7925FRkYqISFB3377bZkDEAAAqBxKFVReeeWVUm3M4XB4FVTee++9Uret6JhCHwAA75UqqGRkZPi7DgAAgLNc8LN+AAAA/I2gAgAAbIugAgAAbIugAgAAbIugUk6YQh8AAO+V6q6fdevWlXqDCQkJF1wMAADAmUoVVNq1ayeHwyFjzHlHBoqKinxSGAAAQKlO/WRkZGjbtm3KyMjQjBkz1KRJE02aNElr1qzRmjVrNGnSJF1yySWaMWOGv+sFAABVSKlGVBo1auT++ZZbbtHrr7+uXr16uZclJCQoNjZWTz75pPr27evzIisDZqYFAMB7Xl9Mm5aWpiZNmpy1vEmTJtq4caNPigIAAJAuIKjExcUpOTlZhYWF7mWFhYVKTk5WXFycT4sDAABVW6lO/ZzprbfeUu/evdWgQQP3HT7r1q2Tw+HQV1995fMCAQBA1eV1ULn88su1bds2ffTRR/r1118lSf3799fAgQMVFhbm8wIBAEDV5XVQkaSwsDDdc889vq4FAADAQ6mCyuzZs9WzZ08FBQVp9uzZ52x74403+qQwq2Vt+U0/zJihvncPU+RFF1ldDgAAVVKpgkrfvn21Z88e1alT55y3Hzscjkoz4dsjfU4Frt07tuvxSW9ZXA0AAFVTqYKKy+Uq9ueqYNuGDT7ZDs/6AQDAezyUEAAA2FapRlRef/31Um/wgQceuOBiAAAAzlSqoPLKK6+UamMOh4OgUgKm0AcAwHulCioZGRn+rgMAAOAsXKMCAABs64ImfNu5c6dmz56tzMxMj2f+SNKECRN8UhgAAIDXQWXBggW68cYb1bRpU/36669q3bq1tm/fLmOM2rdv748aLcVtxQAAWMfrUz9jxozRqFGjlJaWppCQEM2YMUNZWVm65pprdMstt/ijRgAAUEV5HVTS09M1ePBgSVK1atWUn5+v8PBwPfPMM/r73//u8wIBAEDV5XVQCQsLc1+XUq9ePW3dutW9bv/+/b6rDAAAVHleX6NyxRVX6Oeff1ZcXJx69eqlRx55RGlpafriiy90xRVX+KPGSoFrXQAA8J7XQWXChAnKy8uTJI0fP155eXmaPn26mjdvzh0/AADAp7wOKk2bNnX/HBYWprfe4snCAADAP7y+RuXuu+9WSkqKH0qp3JhCHwAA73kdVPbt26cePXooNjZWo0eP1tq1a/1RFwAAgPdBZdasWdq9e7eefPJJrVixQu3bt1erVq30/PPPa/v27X4oEQAAVFUX9Kyf6Oho3XPPPUpJSdGOHTs0dOhQffjhh2rWrJmv6wMAAFVYmR5KeOLECa1cuVLLli3T9u3bFRMT46u67IPbigEAsMwFBZWFCxdq2LBhiomJ0dChQxUREaGvv/5aO3fu9HV9AACgCvP69uSLL75YBw8eVI8ePfTOO++od+/ecjqd/qgNAABUcV4HlXHjxumWW25RVFSUH8oBAAD4H6+DyrBhw/xRR6XHFPoAAHivTBfTAgAA+BNBBQAA2BZBpQyOFRzXDf8Yq7cXfH3etkyhDwCA9wgqZTBp/mzNTV2ue9971epSAAColAgqZZBz7Kj756TnRmvLnv9YWA0AAJUPQcVHFmxYo49/+cHqMgAAqFQIKmXw+1uOf2NEBQAAnyKonMe55j9ZuiXd4/2uQwf8XQ4AAFUKQaUM5qet8njPlG4AAPiWpUElOTlZnTp1Uo0aNVSnTh317dtXmzZtsrKkMgkIIPcBAOBLln6zLlq0SMOHD9fSpUs1f/58nThxQtdff72OHj16/g/b0LlGVJhCHwAA73n9rB9fmjdvnsf7KVOmqE6dOlq1apW6dOliUVUXjjACAIBvWRpUfi8nJ0eSVLNmzWLXFxQUqKCgwP0+Nze3XOoqTnEzzRJUAADwLdtcVOFyufTQQw/pqquuUuvWrYttk5ycrMjISPcrNja2nKv8n+JuRQ44R1BhCn0AALxnm6AyfPhwrV+/XtOmTSuxzZgxY5STk+N+ZWVllWOFnk4WFZ21jBEVAAB8yxZBZcSIEfr666+1cOFCNWjQoMR2TqdTERERHi+rFDd64uAG5VJbu3ix7u/RXRtXrrC6FACAjVkaVIwxGjFihGbOnKkffvhBTZo0sbIcrxR3KzIjKqX33D13a29WpsYNGWx1KQAAG7P0Ytrhw4fr448/1qxZs1SjRg3t2bNHkhQZGanq1atbWZpbSeGjuKWtGjTybzEAAFQxlo6oTJ48WTk5Obr22mtVr14992v69OlWluWhpItgixtReajnzf4uBwCAKsXSEZWKfCdMcdeoBAfa6m5vAAAqPFtcTGs3ZwaoEk/9FHcxLdeoAADgUwSVYsz/9MJOPZ0rpxBiAADwHkGlGHXqX+zx3hijH9PXaX9ujsey3wtw8OsEAMCX+GYtRsRFnlP4f7lysa55dqSajxziXsYU+gAA+B9BpRghoaEe72evWiJJOnwsz72suMuAzxVTKvKFwwAAWIWgUgxnSMlzuHyQMk+rtm1Ws4fPnqiMERUAAHyL+2mLERBYcn67852XSv4c16gAAOBTfLMWIyAg8II+x4AKAAC+RVApxpkjKi6Xy8JKAACo2ggqxThzRMVVRFABAMAqXKNSDEfAmSMqRTp+ovCc7Xu3T1RE9VA5g4L9XRoAAFUKQaUYZz5w8MihQ9pzaP85288e9ay/SwIAoEri1E8xAgI9L6Z1nHOGFAAA4C8ElWL8/snIvribhzlWAADwHkGlGKdHVAr+e/fPovR1Zd4mM9MCAOA9rlEphiMgQFsuCtOaBlFqvTvn/B8AAAB+wYhKMRwOh9Y0iJIkra8XaW0xAABUYQQVAABgWwQVAABgWwQVAABgWwQVAABgWwQVAABgWwSVMhrR8RqrSwAAoNIiqJTRwFadrC4BAIBKi6BSTphCHwAA7xFUyglT6AMA4D2CCgAAsC2CCgAAsC2CShlcsf2g1SUAAFCpEVTKIDYn3+oSAACo1AgqFyis4KTVJQAAUOkRVC5QNZfL6hIAAKj0CCoXyMHdxgAA+B1B5QIxfRsAAP5HULlAjKgAAOB/BJVywhT6AAB4j6BygS46VuhVe6bQBwDAe9WsLqAi+Wu3G5VfWKBLYupr1ZPPWV0OAACVHkHFCxPveMD9861P/M3CSgAAqBo49QMAAGyLoAIAAGyLoFJKV2/bb3UJAABUOQSVYhR3h06Qi7t2AAAobwSVYhTxHB8AAGyBoFKMIlfR2QsZUAEAoNwRVIpxsqiYoAIAAModQaUY/jj1wxT6AAB4j6BSjOKCSnR+oT6fPOmCt8kU+gAAeI+gUozfB5XE7QcUaKRP33zDoooAAKiaLA0qP/74o3r37q369evL4XDoyy+/tLIct4UbUz3eBzAYAgCAJSwNKkePHlXbtm01ceJEK8s4S9aBfVaXAAAAZPFDCXv27KmePXtaWUKxAgI8L3xlQAUAAGtUqKcnFxQUqKCgwP0+NzfXL/txMQstAAC2UKEupk1OTlZkZKT7FRsb65f99Gp3uV+2CwAAvFOhgsqYMWOUk5PjfmVlZfllP5de3FCvtvtjqdsfy8vTqH59NO311/xSDwAAVVWFOvXjdDrldDrLZV/1wyPdP5vzzNU2f/o0ZW7erMzNm3XbAw/6uTIAAKqOCjWiUp6qBQWVqp1DDhWdPOnnagAAqJosHVHJy8vTli1b3O8zMjKUmpqqmjVrqmHDhhZWdiqohB8/obyQINXOKyyxnSnlPUFMoQ8AgPcsDSorV67Udddd534/cuRISdKQIUM0ZcoUi6o6JbBaNXXflK2iAIeCfHAXEFPoAwDgPUuDyrXXXmvbL/BqQUEKkBTArcoAAFiGa1RKEBgYaHUJAABUeQSVEnBNCQAA1iOolMARwK8GAACr8W1cAoejdL8ahxh5AQDAXwgqJfj9gwlLcvLECT9X4j+7d2zX5Cef0K7tGVaXAgBAsQgqJSjtqZ8pf0/2ax1FJ0/q+LFjftn2M3feqYVffKFn77rTL9sHAKCsCColKO2pn9yDB/1ax+ib+mpwpw7Ky8nx+bYP7Nn93//d4/NtV2Sb1qxR6s8/W10GAEAElRKV9tSPv+3culWStHHFcosrqTqe/L+Bev4vw3RoX7bVpQBAlUdQKQm3J1d5h/fvt7oEAKjyCColCCjlqR8AAOA/fBuXwGGTUz8AAFRlBJUSlHTXz4nCkp+kDAAAfIugUoKSTv0UHj9ezpWgPNn1IZkAUFURVErgchUVu/xEQUE5V4LyRFABAHshqJTgeH5+scsLCSoAAJQbgkoJqgUFFbvc5XKdtWzvzix/l4PywogKANgKQcUHDu/b5/VnjDHauXUrpxpshuMBAPZCUPGWj77IPnntVY288U/6aMLLPtkeAACVEUGlBI4SZqY18k1Q+fLddyRJs99/zyfbg28wogIA9kJQ8dJbTz151jK+2yoRDiYA2ApBpQQOFT+ikr5y5dltmcQWAAC/IKhcAE4PVF4cWWsVFhQo+z//sboMADZCUPEFhlQqDUKotcbc+meNuD5Jm9emWl0KAJsgqPgCX26AT2Rt2SJJWjx3rsWVALALggpwJkInANgKQaUk5zidc/zYMY/3fLdVHpz6AQB7IahcgCGXdyzT53leEAAApUNQscBfru1idQkoASMqAGAvBJUS+PNGnqO5uf7bOMqGoAIAtkJQ8QHuTgYAwD8IKiUo6Vk/qNzOHE8paXZiAED5IaiUgGsVAACwHkGlBDVjYkrdlkxTeRBQAcBeCColaJ7Q1uoSYAWCCgDYCkEFAADYFkHFB7jutvLg1A8A2AtBxcemvJDss3lS+NIsf/zOAcBeCCo+cOZ329wPp+rDl160rhgAACoRgoofZG7e7JPt8K97C/A7BwBbIaj4wO+vUTEul282zJdmqaX+9JN+Xb26zNsx4ncOAHZCUPEDl/EuqCz5dl6xyxlRKZ2D2dl6/t579NTtg6wuBQDgYwQVP8jYuNGr9q+MfFgHs7P9VE3ld3jfPp9ti2wIAPZCULGJvMOHz1rGiIr3yvw743cOALZCUPGB8323pS1dotyDB32/YZzFVVRkdQkAAB8iqJSDZ++60+oSqoyyXsh85ogMF9YCgPUIKjZ25tfkSw8+oF0ZGZbVUlG4fBlUGNECAMsRVHyguC+04d266vD+/aXfSDHT8P9+u6+MfNjb0qqcsgYVAIC9EFR8YO3in89atm/XLn35z3dLv5FS/ON9/549xS53uVxaPHeOsnfuLP3+KqkyB5UzwyEDKgBguWpWF1CZFeTnq7CgQA6H47ynEX6eO0dXFhWpcVzc/xaW8tTDT1/N1sSxYyRJn25Iv+B6K4MyX6MiTv3YAb97AKfZYkRl4sSJaty4sUJCQtS5c2ctX77c6pJ8YsHnn+n/2rcr1f/pfvnuO3r0zzd5LCvt/1mvX7bsguqrjHw2K3AFU3j8uNUlAIBfWB5Upk+frpEjR+rpp5/W6tWr1bZtW3Xv3l3ZTIB2QXz9L9HNqWv06M03aUMFCY9lv5j2zJ8rxr/qP35lgm7v1EFb16+3upQq7fC+fXrrqf+nbRs2WF0KUKlYfupnwoQJGjZsmO644w5J0ltvvaU5c+bo/fff1+OPP25pbc0TEvTbunXu922uuEJpS5f6dZ//fGa8Mn5N1/GjR3Vdv5s91h3NzdH8T6drybx5GvLY49qTuUPff/aZxzUy4+8YojvGPKFqQUH6bd06OauHqEZUtKJr19ai2bO09Nt5iq5dW527Xe+x7d07tiswsNpZDy76f4MGurc7cf73kqTtv/4qY4xqREVpw/Llahofr9jmzc/qS+7BQ3KGhCi4enVJUtZvm5V76JBaXd75rH3/6+9/V7PWrdWlT9+znp10JmNOTY4XFhGhgMBTOfvMWX337szS8fx85eflae0vi9Xx2usU5AyWMVLugQOqFhQkl3GpRlSUik4Wac1PP+rS9u0VHhkpScrZf8C9rUP7srVv139KLsYmTl8L9fbTT2n0G29YXI1v5B0+XCF+92eaOHaMNq5YoR9mzHD/twJUBs6Q6oqoWdOy/TuMhf9sLCwsVGhoqD7//HP17dvXvXzIkCE6fPiwZs2a5dG+oKBABQUF7ve5ubmKjY1VTk6OIiIifF5f/tGjGnJ5R/f7fy1fqZDQUPVvHe/zfQEAYEdX9bpBD774kk+3mZubq8jIyFJ9f1s6orJ//34VFRUpJibGY3lMTIx+/fXXs9onJydr/Pjx5VWeqoeFadq69fpqygdq2qqVqoeFSZL63fMXzXzn7XKr40KERUToaG6uV59x/nfkwxgjx3+HNQry893rg5xOnSwsLPaUSHBIyFnLTl83cXq7p7cVWC1IRSdPuNsFBAa6Z5QNcjolyb3/4pze7pn7/P2yM6/ZCHI6JWN0orDwf8uCgz3eF7et07XY3YkzwntFqbkkp/viCAhQtaAgi6vxTmU6DsCZAqtZe/LF0hGVXbt26eKLL9Yvv/yixMRE9/JHH31UixYt0rLfXSRa3iMqAADA9yrMiEqtWrUUGBiovXv3eizfu3ev6tate1Z7p9MpJ/9SAQCgyrD0rp/g4GB16NBBCxYscC9zuVxasGCBxwgLAAComiy/62fkyJEaMmSIOnbsqMsvv1yvvvqqjh496r4LCAAAVF2WB5X+/ftr3759euqpp7Rnzx61a9dO8+bNO+sCWwAAUPVYejFtWXlzMQ4AALAHb76/LZ+ZFgAAoCQEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFsEFQAAYFuWT6FfFqcn1c3NzbW4EgAAUFqnv7dLMzl+hQ4qR44ckSTFxsZaXAkAAPDWkSNHFBkZec42FfpZPy6XS7t27VKNGjXkcDh8uu3c3FzFxsYqKyur0j9HiL5WXlWpv/S18qpK/a0qfTXG6MiRI6pfv74CAs59FUqFHlEJCAhQgwYN/LqPiIiISv3Hcib6WnlVpf7S18qrKvW3KvT1fCMpp3ExLQAAsC2CCgAAsC2CSgmcTqeefvppOZ1Oq0vxO/paeVWl/tLXyqsq9bcq9bW0KvTFtAAAoHJjRAUAANgWQQUAANgWQQUAANgWQQUAANgWQaUYEydOVOPGjRUSEqLOnTtr+fLlVpfktXHjxsnhcHi8Lr30Uvf648ePa/jw4brooosUHh6um2++WXv37vXYRmZmpm644QaFhoaqTp06Gj16tE6ePFneXTnLjz/+qN69e6t+/fpyOBz68ssvPdYbY/TUU0+pXr16ql69upKSkvTbb795tDl48KAGDRqkiIgIRUVF6a677lJeXp5Hm3Xr1ukPf/iDQkJCFBsbq3/84x/+7lqxztffoUOHnnWse/To4dGmovQ3OTlZnTp1Uo0aNVSnTh317dtXmzZt8mjjq7/dlJQUtW/fXk6nU82aNdOUKVP83T0Ppenrtddee9axvffeez3aVIS+Tp48WQkJCe5JzBITE/XNN9+411eWY3ra+fpbWY5ruTHwMG3aNBMcHGzef/99s2HDBjNs2DATFRVl9u7da3VpXnn66adNq1atzO7du92vffv2udffe++9JjY21ixYsMCsXLnSXHHFFebKK690rz958qRp3bq1SUpKMmvWrDFz5841tWrVMmPGjLGiOx7mzp1rnnjiCfPFF18YSWbmzJke61944QUTGRlpvvzyS7N27Vpz4403miZNmpj8/Hx3mx49epi2bduapUuXmp9++sk0a9bMDBgwwL0+JyfHxMTEmEGDBpn169ebTz75xFSvXt28/fbb5dVNt/P1d8iQIaZHjx4ex/rgwYMebSpKf7t3724++OADs379epOammp69eplGjZsaPLy8txtfPG3u23bNhMaGmpGjhxpNm7caN544w0TGBho5s2bZ6u+XnPNNWbYsGEexzYnJ6fC9XX27Nlmzpw5ZvPmzWbTpk1m7NixJigoyKxfv94YU3mOaWn7W1mOa3khqPzO5ZdfboYPH+5+X1RUZOrXr2+Sk5MtrMp7Tz/9tGnbtm2x6w4fPmyCgoLMZ5995l6Wnp5uJJklS5YYY059OQYEBJg9e/a420yePNlERESYgoICv9bujd9/cbtcLlO3bl3z4osvupcdPnzYOJ1O88knnxhjjNm4caORZFasWOFu88033xiHw2H+85//GGOMmTRpkomOjvbo62OPPWZatmzp5x6dW0lBpU+fPiV+piL3Nzs720gyixYtMsb47m/30UcfNa1atfLYV//+/U337t393aUS/b6vxpz6QnvwwQdL/ExF7asxxkRHR5t//vOflfqYnul0f42p3MfVHzj1c4bCwkKtWrVKSUlJ7mUBAQFKSkrSkiVLLKzswvz222+qX7++mjZtqkGDBikzM1OStGrVKp04ccKjn5deeqkaNmzo7ueSJUvUpk0bxcTEuNt0795dubm52rBhQ/l2xAsZGRnas2ePR98iIyPVuXNnj75FRUWpY8eO7jZJSUkKCAjQsmXL3G26dOmi4OBgd5vu3btr06ZNOnToUDn1pvRSUlJUp04dtWzZUvfdd58OHDjgXleR+5uTkyNJqlmzpiTf/e0uWbLEYxun21j53/nv+3raRx99pFq1aql169YaM2aMjh075l5XEftaVFSkadOm6ejRo0pMTKzUx1Q6u7+nVbbj6k8V+qGEvrZ//34VFRV5/HFIUkxMjH799VeLqrownTt31pQpU9SyZUvt3r1b48eP1x/+8AetX79ee/bsUXBwsKKiojw+ExMToz179kiS9uzZU+zv4fQ6uzpdW3G1n9m3OnXqeKyvVq2aatas6dGmSZMmZ23j9Lro6Gi/1H8hevTooZtuuklNmjTR1q1bNXbsWPXs2VNLlixRYGBghe2vy+XSQw89pKuuukqtW7d21+KLv92S2uTm5io/P1/Vq1f3R5dKVFxfJWngwIFq1KiR6tevr3Xr1umxxx7Tpk2b9MUXX5yzH6fXnatNefc1LS1NiYmJOn78uMLDwzVz5kzFx8crNTW1Uh7TkvorVa7jWh4IKpVUz5493T8nJCSoc+fOatSokT799NNK9QcM6bbbbnP/3KZNGyUkJOiSSy5RSkqKunbtamFlZTN8+HCtX79eP//8s9Wl+F1Jfb3nnnvcP7dp00b16tVT165dtXXrVl1yySXlXWaZtGzZUqmpqcrJydHnn3+uIUOGaNGiRVaX5Tcl9Tc+Pr5SHdfywKmfM9SqVUuBgYFnXW2+d+9e1a1b16KqfCMqKkotWrTQli1bVLduXRUWFurw4cMebc7sZ926dYv9PZxeZ1enazvXMaxbt66ys7M91p88eVIHDx6s8P2XpKZNm6pWrVrasmWLpIrZ3xEjRujrr7/WwoUL1aBBA/dyX/3tltQmIiKi3IN8SX0tTufOnSXJ49hWlL4GBwerWbNm6tChg5KTk9W2bVu99tprlfKYSiX3tzgV+biWB4LKGYKDg9WhQwctWLDAvczlcmnBggUe5xYrory8PG3dulX16tVThw4dFBQU5NHPTZs2KTMz093PxMREpaWleXzBzZ8/XxEREe7hSztq0qSJ6tat69G33NxcLVu2zKNvhw8f1qpVq9xtfvjhB7lcLvf/YSQmJurHH3/UiRMn3G3mz5+vli1b2uq0T3F27typAwcOqF69epIqVn+NMRoxYoRmzpypH3744azTUb76201MTPTYxuk25fnf+fn6WpzU1FRJ8ji2FaGvxXG5XCooKKhUx/RcTve3OJXpuPqF1Vfz2s20adOM0+k0U6ZMMRs3bjT33HOPiYqK8rj6uiJ45JFHTEpKisnIyDCLFy82SUlJplatWiY7O9sYc+p2wIYNG5offvjBrFy50iQmJprExET350/fHnf99deb1NRUM2/ePFO7dm1b3J585MgRs2bNGrNmzRojyUyYMMGsWbPG7Nixwxhz6vbkqKgoM2vWLLNu3TrTp0+fYm9Pvuyyy8yyZcvMzz//bJo3b+5xu+7hw4dNTEyMuf3228369evNtGnTTGhoqCW3J5+rv0eOHDGjRo0yS5YsMRkZGeb777837du3N82bNzfHjx+vcP297777TGRkpElJSfG4dfPYsWPuNr742z19a+fo0aNNenq6mThxYrnf2nm+vm7ZssU888wzZuXKlSYjI8PMmjXLNG3a1HTp0qXC9fXxxx83ixYtMhkZGWbdunXm8ccfNw6Hw3z33XfGmMpzTEvT38p0XMsLQaUYb7zxhmnYsKEJDg42l19+uVm6dKnVJXmtf//+pl69eiY4ONhcfPHFpn///mbLli3u9fn5+eavf/2riY6ONqGhoaZfv35m9+7dHtvYvn276dmzp6levbqpVauWeeSRR8yJEyfKuytnWbhwoZF01mvIkCHGmFO3KD/55JMmJibGOJ1O07VrV7Np0yaPbRw4cMAMGDDAhIeHm4iICHPHHXeYI0eOeLRZu3atufrqq43T6TQXX3yxeeGFF8qrix7O1d9jx46Z66+/3tSuXdsEBQWZRo0amWHDhp0VrCtKf4vrpyTzwQcfuNv46m934cKFpl27diY4ONg0bdrUYx/l4Xx9zczMNF26dDE1a9Y0TqfTNGvWzIwePdpjvg1jKkZf77zzTtOoUSMTHBxsateubbp27eoOKcZUnmN62rn6W5mOa3lxGGNM+Y3fAAAAlB7XqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqAAAANsiqACoVFJSUuRwOM56yB2AiomgAgAAbIugAgAAbIugAsCnXC6XkpOT1aRJE1WvXl1t27bV559/Lul/p2XmzJmjhIQEhYSE6IorrtD69es9tjFjxgy1atVKTqdTjRs31ssvv+yxvqCgQI899phiY2PldDrVrFkzvffeex5tVq1apY4dOyo0NFRXXnmlNm3a5N+OA/ALggoAn0pOTtbUqVP11ltvacOGDXr44Yf1f//3f1q0aJG7zejRo/Xyyy9rxYoVql27tnr37q0TJ05IOhUwbr31Vt12221KS0vTuHHj9OSTT2rKlCnuzw8ePFiffPKJXn/9daWnp+vtt99WeHi4Rx1PPPGEXn75Za1cuVLVqlXTnXfeWS79B+BjVj++GUDlcfz4cRMaGmp++eUXj+V33XWXGTBggFm4cKGRZKZNm+Zed+DAAVO9enUzffp0Y4wxAwcONN26dfP4/OjRo018fLwxxphNmzYZSWb+/PnF1nB6H99//7172Zw5c4wkk5+f75N+Aig/jKgA8JktW7bo2LFj6tatm8LDw92vqVOnauvWre52iYmJ7p9r1qypli1bKj09XZKUnp6uq666ymO7V111lX777TcVFRUpNTVVgYGBuuaaa85ZS0JCgvvnevXqSZKys7PL3EcA5aua1QUAqDzy8vIkSXPmzNHFF1/ssc7pdHqElQtVvXr1UrULCgpy/+xwOCSdun4GQMXCiAoAn4mPj5fT6VRmZqaaNWvm8YqNjXW3W7p0qfvnQ4cOafPmzYqLi5MkxcXFafHixR7bXbx4sVq0aKHAwEC1adNGLpfL45oXAJUXIyoAfKZGjRoaNWqUHn74YblcLl199dXKycnR4sWLFRERoUaNGkmSnnnmGV100UWKiYnRE088oVq1aqlv376SpEceeUSdOnXSs88+q/79+2vJkiV68803NWnSJElS48aNNWTIEN155516/fXX1bZtW+3YsUPZ2dm69dZbreo6AD8hqADwqWeffVa1a9dWcnKytm3bpqioKLVv315jx451n3p54YUX9OCDD+q3335Tu3bt9NVXXyk4OFiS1L59e3366ad66qmn9Oyzz6pevXp65plnNHToUPc+Jk+erLFjx+qvf/2rDhw4oIYNG2rs2LFWdBeAnzmMMcbqIgBUDSkpKbruuut06NAhRUVFWV0OgAqAa1QAAIBtEVQAAIBtceoHAADYFiMqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtggqAADAtv4/ajVVJikgSWgAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Done!\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "hex_d1 = '#552a28'\n",
    "hex_d2 = '#005943'\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_loss, hex_d1, label='train loss')\n",
    "plt.ylabel('train loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(valid_loss, hex_d2, label='valid loss')\n",
    "plt.ylabel('valid loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(os.path.join(pic_dir, 'cnn_3_conv+lstm_128_s100_loss.png'))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-26T18:40:29.422959700Z",
     "start_time": "2023-05-26T18:40:27.231090Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(accuracy, color = '#550300', label='accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(pic_dir,'cnn_3_conv+lstm_128_s100_accuracy.png'))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-27T00:04:32.808612800Z",
     "start_time": "2023-05-27T00:04:32.435364500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
