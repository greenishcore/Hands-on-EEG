{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T00:10:59.305990Z",
     "end_time": "2023-05-02T00:10:59.329538Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "#read the model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 4), stride=(1, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(48576, 128)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        #print('x:', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1:', x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2:', x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print('flatten:', x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = EEGNet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T00:11:00.189175Z",
     "end_time": "2023-05-02T00:11:00.298178Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滑窗后信号形状： (33, 3000, 126)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sliding_window(signal, window_size, step_size):\n",
    "    n_channels, n_samples = signal.shape\n",
    "    n_windows = int((n_samples - window_size) / step_size) + 1\n",
    "    windows = np.zeros((n_channels, window_size, n_windows))\n",
    "    for i in range(n_windows):\n",
    "        windows[:, :,i ] = signal.iloc[:, i*step_size:i*step_size+window_size]\n",
    "    return windows\n",
    "\n",
    "signal = pd.read_csv(\"C:\\\\Users\\\\a1882\\Desktop\\EEG\\eegdata\\\\raw\\lefthand_zyy_05_epocflex_2023.03.22t16.50.54+08.00.md.bp.csv\", header=None)\n",
    "signal = pd.DataFrame(signal)\n",
    "# 定义滑窗大小和滑动步长\n",
    "window_size = 3000\n",
    "step_size = 100\n",
    "\n",
    "# 对信号进行滑窗处理\n",
    "windows = sliding_window(signal, window_size, step_size)\n",
    "\n",
    "# 输出滑窗后的信号形状\n",
    "print(\"滑窗后信号形状：\", windows.shape)\n",
    "#print(windows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T00:11:22.048811Z",
     "end_time": "2023-05-02T00:11:23.992043Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('conv1.weight',\n              tensor([[[[-0.3445,  0.3977, -0.2108, -0.3395]]],\n              \n              \n                      [[[ 0.0366,  0.3111, -0.0930, -0.2412]]],\n              \n              \n                      [[[-0.4966, -0.4928,  0.2660, -0.0154]]],\n              \n              \n                      [[[-0.4080,  0.4452, -0.2056, -0.0517]]],\n              \n              \n                      [[[-0.3918, -0.3286, -0.3431, -0.3232]]],\n              \n              \n                      [[[ 0.0481,  0.0398,  0.3754,  0.2612]]],\n              \n              \n                      [[[-0.3075, -0.3402,  0.4785,  0.1673]]],\n              \n              \n                      [[[ 0.3987,  0.2521,  0.4194, -0.1689]]],\n              \n              \n                      [[[-0.2991, -0.3978, -0.2616, -0.3489]]],\n              \n              \n                      [[[-0.4447,  0.2200, -0.4190,  0.0629]]],\n              \n              \n                      [[[ 0.3205, -0.1167, -0.3645,  0.1676]]],\n              \n              \n                      [[[ 0.4178,  0.0815,  0.1320, -0.1873]]],\n              \n              \n                      [[[ 0.4640,  0.4396,  0.5020,  0.2814]]],\n              \n              \n                      [[[-0.1530, -0.0619, -0.1066,  0.0536]]],\n              \n              \n                      [[[-0.2948,  0.4221, -0.4089, -0.1736]]],\n              \n              \n                      [[[-0.3394,  0.1838, -0.1874,  0.1290]]]])),\n             ('conv1.bias',\n              tensor([-0.4293, -0.1886,  0.0158,  0.1430,  0.0787, -0.2679,  0.4434,  0.2869,\n                      -0.4275,  0.1197, -0.0036, -0.4273, -0.0293, -0.0159, -0.0556,  0.3882])),\n             ('bn1.weight',\n              tensor([1.0747, 0.9712, 1.0618, 1.0516, 1.0705, 0.8818, 1.0285, 0.8804, 1.0731,\n                      1.1236, 0.9356, 0.9086, 0.9299, 1.0606, 1.0714, 1.0720])),\n             ('bn1.bias',\n              tensor([ 0.0422,  0.0486,  0.0381,  0.0552,  0.0019,  0.1157, -0.1340,  0.0208,\n                       0.0323, -0.0614, -0.0413,  0.0898,  0.1193,  0.0691,  0.0150,  0.0758])),\n             ('bn1.running_mean',\n              tensor([-1.3460e+02,  3.4673e+00, -1.9949e+02, -5.9196e+01, -3.7466e+02,\n                       1.9553e+02,  9.3960e-02,  2.4378e+02, -3.5372e+02, -1.5678e+02,\n                       2.0016e+00,  1.1960e+02,  4.5579e+02, -7.2349e+01, -1.2304e+02,\n                      -5.7290e+01])),\n             ('bn1.running_var',\n              tensor([1.4723e+06, 1.1512e+03, 3.2567e+06, 2.8802e+05, 1.1487e+07, 3.1359e+06,\n                      1.6158e+02, 4.8506e+06, 1.0210e+07, 2.0141e+06, 3.8899e+02, 1.1791e+06,\n                      1.6996e+07, 4.2805e+05, 1.2373e+06, 2.7215e+05])),\n             ('bn1.num_batches_tracked', tensor(16039)),\n             ('conv2.weight',\n              tensor([[[[-1.3297e-01, -2.0615e-01]],\n              \n                       [[ 7.7670e-02, -4.5860e-02]],\n              \n                       [[-6.2534e-02, -2.6194e-01]],\n              \n                       ...,\n              \n                       [[-2.6472e-01, -2.4736e-01]],\n              \n                       [[-3.7818e-02, -2.5836e-01]],\n              \n                       [[-6.0791e-02, -2.6001e-01]]],\n              \n              \n                      [[[-3.7889e-02,  1.2841e-01]],\n              \n                       [[ 8.7450e-02,  1.4704e-01]],\n              \n                       [[ 1.0182e-01,  1.1529e-01]],\n              \n                       ...,\n              \n                       [[ 4.4186e-02,  1.2193e-01]],\n              \n                       [[-7.1762e-02, -1.5125e-01]],\n              \n                       [[-3.2732e-02, -6.8410e-02]]],\n              \n              \n                      [[[-1.3042e-01,  2.9286e-02]],\n              \n                       [[-1.1250e-01, -4.6541e-02]],\n              \n                       [[-2.2986e-01,  3.2092e-02]],\n              \n                       ...,\n              \n                       [[-1.8468e-01, -1.9537e-01]],\n              \n                       [[-1.9822e-02, -2.1718e-01]],\n              \n                       [[-2.0402e-01, -1.6545e-01]]],\n              \n              \n                      ...,\n              \n              \n                      [[[-1.8735e-01,  1.4110e-04]],\n              \n                       [[ 5.4615e-02,  1.8933e-01]],\n              \n                       [[-1.4112e-01, -1.4405e-01]],\n              \n                       ...,\n              \n                       [[ 4.5142e-02, -5.0185e-02]],\n              \n                       [[ 1.1918e-01, -1.5377e-01]],\n              \n                       [[-1.3598e-01, -8.8258e-02]]],\n              \n              \n                      [[[ 1.2752e-01, -1.0004e-01]],\n              \n                       [[-1.3847e-02, -1.8681e-01]],\n              \n                       [[ 1.3440e-01,  3.0978e-02]],\n              \n                       ...,\n              \n                       [[ 1.1569e-02,  1.4067e-01]],\n              \n                       [[-1.2526e-01, -1.4674e-03]],\n              \n                       [[-9.3585e-02,  7.0538e-02]]],\n              \n              \n                      [[[ 2.8213e-01,  1.9450e-01]],\n              \n                       [[-5.7882e-02, -8.1142e-03]],\n              \n                       [[ 2.7305e-01,  3.4091e-02]],\n              \n                       ...,\n              \n                       [[ 1.0127e-03,  2.9107e-01]],\n              \n                       [[ 2.6130e-01,  2.8918e-01]],\n              \n                       [[ 2.1825e-01,  9.8583e-02]]]])),\n             ('conv2.bias',\n              tensor([ 0.0746,  0.0415, -0.1328, -0.0480,  0.0966, -0.0312,  0.1123, -0.1557,\n                       0.1452, -0.0014, -0.0733,  0.1486, -0.0692,  0.0344, -0.1172, -0.0973,\n                      -0.0791, -0.0006,  0.1521, -0.1171,  0.1369, -0.1434,  0.0810,  0.1048,\n                      -0.0129, -0.1843,  0.0669, -0.0331, -0.0618, -0.1147, -0.0688, -0.1376])),\n             ('bn2.weight',\n              tensor([1.0416, 1.0035, 0.9490, 1.0837, 1.1140, 1.0495, 0.9603, 0.9199, 0.9931,\n                      1.0069, 1.0250, 0.8913, 1.0483, 0.9801, 0.9607, 1.1264, 1.0618, 1.0218,\n                      0.9729, 1.0375, 1.0257, 1.0106, 1.0672, 1.0217, 1.0791, 1.0986, 1.0329,\n                      1.0627, 1.0729, 1.0065, 0.9326, 1.1151])),\n             ('bn2.bias',\n              tensor([-0.0395, -0.0193, -0.0560, -0.0452, -0.0237, -0.0824, -0.0558, -0.0762,\n                      -0.0223, -0.0280, -0.0540, -0.0587, -0.0053, -0.0494, -0.0323,  0.0100,\n                      -0.0657, -0.0488, -0.0942, -0.0385,  0.0089, -0.0554, -0.0044, -0.0730,\n                      -0.0462, -0.0195, -0.0157, -0.0632, -0.0708, -0.0523, -0.0585, -0.0486])),\n             ('bn2.running_mean',\n              tensor([-0.7649,  0.1747, -0.8693,  0.6765,  0.9372, -0.0337, -0.1514, -0.3447,\n                       0.1992,  0.2826,  0.3529,  0.2143,  0.0388, -0.0184,  0.1018,  0.3588,\n                       0.4956,  0.4607,  0.2178, -0.5605,  0.1648,  0.0675,  0.4253, -0.3800,\n                       0.0080, -0.1143,  0.2525,  0.5947, -0.1450, -0.2307, -0.3375,  0.6318])),\n             ('bn2.running_var',\n              tensor([0.7761, 0.2007, 0.4685, 0.5199, 0.7432, 0.1515, 0.8076, 1.0422, 0.6846,\n                      0.2687, 0.3679, 0.3145, 0.0913, 0.7017, 0.2423, 0.4217, 0.4772, 0.3154,\n                      0.3258, 0.3324, 0.1092, 0.3580, 0.2319, 0.3923, 0.1084, 0.1405, 0.2460,\n                      0.4470, 0.1526, 0.6205, 0.6220, 0.7909])),\n             ('bn2.num_batches_tracked', tensor(16039)),\n             ('fc1.weight',\n              tensor([[ 0.0009,  0.0004,  0.0051,  ...,  0.0023,  0.0003, -0.0029],\n                      [-0.0049, -0.0021, -0.0029,  ...,  0.0040, -0.0010, -0.0020],\n                      [-0.0031,  0.0010,  0.0006,  ..., -0.0028, -0.0038, -0.0032],\n                      ...,\n                      [-0.0062, -0.0003, -0.0066,  ..., -0.0027, -0.0041, -0.0021],\n                      [ 0.0016, -0.0031,  0.0003,  ...,  0.0037, -0.0017, -0.0037],\n                      [-0.0037, -0.0039, -0.0045,  ..., -0.0023, -0.0033, -0.0019]])),\n             ('fc1.bias',\n              tensor([-2.7687e-03, -3.1381e-03, -4.6458e-03,  1.1612e-03,  1.3618e-04,\n                       2.3525e-03, -4.1949e-03, -2.8830e-03, -2.1554e-03, -1.2383e-03,\n                      -4.6691e-03, -3.7157e-03,  2.7304e-03,  3.1503e-03,  2.1751e-03,\n                       3.9531e-04, -2.8074e-03, -1.0308e-03, -3.4847e-03, -3.7586e-03,\n                      -4.3677e-03,  2.3245e-03,  1.2320e-03, -3.6581e-03, -1.8703e-04,\n                      -5.2858e-03, -7.9369e-04,  4.6912e-04, -8.7936e-04, -5.6380e-05,\n                      -9.3180e-03, -3.1767e-03, -2.9120e-03,  5.0072e-03,  5.1659e-04,\n                       4.7076e-03,  1.3475e-02, -4.7000e-03,  2.0672e-03, -7.5230e-04,\n                       2.8737e-03, -1.7439e-03,  3.4437e-03, -1.5416e-03, -3.6414e-03,\n                      -2.8602e-03,  3.3709e-03, -4.5874e-03, -1.4465e-03, -1.7929e-03,\n                      -4.7855e-03, -3.8297e-03,  1.9735e-03, -2.8799e-03, -1.2722e-02,\n                       1.2570e-03,  1.5551e-03, -2.1013e-03,  1.8886e-03, -1.5459e-03,\n                      -3.1587e-03,  1.1889e-03,  1.9772e-03,  5.8622e-03, -3.2617e-03,\n                       3.0351e-03,  3.8433e-03,  2.7565e-03,  2.7562e-03, -1.0137e-03,\n                       3.4638e-03, -5.2008e-05, -1.6346e-03,  3.6962e-03,  3.4333e-03,\n                      -3.7521e-03, -2.2177e-03, -2.0661e-03, -1.3140e-02, -4.0425e-03,\n                       2.4999e-03,  1.5499e-03, -1.2767e-03, -2.0662e-03, -1.5782e-03,\n                      -5.3329e-03,  6.8764e-03,  2.4416e-03,  1.4311e-03,  3.0010e-03,\n                      -2.7280e-03, -1.8646e-03,  5.8091e-03, -1.6663e-03, -4.2120e-03,\n                       2.5915e-04,  3.6396e-03, -4.2775e-03,  3.4649e-03,  3.4693e-03,\n                      -2.7036e-03,  1.2307e-02,  4.2718e-04,  2.7013e-03, -1.3620e-03,\n                      -9.0063e-03,  2.4038e-03, -8.2405e-03, -2.9397e-05,  1.1413e-02,\n                      -1.0650e-02,  9.4262e-03, -3.9720e-03,  1.4597e-03, -4.7913e-03,\n                      -4.2749e-03, -1.8094e-03, -3.0397e-03,  3.5185e-03, -5.0957e-03,\n                       6.0837e-03, -3.8471e-03,  8.9497e-03, -1.4185e-03,  3.6984e-03,\n                      -2.4540e-03, -3.0226e-03, -1.4026e-03])),\n             ('fc2.weight',\n              tensor([[ 7.4556e-02,  2.3091e-02,  4.4695e-02, -7.3390e-02, -5.5128e-02,\n                        4.1631e-02, -1.6062e-02, -9.5548e-02, -3.8385e-03,  5.8878e-02,\n                        1.0031e-02,  7.1226e-02,  1.7422e-02,  5.5495e-02,  7.3837e-02,\n                       -8.4362e-02,  3.6740e-02, -2.4433e-02, -7.9351e-02,  1.9952e-02,\n                        6.4485e-02, -2.4169e-02, -7.9951e-02,  5.0657e-02, -6.6620e-02,\n                       -3.2794e-02, -7.3375e-02,  4.8963e-02, -1.3454e-02, -7.1064e-03,\n                       -1.1329e-03, -2.4651e-02,  3.9223e-02,  3.9586e-02, -8.2980e-02,\n                       -2.0390e-02,  5.5769e-02, -5.6584e-02,  2.3951e-03,  3.2021e-02,\n                       -6.3206e-02,  4.8070e-02,  5.7405e-03, -6.4670e-02,  6.8012e-02,\n                       -4.1391e-02,  2.9685e-02,  7.2795e-02,  7.4572e-02, -5.0698e-02,\n                       -9.7731e-06,  4.7749e-02,  7.5230e-02,  5.6671e-02, -1.1754e-01,\n                        7.9645e-02, -1.9990e-02, -5.1052e-02, -6.7469e-02, -8.1563e-02,\n                        8.2455e-02,  9.5824e-03,  6.3814e-02,  4.9377e-02, -6.0101e-02,\n                        4.0961e-02, -3.3123e-02,  4.6260e-02, -6.4020e-02,  8.7412e-02,\n                       -1.1325e-02, -5.8014e-02, -1.1167e-02,  2.5717e-02, -7.1550e-02,\n                       -9.2823e-02,  8.2875e-02,  8.3058e-02, -8.4451e-02, -6.9334e-02,\n                        1.3785e-02, -1.7582e-02, -3.8949e-02,  4.9714e-02,  7.0276e-02,\n                        6.2393e-03,  6.7476e-02,  4.2811e-02, -8.1011e-02,  1.9541e-03,\n                        3.0615e-02, -2.8460e-02,  2.8947e-03, -3.8493e-02,  2.7472e-02,\n                        4.0035e-02, -5.7299e-03,  1.4968e-02, -4.2733e-03, -8.4671e-02,\n                       -4.4071e-02, -4.4074e-02, -4.9131e-02,  4.0543e-02, -2.8007e-02,\n                       -6.5923e-02,  5.5418e-03,  2.3856e-02, -8.3431e-02,  5.2780e-02,\n                       -1.0603e-01,  6.5724e-02, -4.6025e-02, -5.3993e-02,  1.1991e-02,\n                       -3.9651e-02, -5.9008e-02,  5.1173e-02,  8.3267e-02,  6.6422e-02,\n                       -1.4877e-01,  4.3507e-02, -6.5271e-02,  3.0446e-02,  2.2097e-02,\n                        2.5142e-02, -6.6733e-02,  3.8238e-03],\n                      [ 2.9446e-02,  2.0458e-02, -7.0086e-02, -4.8116e-02, -8.8473e-02,\n                        4.5995e-02, -1.4609e-02,  5.1146e-02, -4.1304e-02,  7.6171e-02,\n                        3.4290e-02, -2.1452e-02, -2.2312e-02, -6.8773e-03, -3.3876e-02,\n                        2.0907e-02,  6.5629e-02,  1.3501e-02,  3.1882e-04,  6.1680e-02,\n                        7.3950e-03, -8.2278e-02, -5.6981e-02,  1.7477e-02,  1.7168e-02,\n                        6.9954e-02, -8.6690e-02, -7.8566e-02, -6.1036e-02, -5.4656e-02,\n                       -3.6111e-02,  3.6667e-02, -6.1863e-02,  1.1146e-01, -4.0550e-02,\n                        1.3065e-01,  6.9406e-02, -7.4986e-02,  6.9326e-02, -6.1214e-02,\n                        5.7658e-02, -3.7619e-02,  9.1678e-03, -5.0284e-02,  8.8289e-03,\n                        1.5158e-02,  3.9899e-02, -1.1156e-01, -3.7333e-02, -4.5482e-02,\n                       -6.0247e-02,  3.5285e-02, -1.1653e-01,  3.1985e-02, -1.8409e-02,\n                       -3.5634e-02, -1.1038e-02,  3.9665e-02,  4.3137e-03, -7.7399e-03,\n                        2.3091e-02, -3.1247e-02, -3.9387e-03, -1.4437e-01,  6.9221e-02,\n                        2.0587e-02,  4.3889e-02, -1.5660e-02,  4.8522e-02, -4.9420e-02,\n                        9.5016e-03, -8.9094e-02, -3.7594e-02, -3.6708e-02, -6.9305e-02,\n                       -1.0622e-01,  3.8312e-02,  7.1597e-02, -1.5179e-03, -6.1344e-02,\n                        1.7628e-02, -1.3867e-01,  1.0136e-01, -1.8355e-02,  7.8621e-02,\n                        5.2192e-02, -3.9618e-03, -4.3147e-03,  5.0995e-03, -2.9598e-02,\n                       -5.2455e-02,  4.2480e-02,  9.6323e-02,  6.8360e-02, -6.9063e-02,\n                       -4.4717e-02, -1.5793e-03,  2.7006e-02, -8.1448e-02,  8.1718e-02,\n                        6.4855e-02,  3.6464e-02, -7.1219e-02,  4.2006e-02,  7.4402e-02,\n                        1.0881e-02,  6.6574e-02,  2.3187e-02,  6.2613e-02,  5.2795e-02,\n                       -3.3326e-02,  5.1660e-02,  3.7439e-02, -5.0854e-02,  7.3165e-02,\n                       -5.5054e-02,  5.3339e-02,  2.0722e-02, -1.2370e-02,  8.0793e-02,\n                       -6.9208e-02,  7.4010e-02,  6.3503e-02,  5.2942e-02,  5.1838e-02,\n                        4.6773e-02,  3.1997e-02,  1.0635e-02],\n                      [ 8.0198e-02, -6.2939e-02, -6.0539e-02,  7.5650e-02,  6.2573e-02,\n                       -5.9894e-02, -4.2415e-03, -5.0318e-02, -2.1059e-02, -8.3703e-02,\n                        3.9621e-02,  1.5505e-02,  7.8572e-02, -6.2421e-02, -6.5552e-02,\n                       -8.6019e-02, -4.3384e-02, -5.7755e-02, -7.7532e-02, -9.4007e-02,\n                        7.2258e-02,  9.2387e-02,  5.7128e-02, -5.4526e-02, -8.2155e-02,\n                       -6.6896e-02,  2.1656e-02, -8.2096e-02, -1.6779e-02, -2.1122e-02,\n                        7.9020e-02,  1.0913e-03, -5.7093e-02,  6.7153e-02,  2.2717e-02,\n                       -5.8561e-02,  6.7147e-02, -5.3465e-02, -7.0902e-02,  6.2643e-02,\n                        6.3935e-02, -2.1023e-02,  8.0423e-02,  3.8812e-02, -2.1724e-02,\n                        3.8519e-02,  6.6116e-02,  3.9667e-02, -5.5008e-03, -7.2993e-02,\n                        8.3962e-03,  5.0735e-02,  6.5017e-02, -4.1836e-02, -1.3401e-01,\n                        7.0230e-02,  6.0242e-02,  4.3943e-02,  7.6940e-02, -8.5929e-02,\n                       -1.9466e-02, -3.6613e-02, -6.4871e-02,  4.2491e-02, -4.3640e-02,\n                        2.9642e-02,  2.8965e-02,  4.4398e-02,  9.7604e-03,  6.8086e-02,\n                       -1.2202e-02,  4.1100e-02,  8.3812e-02, -6.4309e-02, -5.8792e-02,\n                        4.0780e-03,  2.8553e-02,  4.7738e-02, -8.5271e-02,  5.7203e-02,\n                        5.8777e-02,  4.3489e-02, -6.8639e-02, -6.8409e-02,  8.0449e-02,\n                        7.9274e-02,  7.1374e-02,  7.6931e-02, -6.5267e-02,  5.1688e-02,\n                        5.2363e-02, -5.7635e-02,  8.4456e-02, -8.7445e-02,  1.4226e-02,\n                        8.1363e-02, -2.4312e-02,  4.9786e-02, -3.8652e-03,  9.0955e-03,\n                       -7.3644e-02,  3.7994e-02,  6.7451e-02,  2.3352e-02, -2.1668e-02,\n                       -7.6835e-02, -6.9488e-03, -6.5232e-02,  7.3588e-02,  5.6264e-02,\n                       -9.0161e-02,  6.1635e-02, -3.2098e-02,  3.2241e-02,  5.7156e-02,\n                       -1.2457e-02,  2.0819e-02,  7.6287e-02,  7.0738e-03,  4.6594e-02,\n                        5.4566e-02, -5.2124e-02,  6.5010e-02,  1.9464e-02,  4.2701e-02,\n                        5.3710e-02,  7.7145e-02,  8.3371e-02],\n                      [ 2.1903e-03, -5.9905e-02, -2.4202e-02,  2.7603e-02, -1.5036e-02,\n                       -6.5786e-02, -3.1092e-02, -2.6005e-02, -6.8239e-02, -3.2918e-02,\n                        6.2694e-02,  8.3258e-03, -2.1316e-02, -8.9369e-02, -4.3597e-03,\n                       -1.0428e-01, -4.4534e-02, -9.9962e-03, -7.6931e-02, -8.6196e-02,\n                       -4.8152e-02, -1.1435e-01, -3.2905e-02, -3.1213e-03, -3.9994e-02,\n                        5.9368e-02, -5.9147e-02, -1.2185e-02, -9.0914e-03,  4.9445e-02,\n                        2.8090e-02, -2.0728e-02,  8.5496e-02,  4.0485e-02,  6.9377e-03,\n                       -5.8273e-02, -4.7650e-02,  2.8592e-02,  5.2224e-02,  5.0280e-02,\n                       -5.1977e-03,  3.1171e-02,  8.1083e-02, -5.8413e-02,  7.8675e-02,\n                       -4.8443e-02, -7.2200e-02,  7.6076e-02,  7.9286e-02,  7.3111e-02,\n                       -8.3678e-02,  1.4100e-02,  7.9894e-02, -3.1692e-02,  4.0386e-02,\n                       -5.7507e-03,  6.6128e-02,  5.7429e-02, -3.9798e-02, -3.8856e-02,\n                       -5.2564e-02, -3.4121e-02,  2.6366e-03, -9.6599e-02,  8.6609e-03,\n                       -2.9815e-02, -8.6938e-02,  5.8440e-02,  1.0766e-02, -1.9211e-02,\n                       -2.5068e-02, -4.5040e-03, -2.0888e-02, -4.6339e-02, -3.5810e-02,\n                        3.6545e-02,  2.8521e-02, -3.5722e-02,  8.2582e-02,  8.8510e-02,\n                       -4.4994e-02,  7.3924e-03,  4.2838e-02,  1.7562e-03,  4.1577e-02,\n                        4.0513e-02, -1.2447e-01, -1.5602e-02, -5.1329e-04, -1.2884e-02,\n                        3.0635e-02,  2.6429e-02, -1.8155e-03,  6.0758e-02,  8.6912e-02,\n                        5.2278e-02,  5.0583e-02, -1.0992e-03,  7.6456e-02,  7.4151e-02,\n                        3.6179e-02, -3.2999e-02, -7.4720e-02, -1.5983e-02,  8.5126e-03,\n                        6.5120e-02, -3.5608e-02, -7.9299e-02, -5.6967e-02, -9.0062e-02,\n                        9.5501e-02, -4.7871e-02,  2.8571e-04,  2.5266e-02, -6.0726e-03,\n                        5.3867e-02,  8.4602e-02,  4.6507e-02, -5.0969e-02, -3.5595e-02,\n                        1.0178e-01,  2.4314e-02, -8.7629e-02, -5.5257e-03,  3.4306e-02,\n                        1.8132e-02, -5.2119e-02,  6.8220e-02],\n                      [-7.5051e-02, -5.3931e-02, -7.1543e-02, -7.6843e-02, -7.8521e-02,\n                        3.5836e-02,  6.6601e-02,  8.2110e-02, -8.0138e-02, -7.0569e-02,\n                        8.4046e-02, -3.4035e-02, -5.1264e-02,  4.7119e-02, -7.1516e-02,\n                        1.6811e-02, -7.4307e-02,  8.0199e-02, -6.6650e-02,  6.7071e-02,\n                        3.3752e-02,  5.5925e-02, -6.7635e-02,  4.1255e-02, -7.1361e-02,\n                        5.5255e-02,  4.9213e-02, -2.7074e-02,  2.3687e-02, -2.6907e-02,\n                        1.0549e-01,  6.5228e-02,  7.4195e-03, -7.7934e-02,  6.4668e-02,\n                       -5.2712e-02, -1.0448e-01,  4.0420e-02, -4.9665e-02,  4.5121e-02,\n                        3.2080e-02,  8.7307e-02, -3.1064e-02, -2.7712e-02, -3.0424e-02,\n                       -3.8530e-02,  5.0057e-02,  7.7392e-02, -1.4183e-02, -7.9364e-02,\n                        8.2206e-02,  2.8761e-02,  7.4472e-02, -8.8144e-03,  1.7472e-03,\n                        8.0811e-02,  7.8487e-02, -7.2243e-02,  1.6715e-03,  6.7072e-02,\n                       -1.7328e-02,  5.0478e-03,  7.9293e-02,  1.7186e-02, -6.4752e-02,\n                        1.6668e-02, -8.4417e-02, -1.9197e-02,  8.5187e-03, -1.1185e-02,\n                       -5.3000e-02,  4.5574e-02, -1.4770e-02, -6.8877e-02, -7.9592e-02,\n                       -7.0821e-02, -4.3762e-02, -4.4352e-02,  7.9421e-02,  8.8053e-02,\n                       -7.2330e-02,  7.5394e-02, -6.1340e-02,  5.7386e-03,  1.3816e-02,\n                       -6.7355e-02, -1.3763e-02, -5.2103e-02, -5.2508e-02,  2.2905e-02,\n                        6.4331e-02, -7.5050e-02,  2.1718e-02,  8.3258e-02, -5.1435e-02,\n                       -1.8324e-02, -1.3326e-02, -6.1936e-02, -2.8363e-02,  4.7817e-02,\n                       -1.0854e-02, -1.1113e-01,  8.6365e-02, -4.5142e-02,  4.9274e-02,\n                        5.4353e-02, -7.6394e-02,  3.2777e-02, -3.7961e-02,  1.4688e-02,\n                       -4.2713e-02, -7.8288e-02, -7.7680e-03, -4.0897e-02,  1.2983e-02,\n                       -4.6475e-02, -5.1902e-02,  7.8641e-02, -8.4168e-02,  8.0748e-02,\n                        1.6923e-02,  3.6387e-02, -1.3485e-01, -7.0202e-03,  7.4229e-02,\n                        3.8928e-02, -3.3200e-02,  2.0127e-02],\n                      [-7.1491e-02,  2.4652e-02, -5.9066e-02, -5.8370e-02,  2.5680e-02,\n                       -3.3888e-02,  2.5456e-02,  8.7219e-02,  8.0517e-02, -3.8090e-02,\n                       -1.8333e-02,  1.3507e-02, -4.0818e-03, -5.2657e-02, -8.1990e-02,\n                        3.1394e-02,  5.6539e-02,  1.3375e-02,  2.5485e-02, -1.4977e-01,\n                        6.2677e-02,  9.6562e-02,  3.5933e-02,  2.8600e-02,  8.6106e-02,\n                        7.6109e-02, -8.1644e-02,  1.9668e-02,  4.4792e-02,  1.7497e-02,\n                       -9.8505e-02, -3.0706e-02, -5.6208e-02, -4.6257e-02, -6.5961e-02,\n                       -7.6724e-02, -7.7386e-02, -7.0969e-02, -2.7728e-02, -6.8862e-02,\n                        7.5157e-02,  8.4754e-02,  2.2481e-03,  8.2259e-02, -5.3275e-02,\n                        7.2971e-02, -1.7378e-02, -6.2299e-02,  5.8245e-02,  7.6088e-02,\n                        1.6281e-02,  1.8058e-02, -3.5404e-02, -2.1662e-02,  2.8074e-02,\n                       -1.4011e-02,  2.0627e-02,  3.3459e-02, -5.3094e-02, -7.7851e-02,\n                       -2.6428e-02,  2.5800e-02, -2.9021e-03,  4.3474e-02, -1.4804e-02,\n                        1.4121e-02,  5.1352e-02, -1.5394e-02,  8.3287e-02, -8.8697e-02,\n                       -1.9452e-02, -1.4571e-02,  1.6474e-02, -7.0437e-02,  3.1121e-03,\n                        2.4926e-02, -7.2090e-02,  7.0637e-02, -2.9170e-03, -6.6143e-02,\n                        1.8784e-02,  7.5067e-02, -2.7964e-02, -4.6574e-02, -8.7509e-02,\n                        1.6290e-03,  6.5423e-02, -1.4452e-03,  6.0974e-02,  7.1708e-02,\n                       -8.0780e-03,  4.2287e-02, -6.8569e-02, -6.7749e-02,  3.3935e-03,\n                       -1.6220e-03,  4.9317e-02, -6.3874e-02, -4.9646e-03, -2.3333e-02,\n                        5.8817e-02, -1.5176e-01,  6.4815e-03,  2.2270e-02, -3.7840e-02,\n                       -7.6514e-02, -6.2991e-03,  4.0902e-02, -4.6047e-02, -1.0384e-01,\n                        8.8163e-02, -4.7206e-02, -6.4865e-02, -8.4812e-02, -9.0859e-03,\n                       -3.8679e-02, -9.1434e-03, -3.2786e-02,  4.7790e-02,  7.1885e-02,\n                       -1.2525e-03, -1.4029e-04, -1.3225e-01, -2.7828e-02,  3.0781e-03,\n                        3.4910e-02, -6.8162e-02, -6.0463e-03]])),\n             ('fc2.bias',\n              tensor([ 0.0990, -0.1372, -0.0399,  0.1097, -0.0944,  0.0023]))])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "save_path = './cnn_3000_30e_26.pt'\n",
    "\n",
    "# onnx_file_name = \"./cnn_3000_100e_100.onnx\"\n",
    "torch.load(save_path, map_location=torch.device('cpu'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T00:11:23.996046Z",
     "end_time": "2023-05-02T00:11:24.121054Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is valid!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "# 我们可以使用异常处理的方法进行检验\n",
    "try:\n",
    "    # 当我们的模型不可用时，将会报出异常\n",
    "    onnx.checker.check_model(onnx_file_name)\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(\"The model is invalid: %s\"%e)\n",
    "else:\n",
    "    # 模型可用时，将不会报出异常，并会输出“The model is valid!”\n",
    "    print(\"The model is valid!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T00:11:25.440519Z",
     "end_time": "2023-05-02T00:11:26.175792Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "rest\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n"
     ]
    }
   ],
   "source": [
    "model = EEGNet()\n",
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "for i in range(windows.shape[2]):\n",
    "    signal = pd.DataFrame(windows[:,:,i])\n",
    "    input = torch.from_numpy(signal.values)\n",
    "    input = input.unsqueeze(0)\n",
    "    output = model(input.float())\n",
    "\n",
    "    # # print(output)\n",
    "    # probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    # print(probabilities)\n",
    "\n",
    "    _, pred = torch.max(output, dim=1)  # 找到预测分数最大的类别，得到预测类别\n",
    "    label_map = {0: 'lefthand', 1:'read' ,  2:'rest', 3: 'walkbase', 4: 'walkl' ,5: 'walkfocus'}\n",
    "    print(label_map[pred.item()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T00:11:27.308969Z",
     "end_time": "2023-05-02T00:11:29.647109Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "    #print(output)\n",
    "    # probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    # print(probabilities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T21:55:51.786321Z",
     "end_time": "2023-04-24T21:55:51.807323Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n",
      "lefthand\n"
     ]
    }
   ],
   "source": [
    "model = EEGNet()\n",
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "probabilities = []\n",
    "for i in range(windows.shape[2]):\n",
    "    signal = pd.DataFrame(windows[:,:,i])\n",
    "    input = torch.from_numpy(signal.values)\n",
    "    input = input.unsqueeze(0)\n",
    "    output = model(input.float())\n",
    "\n",
    "    # # print(output)\n",
    "    #probabilitie = torch.nn.functional.softmax(output, dim=1)\n",
    "    # print(probabilities)\n",
    "    #probabilities.append(probabilitie)\n",
    "    probabilities.append(output)\n",
    "    if len(probabilities) == 5:\n",
    "        average_predict = sum(probabilities)/5\n",
    "        _, pred = torch.max(average_predict, dim=1)  # 找到预测分数最大的类别，得到预测类别\n",
    "        label_map = {0: 'lefthand', 1:'read' ,  2:'rest', 3: 'walkbase', 4: 'walkl' ,5: 'walkfocus'}\n",
    "        print(label_map[pred.item()])\n",
    "        probabilities = []\n",
    "    #\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-02T10:56:31.658492Z",
     "end_time": "2023-05-02T10:56:35.001422Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
