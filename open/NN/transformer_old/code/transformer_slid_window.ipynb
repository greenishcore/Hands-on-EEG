{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:38:35.656888Z",
     "end_time": "2023-04-24T14:38:46.877935Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # 取平均作为输出\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:38:46.886938Z",
     "end_time": "2023-04-24T14:38:46.984667Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "d_model = 32  # Transformer模型中特征的维度\n",
    "nhead = 4  # 多头自注意力头数\n",
    "num_layers = 4  # Transformer编码器层数\n",
    "num_classes = 6  # 分类类别数"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:38:46.977160Z",
     "end_time": "2023-04-24T14:38:47.000528Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滑窗后信号形状： (32, 1000, 146)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sliding_window(signal, window_size, step_size):\n",
    "    n_channels, n_samples = signal.shape\n",
    "    n_windows = int((n_samples - window_size) / step_size) + 1\n",
    "    windows = np.zeros((n_channels, window_size, n_windows))\n",
    "    for i in range(n_windows):\n",
    "        windows[:, :,i ] = signal.iloc[:, i*step_size:i*step_size+window_size]\n",
    "    return windows\n",
    "\n",
    "signal = pd.read_csv(\"C:\\\\Users\\\\a1882\\Desktop\\EEG\\eegdata\\\\raw\\\\lefthand_zyy_05_epocflex_2023.03.22t16.50.54+08.00.md.bp.csv\")\n",
    "signal = pd.DataFrame(signal)\n",
    "# 定义滑窗大小和滑动步长\n",
    "window_size = 1000\n",
    "step_size = 100\n",
    "\n",
    "# 对信号进行滑窗处理\n",
    "windows = sliding_window(signal, window_size, step_size)\n",
    "\n",
    "# 输出滑窗后的信号形状\n",
    "print(\"滑窗后信号形状：\", windows.shape)\n",
    "#print(windows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:19:50.529952Z",
     "end_time": "2023-04-24T18:19:52.238137Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "OrderedDict([('encoder_layer.self_attn.in_proj_weight',\n              tensor([[-0.0299,  0.0612, -0.1074,  ...,  0.1874, -0.1438,  0.1018],\n                      [-0.2022,  0.1797, -0.0896,  ...,  0.0619,  0.1461, -0.0290],\n                      [ 0.1697,  0.1872, -0.1305,  ...,  0.0373, -0.1238,  0.0025],\n                      ...,\n                      [ 0.0434, -0.0207, -0.1276,  ..., -0.2075, -0.1074,  0.0820],\n                      [-0.0797,  0.0602, -0.1537,  ..., -0.1581, -0.0481, -0.1953],\n                      [-0.1915,  0.0174,  0.1310,  ...,  0.0938, -0.1755, -0.0699]])),\n             ('encoder_layer.self_attn.in_proj_bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('encoder_layer.self_attn.out_proj.weight',\n              tensor([[ 0.0442,  0.0661, -0.0040,  ...,  0.1579,  0.1679, -0.1565],\n                      [-0.0869,  0.0993, -0.0980,  ..., -0.1229,  0.0861, -0.0749],\n                      [ 0.0053,  0.0901,  0.0523,  ..., -0.1508,  0.1694,  0.1212],\n                      ...,\n                      [-0.0729,  0.1392,  0.0677,  ..., -0.1376,  0.1651, -0.0069],\n                      [ 0.1157,  0.0458, -0.0585,  ...,  0.0957, -0.0399,  0.1544],\n                      [ 0.1302, -0.1556, -0.0507,  ..., -0.0380,  0.0024, -0.0520]])),\n             ('encoder_layer.self_attn.out_proj.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('encoder_layer.linear1.weight',\n              tensor([[-0.1237,  0.0499, -0.1630,  ..., -0.1528,  0.0133, -0.0794],\n                      [ 0.1681,  0.1307, -0.1386,  ...,  0.0991, -0.0515,  0.0485],\n                      [-0.1156,  0.0164, -0.1597,  ...,  0.0617, -0.0082,  0.1531],\n                      ...,\n                      [ 0.0532, -0.1464,  0.0787,  ...,  0.1167, -0.0852, -0.0752],\n                      [ 0.0141,  0.1348, -0.0979,  ...,  0.0422, -0.0998,  0.0778],\n                      [-0.0122, -0.0044,  0.0580,  ...,  0.0262, -0.1363,  0.0082]])),\n             ('encoder_layer.linear1.bias',\n              tensor([ 0.0215, -0.1281, -0.0230,  ...,  0.1088,  0.1021,  0.0992])),\n             ('encoder_layer.linear2.weight',\n              tensor([[-0.0188, -0.0035,  0.0003,  ..., -0.0050, -0.0206, -0.0119],\n                      [-0.0175,  0.0031,  0.0080,  ...,  0.0077, -0.0211,  0.0083],\n                      [-0.0212,  0.0017, -0.0121,  ..., -0.0100, -0.0142,  0.0166],\n                      ...,\n                      [-0.0006,  0.0021,  0.0044,  ..., -0.0220,  0.0149,  0.0134],\n                      [ 0.0168, -0.0084,  0.0013,  ..., -0.0155, -0.0053,  0.0093],\n                      [-0.0025, -0.0153,  0.0220,  ..., -0.0092, -0.0157,  0.0212]])),\n             ('encoder_layer.linear2.bias',\n              tensor([ 0.0086, -0.0005, -0.0008, -0.0122, -0.0132,  0.0094, -0.0139, -0.0006,\n                       0.0220, -0.0163,  0.0220,  0.0172, -0.0153, -0.0143, -0.0197,  0.0016,\n                      -0.0082,  0.0060,  0.0156,  0.0130, -0.0171, -0.0114,  0.0006,  0.0192,\n                       0.0092, -0.0036,  0.0017, -0.0191, -0.0216,  0.0092, -0.0094,  0.0049])),\n             ('encoder_layer.norm1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('encoder_layer.norm1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('encoder_layer.norm2.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('encoder_layer.norm2.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.0.self_attn.in_proj_weight',\n              tensor([[-0.0299,  0.0612, -0.1074,  ...,  0.1874, -0.1438,  0.1018],\n                      [-0.2022,  0.1797, -0.0896,  ...,  0.0619,  0.1461, -0.0290],\n                      [ 0.1697,  0.1872, -0.1305,  ...,  0.0373, -0.1238,  0.0025],\n                      ...,\n                      [ 0.0434, -0.0207, -0.1276,  ..., -0.2075, -0.1074,  0.0820],\n                      [-0.0797,  0.0602, -0.1537,  ..., -0.1581, -0.0481, -0.1953],\n                      [-0.1915,  0.0174,  0.1310,  ...,  0.0938, -0.1755, -0.0699]])),\n             ('transformer_encoder.layers.0.self_attn.in_proj_bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.0.self_attn.out_proj.weight',\n              tensor([[ 0.0442,  0.0661, -0.0040,  ...,  0.1579,  0.1679, -0.1565],\n                      [-0.0869,  0.0993, -0.0980,  ..., -0.1229,  0.0861, -0.0749],\n                      [ 0.0053,  0.0901,  0.0523,  ..., -0.1508,  0.1694,  0.1212],\n                      ...,\n                      [-0.0729,  0.1392,  0.0677,  ..., -0.1376,  0.1651, -0.0069],\n                      [ 0.1157,  0.0458, -0.0585,  ...,  0.0957, -0.0399,  0.1544],\n                      [ 0.1302, -0.1556, -0.0507,  ..., -0.0380,  0.0024, -0.0520]])),\n             ('transformer_encoder.layers.0.self_attn.out_proj.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.0.linear1.weight',\n              tensor([[-0.1237,  0.0499, -0.1630,  ..., -0.1528,  0.0133, -0.0794],\n                      [ 0.1681,  0.1307, -0.1386,  ...,  0.0991, -0.0515,  0.0485],\n                      [-0.1156,  0.0164, -0.1597,  ...,  0.0617, -0.0082,  0.1531],\n                      ...,\n                      [ 0.0532, -0.1464,  0.0787,  ...,  0.1167, -0.0852, -0.0752],\n                      [ 0.0141,  0.1348, -0.0979,  ...,  0.0422, -0.0998,  0.0778],\n                      [-0.0122, -0.0044,  0.0580,  ...,  0.0262, -0.1363,  0.0082]])),\n             ('transformer_encoder.layers.0.linear1.bias',\n              tensor([ 0.0215, -0.1281, -0.0230,  ...,  0.1088,  0.1021,  0.0992])),\n             ('transformer_encoder.layers.0.linear2.weight',\n              tensor([[-0.0188, -0.0035,  0.0003,  ..., -0.0050, -0.0206, -0.0119],\n                      [-0.0175,  0.0031,  0.0080,  ...,  0.0077, -0.0211,  0.0083],\n                      [-0.0212,  0.0017, -0.0121,  ..., -0.0100, -0.0142,  0.0166],\n                      ...,\n                      [-0.0006,  0.0021,  0.0044,  ..., -0.0220,  0.0149,  0.0134],\n                      [ 0.0168, -0.0084,  0.0013,  ..., -0.0155, -0.0053,  0.0093],\n                      [-0.0025, -0.0153,  0.0220,  ..., -0.0092, -0.0157,  0.0212]])),\n             ('transformer_encoder.layers.0.linear2.bias',\n              tensor([ 0.0086, -0.0005, -0.0008, -0.0122, -0.0132,  0.0094, -0.0139, -0.0006,\n                       0.0220, -0.0163,  0.0220,  0.0172, -0.0153, -0.0143, -0.0197,  0.0016,\n                      -0.0082,  0.0060,  0.0156,  0.0130, -0.0171, -0.0114,  0.0006,  0.0192,\n                       0.0092, -0.0036,  0.0017, -0.0191, -0.0216,  0.0092, -0.0094,  0.0049])),\n             ('transformer_encoder.layers.0.norm1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('transformer_encoder.layers.0.norm1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.0.norm2.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('transformer_encoder.layers.0.norm2.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.1.self_attn.in_proj_weight',\n              tensor([[-0.0299,  0.0612, -0.1074,  ...,  0.1874, -0.1438,  0.1018],\n                      [-0.2022,  0.1797, -0.0896,  ...,  0.0619,  0.1461, -0.0290],\n                      [ 0.1697,  0.1872, -0.1305,  ...,  0.0373, -0.1238,  0.0025],\n                      ...,\n                      [ 0.0434, -0.0207, -0.1276,  ..., -0.2075, -0.1074,  0.0820],\n                      [-0.0797,  0.0602, -0.1537,  ..., -0.1581, -0.0481, -0.1953],\n                      [-0.1915,  0.0174,  0.1310,  ...,  0.0938, -0.1755, -0.0699]])),\n             ('transformer_encoder.layers.1.self_attn.in_proj_bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.1.self_attn.out_proj.weight',\n              tensor([[ 0.0442,  0.0661, -0.0040,  ...,  0.1579,  0.1679, -0.1565],\n                      [-0.0869,  0.0993, -0.0980,  ..., -0.1229,  0.0861, -0.0749],\n                      [ 0.0053,  0.0901,  0.0523,  ..., -0.1508,  0.1694,  0.1212],\n                      ...,\n                      [-0.0729,  0.1392,  0.0677,  ..., -0.1376,  0.1651, -0.0069],\n                      [ 0.1157,  0.0458, -0.0585,  ...,  0.0957, -0.0399,  0.1544],\n                      [ 0.1302, -0.1556, -0.0507,  ..., -0.0380,  0.0024, -0.0520]])),\n             ('transformer_encoder.layers.1.self_attn.out_proj.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.1.linear1.weight',\n              tensor([[-0.1237,  0.0499, -0.1630,  ..., -0.1528,  0.0133, -0.0794],\n                      [ 0.1681,  0.1307, -0.1386,  ...,  0.0991, -0.0515,  0.0485],\n                      [-0.1156,  0.0164, -0.1597,  ...,  0.0617, -0.0082,  0.1531],\n                      ...,\n                      [ 0.0532, -0.1464,  0.0787,  ...,  0.1167, -0.0852, -0.0752],\n                      [ 0.0141,  0.1348, -0.0979,  ...,  0.0422, -0.0998,  0.0778],\n                      [-0.0122, -0.0044,  0.0580,  ...,  0.0262, -0.1363,  0.0082]])),\n             ('transformer_encoder.layers.1.linear1.bias',\n              tensor([ 0.0215, -0.1281, -0.0230,  ...,  0.1088,  0.1021,  0.0992])),\n             ('transformer_encoder.layers.1.linear2.weight',\n              tensor([[-0.0188, -0.0035,  0.0003,  ..., -0.0050, -0.0206, -0.0119],\n                      [-0.0175,  0.0031,  0.0080,  ...,  0.0077, -0.0211,  0.0083],\n                      [-0.0212,  0.0017, -0.0121,  ..., -0.0100, -0.0142,  0.0166],\n                      ...,\n                      [-0.0006,  0.0021,  0.0044,  ..., -0.0220,  0.0149,  0.0134],\n                      [ 0.0168, -0.0084,  0.0013,  ..., -0.0155, -0.0053,  0.0093],\n                      [-0.0025, -0.0153,  0.0220,  ..., -0.0092, -0.0157,  0.0212]])),\n             ('transformer_encoder.layers.1.linear2.bias',\n              tensor([ 0.0086, -0.0005, -0.0008, -0.0122, -0.0132,  0.0094, -0.0139, -0.0006,\n                       0.0220, -0.0163,  0.0220,  0.0172, -0.0153, -0.0143, -0.0197,  0.0016,\n                      -0.0082,  0.0060,  0.0156,  0.0130, -0.0171, -0.0114,  0.0006,  0.0192,\n                       0.0092, -0.0036,  0.0017, -0.0191, -0.0216,  0.0092, -0.0094,  0.0049])),\n             ('transformer_encoder.layers.1.norm1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('transformer_encoder.layers.1.norm1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.1.norm2.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('transformer_encoder.layers.1.norm2.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.2.self_attn.in_proj_weight',\n              tensor([[-0.0299,  0.0612, -0.1074,  ...,  0.1874, -0.1438,  0.1018],\n                      [-0.2022,  0.1797, -0.0896,  ...,  0.0619,  0.1461, -0.0290],\n                      [ 0.1697,  0.1872, -0.1305,  ...,  0.0373, -0.1238,  0.0025],\n                      ...,\n                      [ 0.0434, -0.0207, -0.1276,  ..., -0.2075, -0.1074,  0.0820],\n                      [-0.0797,  0.0602, -0.1537,  ..., -0.1581, -0.0481, -0.1953],\n                      [-0.1915,  0.0174,  0.1310,  ...,  0.0938, -0.1755, -0.0699]])),\n             ('transformer_encoder.layers.2.self_attn.in_proj_bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.2.self_attn.out_proj.weight',\n              tensor([[ 0.0442,  0.0661, -0.0040,  ...,  0.1579,  0.1679, -0.1565],\n                      [-0.0869,  0.0993, -0.0980,  ..., -0.1229,  0.0861, -0.0749],\n                      [ 0.0053,  0.0901,  0.0523,  ..., -0.1508,  0.1694,  0.1212],\n                      ...,\n                      [-0.0729,  0.1392,  0.0677,  ..., -0.1376,  0.1651, -0.0069],\n                      [ 0.1157,  0.0458, -0.0585,  ...,  0.0957, -0.0399,  0.1544],\n                      [ 0.1302, -0.1556, -0.0507,  ..., -0.0380,  0.0024, -0.0520]])),\n             ('transformer_encoder.layers.2.self_attn.out_proj.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.2.linear1.weight',\n              tensor([[-0.1237,  0.0499, -0.1630,  ..., -0.1528,  0.0133, -0.0794],\n                      [ 0.1681,  0.1307, -0.1386,  ...,  0.0991, -0.0515,  0.0485],\n                      [-0.1156,  0.0164, -0.1597,  ...,  0.0617, -0.0082,  0.1531],\n                      ...,\n                      [ 0.0532, -0.1464,  0.0787,  ...,  0.1167, -0.0852, -0.0752],\n                      [ 0.0141,  0.1348, -0.0979,  ...,  0.0422, -0.0998,  0.0778],\n                      [-0.0122, -0.0044,  0.0580,  ...,  0.0262, -0.1363,  0.0082]])),\n             ('transformer_encoder.layers.2.linear1.bias',\n              tensor([ 0.0215, -0.1281, -0.0230,  ...,  0.1088,  0.1021,  0.0992])),\n             ('transformer_encoder.layers.2.linear2.weight',\n              tensor([[-0.0188, -0.0035,  0.0003,  ..., -0.0050, -0.0206, -0.0119],\n                      [-0.0175,  0.0031,  0.0080,  ...,  0.0077, -0.0211,  0.0083],\n                      [-0.0212,  0.0017, -0.0121,  ..., -0.0100, -0.0142,  0.0166],\n                      ...,\n                      [-0.0006,  0.0021,  0.0044,  ..., -0.0220,  0.0149,  0.0134],\n                      [ 0.0168, -0.0084,  0.0013,  ..., -0.0155, -0.0053,  0.0093],\n                      [-0.0025, -0.0153,  0.0220,  ..., -0.0092, -0.0157,  0.0212]])),\n             ('transformer_encoder.layers.2.linear2.bias',\n              tensor([ 0.0086, -0.0005, -0.0008, -0.0122, -0.0132,  0.0094, -0.0139, -0.0006,\n                       0.0220, -0.0163,  0.0220,  0.0172, -0.0153, -0.0143, -0.0197,  0.0016,\n                      -0.0082,  0.0060,  0.0156,  0.0130, -0.0171, -0.0114,  0.0006,  0.0192,\n                       0.0092, -0.0036,  0.0017, -0.0191, -0.0216,  0.0092, -0.0094,  0.0049])),\n             ('transformer_encoder.layers.2.norm1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('transformer_encoder.layers.2.norm1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.2.norm2.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('transformer_encoder.layers.2.norm2.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.3.self_attn.in_proj_weight',\n              tensor([[-0.0299,  0.0612, -0.1074,  ...,  0.1874, -0.1438,  0.1018],\n                      [-0.2022,  0.1797, -0.0896,  ...,  0.0619,  0.1461, -0.0290],\n                      [ 0.1697,  0.1872, -0.1305,  ...,  0.0373, -0.1238,  0.0025],\n                      ...,\n                      [ 0.0434, -0.0207, -0.1276,  ..., -0.2075, -0.1074,  0.0820],\n                      [-0.0797,  0.0602, -0.1537,  ..., -0.1581, -0.0481, -0.1953],\n                      [-0.1915,  0.0174,  0.1310,  ...,  0.0938, -0.1755, -0.0699]])),\n             ('transformer_encoder.layers.3.self_attn.in_proj_bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.3.self_attn.out_proj.weight',\n              tensor([[ 0.0442,  0.0661, -0.0040,  ...,  0.1579,  0.1679, -0.1565],\n                      [-0.0869,  0.0993, -0.0980,  ..., -0.1229,  0.0861, -0.0749],\n                      [ 0.0053,  0.0901,  0.0523,  ..., -0.1508,  0.1694,  0.1212],\n                      ...,\n                      [-0.0729,  0.1392,  0.0677,  ..., -0.1376,  0.1651, -0.0069],\n                      [ 0.1157,  0.0458, -0.0585,  ...,  0.0957, -0.0399,  0.1544],\n                      [ 0.1302, -0.1556, -0.0507,  ..., -0.0380,  0.0024, -0.0520]])),\n             ('transformer_encoder.layers.3.self_attn.out_proj.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.3.linear1.weight',\n              tensor([[-0.1237,  0.0499, -0.1630,  ..., -0.1528,  0.0133, -0.0794],\n                      [ 0.1681,  0.1307, -0.1386,  ...,  0.0991, -0.0515,  0.0485],\n                      [-0.1156,  0.0164, -0.1597,  ...,  0.0617, -0.0082,  0.1531],\n                      ...,\n                      [ 0.0532, -0.1464,  0.0787,  ...,  0.1167, -0.0852, -0.0752],\n                      [ 0.0141,  0.1348, -0.0979,  ...,  0.0422, -0.0998,  0.0778],\n                      [-0.0122, -0.0044,  0.0580,  ...,  0.0262, -0.1363,  0.0082]])),\n             ('transformer_encoder.layers.3.linear1.bias',\n              tensor([ 0.0215, -0.1281, -0.0230,  ...,  0.1088,  0.1021,  0.0992])),\n             ('transformer_encoder.layers.3.linear2.weight',\n              tensor([[-0.0188, -0.0035,  0.0003,  ..., -0.0050, -0.0206, -0.0119],\n                      [-0.0175,  0.0031,  0.0080,  ...,  0.0077, -0.0211,  0.0083],\n                      [-0.0212,  0.0017, -0.0121,  ..., -0.0100, -0.0142,  0.0166],\n                      ...,\n                      [-0.0006,  0.0021,  0.0044,  ..., -0.0220,  0.0149,  0.0134],\n                      [ 0.0168, -0.0084,  0.0013,  ..., -0.0155, -0.0053,  0.0093],\n                      [-0.0025, -0.0153,  0.0220,  ..., -0.0092, -0.0157,  0.0212]])),\n             ('transformer_encoder.layers.3.linear2.bias',\n              tensor([ 0.0086, -0.0005, -0.0008, -0.0122, -0.0132,  0.0094, -0.0139, -0.0006,\n                       0.0220, -0.0163,  0.0220,  0.0172, -0.0153, -0.0143, -0.0197,  0.0016,\n                      -0.0082,  0.0060,  0.0156,  0.0130, -0.0171, -0.0114,  0.0006,  0.0192,\n                       0.0092, -0.0036,  0.0017, -0.0191, -0.0216,  0.0092, -0.0094,  0.0049])),\n             ('transformer_encoder.layers.3.norm1.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('transformer_encoder.layers.3.norm1.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('transformer_encoder.layers.3.norm2.weight',\n              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n             ('transformer_encoder.layers.3.norm2.bias',\n              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                      0., 0., 0., 0., 0., 0., 0., 0.])),\n             ('fc.weight',\n              tensor([[ 1.1419e-01, -8.5616e-02, -4.4856e-02, -1.2534e-01, -9.1750e-02,\n                       -9.2200e-03,  2.9937e-02, -1.0026e-01, -1.0753e-01,  1.4776e-01,\n                        1.3461e-01, -9.0483e-02,  2.5837e-02, -1.6965e-01, -1.1305e-01,\n                        5.7421e-02, -4.7127e-02, -2.0099e-02, -7.3755e-02,  1.1230e-01,\n                        1.4994e-01, -2.4798e-02,  9.1310e-02, -4.9150e-02, -1.6556e-03,\n                        7.0877e-02, -9.9013e-02,  4.1845e-02, -1.2083e-01, -1.9219e-02,\n                        1.4639e-01, -1.2240e-01],\n                      [ 7.5886e-02,  1.0507e-01, -1.7333e-01,  1.1371e-01, -5.6823e-02,\n                        3.0297e-02, -1.0171e-01, -1.3389e-01, -6.8411e-02,  4.7047e-02,\n                        3.0708e-02, -1.3188e-01, -3.9474e-02, -1.5534e-01,  9.0594e-02,\n                       -6.8776e-02, -2.3552e-02, -5.1182e-02,  1.5244e-01, -1.6713e-01,\n                        1.7239e-01,  1.1911e-01,  1.3453e-01,  8.0212e-03, -9.5832e-02,\n                        6.5498e-03,  4.5648e-02,  1.3761e-01,  9.1861e-02,  3.8212e-02,\n                       -1.2514e-01,  2.3083e-02],\n                      [ 4.9054e-02,  7.0527e-02, -2.7233e-02,  1.2551e-01, -7.6751e-02,\n                        6.5114e-02, -1.6839e-01,  8.1279e-02, -1.2884e-01, -1.7234e-01,\n                       -7.6225e-02, -2.9287e-02,  1.7315e-01, -4.6572e-02,  1.3860e-01,\n                       -1.2604e-01,  7.2368e-03,  1.6766e-01, -7.4021e-02, -7.3921e-02,\n                       -6.7106e-02, -1.0044e-01,  1.0948e-01, -1.5655e-01, -1.0021e-01,\n                       -1.1717e-01,  7.7466e-02,  3.8521e-02, -1.5398e-01,  8.0990e-02,\n                       -1.0535e-02, -1.5152e-01],\n                      [-1.5765e-01, -5.5684e-03, -1.2315e-01,  3.7880e-02, -1.4822e-01,\n                        9.5804e-02, -4.4631e-02,  3.1276e-02, -1.7436e-01,  8.1498e-02,\n                       -9.1640e-03,  6.5515e-03, -7.4824e-02, -1.6414e-01,  3.6131e-02,\n                       -8.7985e-02,  9.6743e-02,  7.1333e-02, -2.5397e-02,  1.4417e-01,\n                        5.1116e-02, -1.6355e-01, -1.0903e-01,  1.5154e-01,  1.1424e-01,\n                       -1.6180e-01, -1.2812e-01,  1.4670e-01, -9.8236e-02, -8.4771e-02,\n                        5.7563e-02,  1.2344e-01],\n                      [ 1.8089e-02, -7.6550e-02, -6.2147e-02, -1.5609e-01, -7.0966e-02,\n                       -1.1801e-01,  6.0102e-02,  8.0904e-02,  9.9513e-02,  5.9233e-02,\n                       -1.3725e-02, -1.4868e-01,  6.2279e-02,  1.3099e-01,  1.6393e-01,\n                        7.5791e-02, -2.4259e-02, -1.0308e-01,  1.5019e-01, -7.5958e-02,\n                       -7.5526e-02, -1.0298e-01, -1.6816e-01, -1.7633e-01,  5.3433e-02,\n                       -1.5734e-01, -1.6906e-01,  5.6905e-02,  1.8503e-02, -7.2818e-02,\n                        1.5159e-01, -9.3217e-02],\n                      [-1.4872e-01, -1.5519e-01, -1.3241e-01, -8.2673e-02,  1.2122e-01,\n                        7.1951e-02,  9.9875e-02,  4.0529e-02, -1.6656e-01, -2.3874e-02,\n                        1.2625e-03, -1.0163e-01,  9.4322e-02, -4.8310e-02, -5.5755e-02,\n                        1.2148e-01, -3.2941e-02,  7.2501e-02, -2.7430e-02, -7.3937e-02,\n                       -7.0308e-03, -5.5419e-02, -1.6939e-01,  1.4978e-01, -1.0229e-01,\n                        3.8006e-02,  1.2736e-01, -3.0040e-02, -1.0383e-04,  1.4911e-01,\n                       -1.5031e-01,  1.2714e-01]])),\n             ('fc.bias',\n              tensor([-0.0480,  0.0742,  0.1644, -0.0211, -0.1712, -0.0033]))])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "save_path = \"C:\\\\Users\\\\a1882\\Desktop\\\\EEG\\\\model_saved\\\\transformer_1000_100e_1.pt\"\n",
    "\n",
    "onnx_file_name = \"./cnn_3000_100e.onnx\"\n",
    "torch.load(save_path, map_location=torch.device('cpu'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:19:52.972041Z",
     "end_time": "2023-04-24T18:19:53.110110Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is valid!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "# 我们可以使用异常处理的方法进行检验\n",
    "try:\n",
    "    # 当我们的模型不可用时，将会报出异常\n",
    "    onnx.checker.check_model(onnx_file_name)\n",
    "except onnx.checker.ValidationError as e:\n",
    "    print(\"The model is invalid: %s\"%e)\n",
    "else:\n",
    "    # 模型可用时，将不会报出异常，并会输出“The model is valid!”\n",
    "    print(\"The model is valid!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:19:53.621596Z",
     "end_time": "2023-04-24T18:19:55.559205Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "rest\n",
      "rest\n",
      "rest\n",
      "rest\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "rest\n",
      "rest\n",
      "rest\n",
      "rest\n",
      "rest\n",
      "rest\n",
      "rest\n",
      "rest\n",
      "rest\n",
      "rest\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n",
      "walkfocus\n"
     ]
    }
   ],
   "source": [
    "for i in range(windows.shape[2]):\n",
    "    signal = pd.DataFrame(windows[:,:,i]).T\n",
    "    model = Transformer(d_model=32, nhead=4, num_layers=4)\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "    input = torch.from_numpy(signal.values)\n",
    "    input = input.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input.float())\n",
    "        #print(output)\n",
    "    # probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "    # print(probabilities)\n",
    "\n",
    "    _, pred = torch.max(output, dim=1)  # 找到预测分数最大的类别，得到预测类别\n",
    "    label_map = {0: 'lefthand', 1:'read' ,  2:'rest', 3: 'walkbase', 4: 'walkl' ,5: 'walkfocus'}\n",
    "    print(label_map[pred.item()])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:19:55.560204Z",
     "end_time": "2023-04-24T18:20:12.428646Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T18:14:36.163786Z",
     "end_time": "2023-04-24T18:14:36.187782Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
