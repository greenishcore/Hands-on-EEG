{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T06:31:01.367808600Z",
     "start_time": "2023-05-06T06:30:49.940370400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#read the model\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 4), stride=(1, 2))\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.dropout1 = nn.Dropout(p=0.25)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.bn2 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 4), stride=(1, 4))\n",
    "        self.dropout2 = nn.Dropout(p=0.25)\n",
    "        self.fc1 = nn.Linear(48576, 128)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        #print('x:', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        #print('conv1:', x.shape)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        #print('conv2:', x.shape)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print('flatten:', x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = EEGNet()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T06:31:01.631885Z",
     "start_time": "2023-05-06T06:31:01.369796800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "滑窗后信号形状： (33, 3000, 126)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sliding_window(signal, window_size, step_size):\n",
    "    n_channels, n_samples = signal.shape\n",
    "    n_windows = int((n_samples - window_size) / step_size) + 1\n",
    "    windows = np.zeros((n_channels, window_size, n_windows))\n",
    "    for i in range(n_windows):\n",
    "        windows[:, :,i ] = signal.iloc[:, i*step_size:i*step_size+window_size]\n",
    "    return windows\n",
    "\n",
    "signal = pd.read_csv(\"C:\\\\Users\\\\a1882\\Desktop\\EEG\\eegdata\\\\raw\\lefthand_zyy_05_epocflex_2023.03.22t16.50.54+08.00.md.bp.csv\", header=None)\n",
    "signal = pd.DataFrame(signal)\n",
    "# 定义滑窗大小和滑动步长\n",
    "window_size = 3000\n",
    "step_size = 100\n",
    "\n",
    "# 对信号进行滑窗处理\n",
    "windows = sliding_window(signal, window_size, step_size)\n",
    "\n",
    "# 输出滑窗后的信号形状\n",
    "print(\"滑窗后信号形状：\", windows.shape)\n",
    "#print(windows)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T06:31:04.086053500Z",
     "start_time": "2023-05-06T06:31:01.632889800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "save_path = './cnn_3000_30e_26.pt'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T06:31:04.428642400Z",
     "start_time": "2023-05-06T06:31:04.087057Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time: 0.33 seconds\n",
      "lefthand\n",
      "inference time: 0.09 seconds\n",
      "lefthand\n",
      "inference time: 0.05 seconds\n",
      "lefthand\n",
      "inference time: 0.05 seconds\n",
      "lefthand\n",
      "inference time: 0.13 seconds\n",
      "lefthand\n",
      "inference time: 0.07 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.07 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "rest\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.08 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "rest\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.07 seconds\n",
      "rest\n",
      "inference time: 0.02 seconds\n",
      "rest\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "rest\n",
      "inference time: 0.02 seconds\n",
      "rest\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "rest\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.07 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "rest\n",
      "inference time: 0.03 seconds\n",
      "rest\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "rest\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.05 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.04 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.04 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.02 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "inference time: 0.03 seconds\n",
      "lefthand\n",
      "Average inference time: 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "total_time = 0\n",
    "for i in range(windows.shape[2]):\n",
    "    start_time = time.time()\n",
    "    signal = pd.DataFrame(windows[:,:,i])\n",
    "    input = torch.from_numpy(signal.values)\n",
    "    input = input.unsqueeze(0)\n",
    "    output = model(input.float())\n",
    "    _, pred = torch.max(output, dim=1)  # 找到预测分数最大的类别，得到预测类别\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    print('inference time: {:.2f} seconds'.format(inference_time))\n",
    "    label_map = {0: 'lefthand', 1:'read' ,  2:'rest', 3: 'walkbase', 4: 'walkl' ,5: 'walkfocus'}\n",
    "    print(label_map[pred.item()])\n",
    "    total_time += inference_time\n",
    "\n",
    "average_time = total_time / 126\n",
    "\n",
    "# 输出结果\n",
    "print('Average inference time: {:.2f} seconds'.format(average_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-06T06:35:37.108889700Z",
     "start_time": "2023-05-06T06:35:33.474135200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
